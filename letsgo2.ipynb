{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f8fda7",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-09-14T19:32:31.090826Z",
     "iopub.status.busy": "2025-09-14T19:32:31.090336Z",
     "iopub.status.idle": "2025-09-14T19:33:07.274485Z",
     "shell.execute_reply": "2025-09-14T19:33:07.273641Z"
    },
    "papermill": {
     "duration": 36.196184,
     "end_time": "2025-09-14T19:33:07.276000",
     "exception": false,
     "start_time": "2025-09-14T19:32:31.079816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: --quiet, /kaggle/input/autogluon-package\r\n",
      "Processing ./autogluon.tabular-1.3.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy<2.3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular==1.3.1) (1.26.4)\r\n",
      "Requirement already satisfied: scipy<1.16,>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular==1.3.1) (1.15.3)\r\n",
      "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular==1.3.1) (2.2.3)\r\n",
      "\u001b[33mWARNING: Location '--quiet' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/autogluon-package/scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from autogluon.tabular==1.3.1)\r\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular==1.3.1) (3.5)\r\n",
      "\u001b[33mWARNING: Location '--quiet' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/autogluon-package/autogluon.core-1.3.1-py3-none-any.whl (from autogluon.tabular==1.3.1)\r\n",
      "\u001b[33mWARNING: Location '--quiet' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/autogluon-package/autogluon.features-1.3.1-py3-none-any.whl (from autogluon.tabular==1.3.1)\r\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.tabular==1.3.1) (4.67.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.tabular==1.3.1) (2.32.4)\r\n",
      "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.tabular==1.3.1) (3.7.2)\r\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.tabular==1.3.1) (1.39.1)\r\n",
      "\u001b[33mWARNING: Location '--quiet' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/autogluon-package/autogluon.common-1.3.1-py3-none-any.whl (from autogluon.core==1.3.1->autogluon.tabular==1.3.1)\r\n",
      "Requirement already satisfied: psutil<7.1.0,>=5.7.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.common==1.3.1->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (7.0.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular==1.3.1) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular==1.3.1) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular==1.3.1) (2025.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=1.4.0->autogluon.tabular==1.3.1) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=1.4.0->autogluon.tabular==1.3.1) (3.6.0)\r\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (1.39.1)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (0.13.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (4.58.4)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (1.4.8)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (25.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (11.2.1)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (3.0.9)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.0.0->autogluon.tabular==1.3.1) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2024.2.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (2025.6.15)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2024.2.0)\r\n",
      "Installing collected packages: scikit-learn, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed autogluon.common-1.3.1 autogluon.core-1.3.1 autogluon.features-1.3.1 autogluon.tabular-1.3.1 scikit-learn-1.6.1\r\n"
     ]
    }
   ],
   "source": [
    "!cp -r /kaggle/input/autogluon-package/* /kaggle/working/\n",
    "!pip install -f --quiet --no-index --find-links='/kaggle/input/autogluon-package' 'autogluon.tabular-1.3.1-py3-none-any.whl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b132d8b",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:07.295159Z",
     "iopub.status.busy": "2025-09-14T19:33:07.294909Z",
     "iopub.status.idle": "2025-09-14T19:33:13.750744Z",
     "shell.execute_reply": "2025-09-14T19:33:13.749964Z"
    },
    "papermill": {
     "duration": 6.466715,
     "end_time": "2025-09-14T19:33:13.752206",
     "exception": false,
     "start_time": "2025-09-14T19:33:07.285491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: --quiet, /kaggle/input/scikit-package\r\n",
      "Processing ./scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.5.2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\r\n",
      "Installing collected packages: scikit-learn\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.6.1\r\n",
      "    Uninstalling scikit-learn-1.6.1:\r\n",
      "      Successfully uninstalled scikit-learn-1.6.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.5.2\r\n"
     ]
    }
   ],
   "source": [
    "!cp -r /kaggle/input/scikit-package/* /kaggle/working/\n",
    "!pip install -f --quiet --no-index --find-links='/kaggle/input/scikit-package' 'scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb46231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:13.771974Z",
     "iopub.status.busy": "2025-09-14T19:33:13.771720Z",
     "iopub.status.idle": "2025-09-14T19:33:17.354417Z",
     "shell.execute_reply": "2025-09-14T19:33:17.353626Z"
    },
    "papermill": {
     "duration": 3.594085,
     "end_time": "2025-09-14T19:33:17.355958",
     "exception": false,
     "start_time": "2025-09-14T19:33:13.761873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8209a851",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:17.375614Z",
     "iopub.status.busy": "2025-09-14T19:33:17.375187Z",
     "iopub.status.idle": "2025-09-14T19:33:22.605182Z",
     "shell.execute_reply": "2025-09-14T19:33:22.604382Z"
    },
    "papermill": {
     "duration": 5.241212,
     "end_time": "2025-09-14T19:33:22.606813",
     "exception": false,
     "start_time": "2025-09-14T19:33:17.365601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\r\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.2.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Installing collected packages: rdkit\r\n",
      "Successfully installed rdkit-2025.3.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35a46b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:22.626817Z",
     "iopub.status.busy": "2025-09-14T19:33:22.626545Z",
     "iopub.status.idle": "2025-09-14T19:33:27.508506Z",
     "shell.execute_reply": "2025-09-14T19:33:27.507740Z"
    },
    "papermill": {
     "duration": 4.893462,
     "end_time": "2025-09-14T19:33:27.510121",
     "exception": false,
     "start_time": "2025-09-14T19:33:22.616659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/mordred-1-2-0-py3-none-any/\r\n",
      "Processing /kaggle/input/mordred-1-2-0-py3-none-any/mordred-1.2.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: six==1.* in /usr/local/lib/python3.11/dist-packages (from mordred) (1.17.0)\r\n",
      "Requirement already satisfied: numpy==1.* in /usr/local/lib/python3.11/dist-packages (from mordred) (1.26.4)\r\n",
      "Processing /kaggle/input/mordred-1-2-0-py3-none-any/networkx-2.8.8-py3-none-any.whl (from mordred)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.*->mordred) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.*->mordred) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.*->mordred) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.*->mordred) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.*->mordred) (2024.2.0)\r\n",
      "Installing collected packages: networkx, mordred\r\n",
      "  Attempting uninstall: networkx\r\n",
      "    Found existing installation: networkx 3.5\r\n",
      "    Uninstalling networkx-3.5:\r\n",
      "      Successfully uninstalled networkx-3.5\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "autogluon-core 1.3.1 requires networkx<4,>=3.0, but you have networkx 2.8.8 which is incompatible.\r\n",
      "autogluon-tabular 1.3.1 requires networkx<4,>=3.0, but you have networkx 2.8.8 which is incompatible.\r\n",
      "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\r\n",
      "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed mordred-1.2.0 networkx-2.8.8\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mordred --no-index --find-links=file:///kaggle/input/mordred-1-2-0-py3-none-any/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb082f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:27.531517Z",
     "iopub.status.busy": "2025-09-14T19:33:27.531292Z",
     "iopub.status.idle": "2025-09-14T19:33:28.377218Z",
     "shell.execute_reply": "2025-09-14T19:33:28.376334Z"
    },
    "papermill": {
     "duration": 0.857953,
     "end_time": "2025-09-14T19:33:28.378764",
     "exception": false,
     "start_time": "2025-09-14T19:33:27.520811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affe9128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:28.400657Z",
     "iopub.status.busy": "2025-09-14T19:33:28.400076Z",
     "iopub.status.idle": "2025-09-14T19:33:28.403717Z",
     "shell.execute_reply": "2025-09-14T19:33:28.403159Z"
    },
    "papermill": {
     "duration": 0.016031,
     "end_time": "2025-09-14T19:33:28.404813",
     "exception": false,
     "start_time": "2025-09-14T19:33:28.388782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd37451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:28.424531Z",
     "iopub.status.busy": "2025-09-14T19:33:28.424322Z",
     "iopub.status.idle": "2025-09-14T19:33:28.427331Z",
     "shell.execute_reply": "2025-09-14T19:33:28.426834Z"
    },
    "papermill": {
     "duration": 0.013975,
     "end_time": "2025-09-14T19:33:28.428450",
     "exception": false,
     "start_time": "2025-09-14T19:33:28.414475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f64ab7d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:28.448413Z",
     "iopub.status.busy": "2025-09-14T19:33:28.447776Z",
     "iopub.status.idle": "2025-09-14T19:33:35.174267Z",
     "shell.execute_reply": "2025-09-14T19:33:35.173357Z"
    },
    "papermill": {
     "duration": 6.738247,
     "end_time": "2025-09-14T19:33:35.176169",
     "exception": false,
     "start_time": "2025-09-14T19:33:28.437922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/betterback/rdkit-2025-3-3-cp311/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl -q\n",
    "!pip install mordred --no-index --find-links=file:///kaggle/input/betterback/mordred-1-2-0-py3-none-any/mordred-1-2-0-py3-none-any -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1383440f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:35.196832Z",
     "iopub.status.busy": "2025-09-14T19:33:35.196555Z",
     "iopub.status.idle": "2025-09-14T19:33:38.332713Z",
     "shell.execute_reply": "2025-09-14T19:33:38.331730Z"
    },
    "papermill": {
     "duration": 3.147825,
     "end_time": "2025-09-14T19:33:38.334149",
     "exception": false,
     "start_time": "2025-09-14T19:33:35.186324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\r\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.2.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "rdkit is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b1a34f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:38.430351Z",
     "iopub.status.busy": "2025-09-14T19:33:38.429628Z",
     "iopub.status.idle": "2025-09-14T19:33:42.872006Z",
     "shell.execute_reply": "2025-09-14T19:33:42.870975Z"
    },
    "papermill": {
     "duration": 4.455402,
     "end_time": "2025-09-14T19:33:42.874077",
     "exception": false,
     "start_time": "2025-09-14T19:33:38.418675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/torch-geometric-2-6-1/torch_geometric-2.6.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.12.13)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (2025.5.1)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.1.6)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (1.26.4)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (7.0.0)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.0.9)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (4.67.1)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (6.6.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.20.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.6.1) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2.4.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (2025.6.15)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric==2.6.1) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric==2.6.1) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric==2.6.1) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric==2.6.1) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric==2.6.1) (2024.2.0)\r\n",
      "Installing collected packages: torch-geometric\r\n",
      "Successfully installed torch-geometric-2.6.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/torch-geometric-2-6-1/torch_geometric-2.6.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7b4d1",
   "metadata": {
    "papermill": {
     "duration": 0.009146,
     "end_time": "2025-09-14T19:33:38.372242",
     "exception": false,
     "start_time": "2025-09-14T19:33:38.363096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Notebook Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51693f",
   "metadata": {
    "papermill": {
     "duration": 0.009208,
     "end_time": "2025-09-14T19:33:38.390762",
     "exception": false,
     "start_time": "2025-09-14T19:33:38.381554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is a deep dive into hybrid ensemble systems for molecular property prediction. I experimented with and successfully integrated:\n",
    "\n",
    "Core Models: XGBoost, CatBoost, LightGBM, Extra Trees, Random Forest, Neural Networks (PyTorch), TabNet, HistGradientBoosting\n",
    "\n",
    "Stacking Ensembles: Blending multiple models with meta-learners (ElasticNet, CatBoost, LightGBM, XGBoost)\n",
    "\n",
    "Feature Engineering: RDKit + Mordred descriptors, Morgan & MACCS fingerprints, ChemBERTA embeddings, graph-based features\n",
    "\n",
    "Feature Selection: Lasso, ElasticNet, Variance Threshold, SHAP, permutation importance\n",
    "\n",
    "Neural Networks: Custom FFNets optimized by dataset size (Rg, Tc, Tg, Density, FFV)\n",
    "\n",
    "The hybrid setup combined cross-validation, model averaging, and multi-source feature importance to build a robust and adaptive prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f21216a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:33:42.906241Z",
     "iopub.status.busy": "2025-09-14T19:33:42.905971Z",
     "iopub.status.idle": "2025-09-14T19:53:40.131561Z",
     "shell.execute_reply": "2025-09-14T19:53:40.130578Z"
    },
    "papermill": {
     "duration": 1197.248121,
     "end_time": "2025-09-14T19:53:40.132942",
     "exception": false,
     "start_time": "2025-09-14T19:33:42.884821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Code A] Starting...\n",
      "Loading competition data...\n",
      "Training data shape: (7973, 7), Test data shape: (3, 2)\n",
      "Cleaning and validating SMILES...\n",
      "   Removed 0 invalid SMILES from training data\n",
      "   Removed 0 invalid SMILES from test data\n",
      "   Final training samples: 7973\n",
      "   Final test samples: 3\n",
      "\n",
      "📂 Loading external datasets...\n",
      "   ✅ Tc data: 874 samples\n",
      "   ⚠️ TgSS enriched data failed: [Errno 2] No such file or directory: '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleane\n",
      "   ✅ JCIM Tg data: 662 samples\n",
      "   ✅ Xlsx Tg data: 501 samples\n",
      "   ✅ Density data: 786 samples\n",
      "   ✅ dataset 4: 862 samples\n",
      "\n",
      "🔄 Integrating external data...\n",
      "   Processing Tc data...\n",
      "      Processing 874 Tc samples...\n",
      "      Kept 874/874 valid samples\n",
      "      Tc: +129 samples, +129 unique SMILES\n",
      "      Filled 0 missing entries in train for Tc\n",
      "      Added 129 new entries for Tc\n",
      "   Processing Tg data...\n",
      "      Processing 662 Tg samples...\n",
      "      Kept 662/662 valid samples\n",
      "      Tg: +151 samples, +136 unique SMILES\n",
      "      Filled 15 missing entries in train for Tg\n",
      "      Added 136 new entries for Tg\n",
      "   Processing Tg data...\n",
      "      Processing 501 Tg samples...\n",
      "      Kept 501/501 valid samples\n",
      "      Tg: +499 samples, +499 unique SMILES\n",
      "      Filled 0 missing entries in train for Tg\n",
      "      Added 499 new entries for Tg\n",
      "   Processing Density data...\n",
      "      Processing 786 Density samples...\n",
      "      Kept 780/786 valid samples\n",
      "      Density: +634 samples, +524 unique SMILES\n",
      "      Filled 110 missing entries in train for Density\n",
      "      Added 524 new entries for Density\n",
      "   Processing FFV data...\n",
      "      Processing 862 FFV samples...\n",
      "      Kept 862/862 valid samples\n",
      "      FFV: +862 samples, +819 unique SMILES\n",
      "      Filled 43 missing entries in train for FFV\n",
      "      Added 819 new entries for FFV\n",
      "\n",
      "📊 Final training data:\n",
      "   Original samples: 7973\n",
      "   Extended samples: 10080\n",
      "   Gain: +2107 samples\n",
      "   Tg: 1,161 samples (+650)\n",
      "   FFV: 7,892 samples (+862)\n",
      "   Tc: 866 samples (+129)\n",
      "   Density: 1,247 samples (+634)\n",
      "   Rg: 614 samples (+0)\n",
      "\n",
      "✅ Data integration complete with clean SMILES!\n",
      "Tg NaNs per column:\n",
      "SMILES    0\n",
      "Tg        0\n",
      "dtype: int64\n",
      "(1161, 2)\n",
      "----------------------------------------\n",
      "FFV NaNs per column:\n",
      "SMILES    0\n",
      "FFV       0\n",
      "dtype: int64\n",
      "(7892, 2)\n",
      "----------------------------------------\n",
      "Tc NaNs per column:\n",
      "SMILES    0\n",
      "Tc        0\n",
      "dtype: int64\n",
      "(866, 2)\n",
      "----------------------------------------\n",
      "Density NaNs per column:\n",
      "SMILES     0\n",
      "Density    0\n",
      "dtype: int64\n",
      "(1247, 2)\n",
      "----------------------------------------\n",
      "Rg NaNs per column:\n",
      "SMILES    0\n",
      "Rg        0\n",
      "dtype: int64\n",
      "(614, 2)\n",
      "----------------------------------------\n",
      "\n",
      "=== Loading models and data for label: Tg ===\n",
      "Loaded 10 models for label 'Tg'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping correlated columns from test: ['ATS3d', 'ATS5d', 'ATS1Z', 'Xp-2d', 'ETA_eta_L', 'ETA_eta_FL', 'IC4']\n",
      "Model 1 holdout MAE: 15.50679819998558\n",
      "Model 2 holdout MAE: 15.132775055537877\n",
      "Model 3 holdout MAE: 14.746208292911291\n",
      "Model 4 holdout MAE: 18.796538719059015\n",
      "Model 5 holdout MAE: 16.259336387111773\n",
      "Model 6 holdout MAE: 17.838669646433686\n",
      "Model 7 holdout MAE: 18.303901659413537\n",
      "Model 8 holdout MAE: 17.322172298437305\n",
      "Model 9 holdout MAE: 15.448651658787695\n",
      "Model 10 holdout MAE: 15.791739486828485\n",
      "Tg Holdout MAE (mean ± std over all models): 16.51468 ± 1.36347\n",
      "[167.22568  171.49619  112.769226]\n",
      "\n",
      "=== Loading models and data for label: FFV ===\n",
      "Loaded 10 models for label 'FFV'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping correlated columns from test: ['SpDiam_A', 'VR2_A', 'ATS6d', 'ATS8Z', 'AATS2dv', 'AATS1d', 'Xpc-5d', 'Xpc-6d', 'Xp-4d', 'AXp-0d', 'SsCH3', 'StsC', 'SaaaC', 'SssNH', 'SaaN', 'SaasN', 'SssO', 'AETA_beta_ns', 'AETA_dBeta', 'SIC5', 'BIC0', 'CIC2', 'CIC4', 'CIC5', 'ZMIC4', 'ZMIC5', 'MID_h', 'MID_N', 'MID_O', 'MPC9', 'piPC8', 'piPC10', 'TpiPC10', 'TopoPSA', 'GGI2', 'GGI3', 'GGI9', 'AMW']\n",
      "Model 1 holdout MAE: 0.003181394273829\n",
      "Model 2 holdout MAE: 0.0030051946982283034\n",
      "Model 3 holdout MAE: 0.002603549133033655\n",
      "Model 4 holdout MAE: 0.0028116787887928563\n",
      "Model 5 holdout MAE: 0.003015804047563911\n",
      "Model 6 holdout MAE: 0.002573779564237642\n",
      "Model 7 holdout MAE: 0.0025629417617373776\n",
      "Model 8 holdout MAE: 0.0025816899973995187\n",
      "Model 9 holdout MAE: 0.002434586068605529\n",
      "Model 10 holdout MAE: 0.003021486554341292\n",
      "FFV Holdout MAE (mean ± std over all models): 0.00278 ± 0.00025\n",
      "[0.3750729  0.37614182 0.35092032]\n",
      "\n",
      "=== Loading models and data for label: Tc ===\n",
      "Loaded 10 models for label 'Tc'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping correlated columns from test: ['ATS2dv', 'AATS1d', 'MZ', 'AETA_beta_ns', 'CIC5', 'VSA_EState7']\n",
      "Model 1 holdout MAE: 0.029072056111796144\n",
      "Model 2 holdout MAE: 0.02525132977473781\n",
      "Model 3 holdout MAE: 0.03151671406674659\n",
      "Model 4 holdout MAE: 0.029781155687776107\n",
      "Model 5 holdout MAE: 0.03319188211964465\n",
      "Model 6 holdout MAE: 0.031186267680683357\n",
      "Model 7 holdout MAE: 0.0287859162002231\n",
      "Model 8 holdout MAE: 0.034352132459344535\n",
      "Model 9 holdout MAE: 0.029098924963090616\n",
      "Model 10 holdout MAE: 0.032117810254672474\n",
      "Tc Holdout MAE (mean ± std over all models): 0.03044 ± 0.00247\n",
      "[0.18172295 0.24017832 0.2527067 ]\n",
      "\n",
      "=== Loading models and data for label: Density ===\n",
      "Loaded 10 models for label 'Density'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping correlated columns from test: ['ATS5d', 'ATS7d', 'ATS7Z', 'AATS2dv', 'nBondsS', 'nBondsKS', 'FCSP3', 'MZ', 'SsNH2', 'TIC2', 'TIC3', 'TIC5', 'BIC0', 'SlogP_VSA6', 'MPC8', 'SMR', 'AMW']\n",
      "Model 1 holdout MAE: 0.015373099922109373\n",
      "Model 2 holdout MAE: 0.016884704879880856\n",
      "Model 3 holdout MAE: 0.02180963844608985\n",
      "Model 4 holdout MAE: 0.02268799879478906\n",
      "Model 5 holdout MAE: 0.012700415204003907\n",
      "Model 6 holdout MAE: 0.018609349198783688\n",
      "Model 7 holdout MAE: 0.01659094873413842\n",
      "Model 8 holdout MAE: 0.016453451721263668\n",
      "Model 9 holdout MAE: 0.013605503480093265\n",
      "Model 10 holdout MAE: 0.01586205507145117\n",
      "Density Holdout MAE (mean ± std over all models): 0.01706 ± 0.00304\n",
      "[1.1703981 1.0836802 1.113959 ]\n",
      "\n",
      "=== Loading models and data for label: Rg ===\n",
      "Loaded 10 models for label 'Rg'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping correlated columns from test: ['ATS6d', 'nBondsKS', 'Xp-1d', 'SsCH3', 'ZMIC1']\n",
      "Model 1 holdout MAE: 0.854962744812051\n",
      "Model 2 holdout MAE: 0.6686672602067715\n",
      "Model 3 holdout MAE: 0.7273912048275264\n",
      "Model 4 holdout MAE: 0.9728035707123394\n",
      "Model 5 holdout MAE: 0.7460436389433437\n",
      "Model 6 holdout MAE: 0.7648310415865206\n",
      "Model 7 holdout MAE: 0.7340207549049034\n",
      "Model 8 holdout MAE: 0.6858005633369298\n",
      "Model 9 holdout MAE: 0.5996891318617299\n",
      "Model 10 holdout MAE: 0.7776748795477888\n",
      "Rg Holdout MAE (mean ± std over all models): 0.75319 ± 0.09780\n",
      "[20.529217 19.965155 21.623934]\n",
      "\n",
      "Mean Absolute Error for each label:\n",
      "     label fold_mae_mean fold_mae_std  holdout_mae_mean  holdout_mae_std\n",
      "0       Tg          None         None         16.514679         1.363467\n",
      "1      FFV          None         None          0.002779         0.000246\n",
      "2       Tc          None         None          0.030435         0.002473\n",
      "3  Density          None         None          0.017058         0.003041\n",
      "4       Rg          None         None          0.753188         0.097803\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  167.225677  0.375073  0.181723  1.170398  20.529217\n",
      "1  1422188626  171.496185  0.376142  0.240178  1.083680  19.965155\n",
      "2  2032016830  112.769226  0.350920  0.252707  1.113959  21.623934\n",
      "[Code A] Finished.\n",
      "\n",
      "[Code B] Starting...\n",
      "Loading competition data...\n",
      "Training data shape: (7973, 7), Test data shape: (3, 2)\n",
      "Cleaning and validating SMILES...\n",
      "   Removed 0 invalid SMILES from training data\n",
      "   Removed 0 invalid SMILES from test data\n",
      "   Final training samples: 7973\n",
      "   Final test samples: 3\n",
      "\n",
      "📂 Loading external datasets...\n",
      "   ✅ Tc data: 874 samples\n",
      "   ⚠️ TgSS enriched data failed: [Errno 2] No such file or directory: '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleane\n",
      "   ✅ JCIM Tg data: 662 samples\n",
      "   ✅ Xlsx Tg data: 501 samples\n",
      "   ✅ Density data: 786 samples\n",
      "   ✅ dataset 4: 862 samples\n",
      "\n",
      "🔄 Integrating external data...\n",
      "   Processing Tc data...\n",
      "      Processing 874 Tc samples...\n",
      "      Kept 874/874 valid samples\n",
      "      Tc: +129 samples, +129 unique SMILES\n",
      "      Filled 0 missing entries in train for Tc\n",
      "      Added 129 new entries for Tc\n",
      "   Processing Tg data...\n",
      "      Processing 662 Tg samples...\n",
      "      Kept 662/662 valid samples\n",
      "      Tg: +151 samples, +136 unique SMILES\n",
      "      Filled 15 missing entries in train for Tg\n",
      "      Added 136 new entries for Tg\n",
      "   Processing Tg data...\n",
      "      Processing 501 Tg samples...\n",
      "      Kept 501/501 valid samples\n",
      "      Tg: +499 samples, +499 unique SMILES\n",
      "      Filled 0 missing entries in train for Tg\n",
      "      Added 499 new entries for Tg\n",
      "   Processing Density data...\n",
      "      Processing 786 Density samples...\n",
      "      Kept 780/786 valid samples\n",
      "      Density: +634 samples, +524 unique SMILES\n",
      "      Filled 110 missing entries in train for Density\n",
      "      Added 524 new entries for Density\n",
      "   Processing FFV data...\n",
      "      Processing 862 FFV samples...\n",
      "      Kept 862/862 valid samples\n",
      "      FFV: +862 samples, +819 unique SMILES\n",
      "      Filled 43 missing entries in train for FFV\n",
      "      Added 819 new entries for FFV\n",
      "\n",
      "📊 Final training data:\n",
      "   Original samples: 7973\n",
      "   Extended samples: 10080\n",
      "   Gain: +2107 samples\n",
      "   Tg: 1,161 samples (+650)\n",
      "   FFV: 7,892 samples (+862)\n",
      "   Tc: 866 samples (+129)\n",
      "   Density: 1,247 samples (+634)\n",
      "   Rg: 614 samples (+0)\n",
      "\n",
      "✅ Data integration complete with clean SMILES!\n",
      "\n",
      "==================== Processing GNN for label: Tg ====================\n",
      "Using MLP Config: Neurons=[512, 256, 128], Dropouts=[0.5, 0.4, 0.2]\n",
      "Loading 10 models and ALL 3 RobustScalers for Tg ensemble...\n",
      "Loaded Y, U, and X RobustScalers.\n",
      "Successfully loaded saved model for Tg_fold0 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold0.pth\n",
      "Successfully loaded saved model for Tg_fold1 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold1.pth\n",
      "Successfully loaded saved model for Tg_fold2 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold2.pth\n",
      "Successfully loaded saved model for Tg_fold3 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold3.pth\n",
      "Successfully loaded saved model for Tg_fold4 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold4.pth\n",
      "Successfully loaded saved model for Tg_fold5 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold5.pth\n",
      "Successfully loaded saved model for Tg_fold6 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold6.pth\n",
      "Successfully loaded saved model for Tg_fold7 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold7.pth\n",
      "Successfully loaded saved model for Tg_fold8 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold8.pth\n",
      "Successfully loaded saved model for Tg_fold9 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold9.pth\n",
      "Successfully loaded 10 models for ensemble.\n",
      "Making ensemble (scaled) predictions for Tg using 10 models...\n",
      "\n",
      "==================== Processing GNN for label: FFV ====================\n",
      "Using MLP Config: Neurons=[1024, 512, 64], Dropouts=[0.6, 0.5, 0.4]\n",
      "Loading 10 models and ALL 3 RobustScalers for FFV ensemble...\n",
      "Loaded Y, U, and X RobustScalers.\n",
      "Successfully loaded saved model for FFV_fold0 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold0.pth\n",
      "Successfully loaded saved model for FFV_fold1 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold1.pth\n",
      "Successfully loaded saved model for FFV_fold2 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold2.pth\n",
      "Successfully loaded saved model for FFV_fold3 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold3.pth\n",
      "Successfully loaded saved model for FFV_fold4 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold4.pth\n",
      "Successfully loaded saved model for FFV_fold5 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold5.pth\n",
      "Successfully loaded saved model for FFV_fold6 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold6.pth\n",
      "Successfully loaded saved model for FFV_fold7 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold7.pth\n",
      "Successfully loaded saved model for FFV_fold8 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold8.pth\n",
      "Successfully loaded saved model for FFV_fold9 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold9.pth\n",
      "Successfully loaded 10 models for ensemble.\n",
      "Making ensemble (scaled) predictions for FFV using 10 models...\n",
      "\n",
      "==================== Processing GNN for label: Tc ====================\n",
      "Using MLP Config: Neurons=[128, 64], Dropouts=[0.4, 0.3]\n",
      "Loading 10 models and ALL 3 RobustScalers for Tc ensemble...\n",
      "Loaded Y, U, and X RobustScalers.\n",
      "Successfully loaded saved model for Tc_fold0 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold0.pth\n",
      "Successfully loaded saved model for Tc_fold1 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold1.pth\n",
      "Successfully loaded saved model for Tc_fold2 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold2.pth\n",
      "Successfully loaded saved model for Tc_fold3 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold3.pth\n",
      "Successfully loaded saved model for Tc_fold4 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold4.pth\n",
      "Successfully loaded saved model for Tc_fold5 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold5.pth\n",
      "Successfully loaded saved model for Tc_fold6 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold6.pth\n",
      "Successfully loaded saved model for Tc_fold7 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold7.pth\n",
      "Successfully loaded saved model for Tc_fold8 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold8.pth\n",
      "Successfully loaded saved model for Tc_fold9 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold9.pth\n",
      "Successfully loaded 10 models for ensemble.\n",
      "Making ensemble (scaled) predictions for Tc using 10 models...\n",
      "\n",
      "==================== Processing GNN for label: Density ====================\n",
      "Using MLP Config: Neurons=[1024, 256, 64], Dropouts=[0.5, 0.4, 0.3]\n",
      "Loading 10 models and ALL 3 RobustScalers for Density ensemble...\n",
      "Loaded Y, U, and X RobustScalers.\n",
      "Successfully loaded saved model for Density_fold0 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold0.pth\n",
      "Successfully loaded saved model for Density_fold1 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold1.pth\n",
      "Successfully loaded saved model for Density_fold2 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold2.pth\n",
      "Successfully loaded saved model for Density_fold3 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold3.pth\n",
      "Successfully loaded saved model for Density_fold4 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold4.pth\n",
      "Successfully loaded saved model for Density_fold5 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold5.pth\n",
      "Successfully loaded saved model for Density_fold6 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold6.pth\n",
      "Successfully loaded saved model for Density_fold7 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold7.pth\n",
      "Successfully loaded saved model for Density_fold8 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold8.pth\n",
      "Successfully loaded saved model for Density_fold9 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold9.pth\n",
      "Successfully loaded 10 models for ensemble.\n",
      "Making ensemble (scaled) predictions for Density using 10 models...\n",
      "\n",
      "==================== Processing GNN for label: Rg ====================\n",
      "Using MLP Config: Neurons=[128, 64, 64], Dropouts=[0.4, 0.3, 0.3]\n",
      "Loading 10 models and ALL 3 RobustScalers for Rg ensemble...\n",
      "Loaded Y, U, and X RobustScalers.\n",
      "Successfully loaded saved model for Rg_fold0 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold0.pth\n",
      "Successfully loaded saved model for Rg_fold1 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold1.pth\n",
      "Successfully loaded saved model for Rg_fold2 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold2.pth\n",
      "Successfully loaded saved model for Rg_fold3 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold3.pth\n",
      "Successfully loaded saved model for Rg_fold4 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold4.pth\n",
      "Successfully loaded saved model for Rg_fold5 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold5.pth\n",
      "Successfully loaded saved model for Rg_fold6 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold6.pth\n",
      "Successfully loaded saved model for Rg_fold7 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold7.pth\n",
      "Successfully loaded saved model for Rg_fold8 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold8.pth\n",
      "Successfully loaded saved model for Rg_fold9 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold9.pth\n",
      "Successfully loaded 10 models for ensemble.\n",
      "Making ensemble (scaled) predictions for Rg using 10 models...\n",
      "\n",
      "✅ GNN Ensemble predictions (Original Scale) saved to submission_hybrid_gnn_final.csv\n",
      "\n",
      "GNN Submission Preview:\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  189.751847  0.373679  0.187229  1.168659  21.798934\n",
      "1  1422188626  174.595003  0.375173  0.266134  1.103980  23.134463\n",
      "2  2032016830  111.029739  0.350787  0.250217  1.113623  19.598631\n",
      "[Code B] Finished.\n",
      "\n",
      "[Code C FAST] Starting (TF-IDF + RidgeCV)...\n",
      "[Code C FAST] Fitting TF-IDF vectorizer...\n",
      "\n",
      "[Code C FAST] === Tg ===\n",
      "   Training RidgeCV...\n",
      "   Chosen alpha: 1.0\n",
      "\n",
      "[Code C FAST] === FFV ===\n",
      "   Training RidgeCV...\n",
      "   Chosen alpha: 0.1\n",
      "\n",
      "[Code C FAST] === Tc ===\n",
      "   Training RidgeCV...\n",
      "   Chosen alpha: 0.3\n",
      "\n",
      "[Code C FAST] === Density ===\n",
      "   Training RidgeCV...\n",
      "   Chosen alpha: 0.1\n",
      "\n",
      "[Code C FAST] === Rg ===\n",
      "   Training RidgeCV...\n",
      "   Chosen alpha: 0.1\n",
      "\n",
      "[Code C FAST] ✅ predictions saved to submission_code_c_fast.csv\n",
      "\n",
      "[Code C FAST] Submission preview:\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  119.778389  0.377230  0.176128  1.149574  23.174107\n",
      "1  1422188626  177.832489  0.378595  0.262756  1.067260  21.672651\n",
      "2  2032016830  145.358261  0.358337  0.308669  1.119274  21.712187\n",
      "[Code C FAST] Finished.\n",
      "\n",
      "[Code D FAST] Started.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[speed] sklearnex patch active\n",
      "[mordred] computing descriptors (single-threaded batches)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mordred] computed 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mordred] cached to /kaggle/working/desc_test.parquet\n",
      "[Tg] MAE — Cat(mean): 28.269714 | XGB(mean): 22.613513 | Blend(w=0.00): 22.606426\n",
      "[FFV] MAE — Cat(mean): 0.007063 | XGB(mean): 0.005360 | Blend(w=0.00): 0.005354\n",
      "[Tc] MAE — Cat(mean): 0.031991 | XGB(mean): 0.031420 | Blend(w=0.40): 0.031065\n",
      "[Density] MAE — Cat(mean): 0.062805 | XGB(mean): 0.076716 | Blend(w=1.00): 0.062641\n",
      "[Rg] MAE — Cat(mean): 1.411180 | XGB(mean): 1.395229 | Blend(w=0.45): 1.343468\n",
      "Local holdout MAE: Tg: 22.606426 FFV: 0.005354 Tc: 0.031065 Density: 0.062641 Rg: 1.343468\n",
      "\n",
      "[Code C FAST] ✅ predictions saved to submission_code_d.csv\n",
      "\n",
      "[Code C FAST] Submission preview:\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  164.099182  0.372704  0.173065  1.156084  19.799417\n",
      "1  1422188626  153.224548  0.379635  0.227679  1.070280  20.430359\n",
      "2  2032016830  140.377808  0.350907  0.255535  1.097286  21.836700\n",
      "[Code C FAST] Finished.\n",
      "\n",
      "[Code D FAST] Finished.\n",
      "\n",
      "/kaggle/input/torch-geometric-2-6-1/torch_geometric-2.6.1-py3-none-any.whl\n",
      "/kaggle/input/mordred-1-2-0-py3-none-any/mordred-1.2.0-py3-none-any.whl\n",
      "/kaggle/input/mordred-1-2-0-py3-none-any/networkx-2.8.8-py3-none-any.whl\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/train.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/test.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset2.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\n",
      "/kaggle/input/betterback/tg-xcg/tg-xcg/Tg_xgb.json\n",
      "/kaggle/input/betterback/rdkit-2025-3-3-cp311/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/betterback/mordred-1-2-0-py3-none-any/mordred-1-2-0-py3-none-any/mordred-1.2.0-py3-none-any.whl\n",
      "/kaggle/input/betterback/mordred-1-2-0-py3-none-any/mordred-1-2-0-py3-none-any/networkx-2.8.8-py3-none-any.whl\n",
      "/kaggle/input/betterback/modred-dataset/modred-dataset/desc_ffv.csv\n",
      "/kaggle/input/betterback/modred-dataset/modred-dataset/desc_de.csv\n",
      "/kaggle/input/betterback/modred-dataset/modred-dataset/desc_tc.csv\n",
      "/kaggle/input/betterback/modred-dataset/modred-dataset/desc_tg.csv\n",
      "/kaggle/input/betterback/modred-dataset/modred-dataset/desc_rg.csv\n",
      "/kaggle/input/download-molformer/__results__.html\n",
      "/kaggle/input/download-molformer/__huggingface_repos__.json\n",
      "/kaggle/input/download-molformer/__notebook__.ipynb\n",
      "/kaggle/input/download-molformer/__output__.json\n",
      "/kaggle/input/download-molformer/custom.css\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/config.json\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/configuration_molformer.py\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/modeling_molformer.py\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/README.md\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/tokenizer.json\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/vocab.json\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/tokenization_molformer.py\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/tokenizer_config.json\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/convert_molformer_original_checkpoint_to_pytorch.py\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/pytorch_model.bin\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/tokenization_molformer_fast.py\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/model.safetensors\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/special_tokens_map.json\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.gitattributes\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/pipeline.jpeg\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/config\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/packed-refs\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/HEAD\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/index\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/description\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/info/exclude\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/refs/heads/main\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/refs/remotes/origin/HEAD\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/pre-merge-commit.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/post-merge\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/pre-push\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/prepare-commit-msg.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/update.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/pre-push.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/pre-rebase.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/pre-applypatch.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/post-commit\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/post-checkout\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/push-to-checkout.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/pre-commit.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/commit-msg.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/post-update.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/pre-receive.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/fsmonitor-watchman.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/hooks/applypatch-msg.sample\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/lfs/objects/07/95/0795977fe7192c4acdaf052f0e8464af57bc4bb59211271c5e61aaba2637b9c6\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/lfs/objects/93/e3/93e3fced64b896fcfea4934505ac80275db7afb7320d0b32ee0c691d99ab8678\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/logs/HEAD\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/logs/refs/heads/main\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/logs/refs/remotes/origin/HEAD\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/c5/51b8ab1b2be09d8967b3740026d27a84f459cb\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/2e/da163a665c006fefa3ad28a142fb30c25b9360\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/36/5339483e514d724a9c3d98ec31ca2ddfc973a9\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/ff/1789999abf7087ab6fcab1ab88ed3a3d674f93\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/42/3b6b1c406f55074e6811b1bc8581f4fefde35d\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/b1/6b91001e513cbc7bba4a83036c0f3a2597ed15\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/ef/89e0d3ff04d8ce06993befcaf133b6c40e03e3\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/31/d6c4969840661076701b2c805efaea1492a68e\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/31/b2fd7e37f44a2125280d69ff7d9bb85a768dae\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/1a/366b8852e1ee4c6283d347d34f82be90c49c4a\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/c2/25fa2e291e5424ed5155a318ee941841da4d93\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/67/768713a20ade119d3a4bd42dd924481eecb29c\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/ab/b341ed7cee8c7783088d70bf9f86dff266844e\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/66/f8ead9af7332c6670e611c8e3df443e1afe5b2\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/a5/8a743c6ee41fc7ae4038a290d25e68b66d4cfd\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/bc/cab01b421a2445b1e619228f32d8460ed81af9\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/16/076bfe31080edbc0bdb1f1765af324a1e4f4ec\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/7b/12d946c181a37f6012b9dc3b002275de070314\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/ba/1a41edace11006ef54ec8b274b43645936ebb5\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/15/4df8298fab5ecf322016157858e08cd1bccbe1\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/43/246c556f7d57aa9db7a8dbb2bc0530d25eb2e3\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/09/c8af343ba46874dea2d3aed48dc26409b87645\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/30/159e4eaab336ab8bce3e17bb3caaddd590d846\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/76/c1698d125bfdf1d1f3d9f5441797aa2aa75054\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/a6/344aac8c09253b3b630fb776ae94478aa0275b\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/f2/4331e0bb3a00ca7557226d3dddc61c78b691f8\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/6b/d6ed61d211ccb952ef8f08d7941d846c5b7561\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/44/a1a314a828e8cead4b84b31f853f8e18a7180e\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/9a/2f2ca032372baa7f439f60d245b7ca66069357\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/e9/7233825700c8baea008c2ad8969c86b4626de0\n",
      "/kaggle/input/download-molformer/molformer-xl-both-10pct/.git/objects/48/105a2ff71e781579b2034852086db3e8b01906\n",
      "/kaggle/input/autogluon-package/spacy_loggers-1.0.5-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/aiohttp-3.12.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/fasttransform-0.0.2-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/autogluon-package/contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/shellingham-1.5.4-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/fonttools-4.58.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/autogluon-package/plum_dispatch-2.5.7-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/pyasn1-0.6.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/joblib-1.5.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/cachetools-5.5.2-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/click-8.2.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/grpcio-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/google_auth-2.40.3-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/tensorboardx-2.6.4-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl\n",
      "/kaggle/input/autogluon-package/triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/fastai-2.8.2-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/filelock-3.18.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/jsonschema_specifications-2025.4.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/jmespath-1.0.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/future-1.0.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/autogluon.core-1.3.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/opencensus-0.11.4-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/beartype-0.21.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/googleapis_common_protos-1.70.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/langcodes-3.5.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/autogluon-package/requests-2.32.4-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/tqdm-4.67.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/aiosignal-1.3.2-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/fastcore-1.8.4-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/py4j-0.10.9.9-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/thinc-8.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/rpds_py-0.25.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/autogluon.features-1.3.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/platformdirs-4.3.8-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/cloudpathlib-0.21.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/__results__.html\n",
      "/kaggle/input/autogluon-package/networkx-3.5-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/fastdownload-0.0.7-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/fsspec-2025.5.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/annotated_types-0.7.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/pytz-2025.2-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/cycler-0.12.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/weasel-0.4.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/autogluon.tabular-1.3.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/idna-3.10-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/urllib3-2.5.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/mpmath-1.3.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/aiohappyeyeballs-2.6.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/jinja2-3.1.6-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/botocore-1.38.45-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/google_api_core-2.25.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/graphviz-0.21-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/preshed-3.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/ray-2.44.1-cp311-cp311-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/pyasn1_modules-0.4.2-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/six-1.17.0-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/typing_inspection-0.4.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/referencing-0.36.2-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/hyperopt-0.2.7-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n",
      "/kaggle/input/autogluon-package/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/language_data-1.3.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/autogluon.common-1.3.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/threadpoolctl-3.6.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/narwhals-1.44.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/pip-25.1.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/multidict-6.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/autogluon-package/torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl\n",
      "/kaggle/input/autogluon-package/pyparsing-3.2.3-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/rich-14.0.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/certifi-2025.6.15-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/jsonschema-4.24.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/pygments-2.19.2-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/spacy-3.8.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/pydantic-2.11.7-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/attrs-25.3.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/autogluon-package/matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/cloudpickle-3.1.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/plotly-6.2.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/__notebook__.ipynb\n",
      "/kaggle/input/autogluon-package/proto_plus-1.26.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/typing_extensions-4.14.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/s3transfer-0.13.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/catalogue-2.0.10-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/prometheus_client-0.22.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/packaging-25.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/rsa-4.9.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/murmurhash-1.0.13-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/__output__.json\n",
      "/kaggle/input/autogluon-package/spacy_legacy-3.0.12-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/setuptools-80.9.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/boto3-1.38.45-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/typer-0.16.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/huggingface_hub-0.33.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/sympy-1.13.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/mdurl-0.1.2-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/smart_open-7.1.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/colorful-0.5.6-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/opencensus_context-0.1.3-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/wasabi-1.1.3-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/fastprogress-1.0.3-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/aiohttp_cors-0.8.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/tzdata-2025.2-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/autogluon-package/distlib-0.3.9-py2.py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/virtualenv-20.31.2-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/confection-0.1.5-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/autogluon-package/einops-0.8.1-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/markdown_it_py-3.0.0-py3-none-any.whl\n",
      "/kaggle/input/autogluon-package/custom.css\n",
      "/kaggle/input/scikit-package/joblib-1.5.1-py3-none-any.whl\n",
      "/kaggle/input/scikit-package/numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/scikit-package/scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/scikit-package/__results__.html\n",
      "/kaggle/input/scikit-package/scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/scikit-package/threadpoolctl-3.6.0-py3-none-any.whl\n",
      "/kaggle/input/scikit-package/__notebook__.ipynb\n",
      "/kaggle/input/scikit-package/__output__.json\n",
      "/kaggle/input/scikit-package/custom.css\n",
      "/kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/tc-smiles/Tc_SMILES.csv\n",
      "/kaggle/input/smiles-extra-data/data_dnst1.xlsx\n",
      "/kaggle/input/smiles-extra-data/data_tg3.xlsx\n",
      "/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv\n",
      "/kaggle/input/neurips-2025/GNN_v14/submission.csv\n",
      "/kaggle/input/neurips-2025/GNN_v14/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/GNN_v14/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Density_fold9.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tc_fold9.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_FFV_fold8.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Density_fold7.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tc_fold6.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Rg_fold8.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tg_fold5.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Density_fold8.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tc_fold5.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tg_fold6.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tc_fold6.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_FFV_fold6.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_FFV_fold9.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Density_fold7.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tg_fold8.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tg_fold8.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tg_fold6.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tc_fold7.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Density_fold6.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_FFV_fold5.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Density_fold6.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Rg_fold9.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_FFV_fold7.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Density_fold8.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tc_fold9.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tc_fold8.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_FFV_fold7.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tg_fold9.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Rg_fold7.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Rg_fold9.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tg_fold5.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Rg_fold5.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Rg_fold7.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tc_fold8.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tc_fold5.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_FFV_fold8.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tc_fold7.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Rg_fold5.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Rg_fold6.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tg_fold7.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tg_fold7.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Density_fold5.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_FFV_fold5.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_FFV_fold6.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Rg_fold6.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Density_fold5.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_FFV_fold9.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Rg_fold8.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Density_fold9.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tg_fold9.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v14/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/submission.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v19/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/submission.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Density_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tc_fold9.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_FFV_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Density_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tc_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Rg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Density_fold8.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tc_fold5.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tc_fold6.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_FFV_fold6.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_FFV_fold9.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Density_fold7.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tc_fold7.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Density_fold6.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_FFV_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Density_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Rg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_FFV_fold7.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Density_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tc_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tc_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_FFV_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Rg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Rg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Rg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Rg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tc_fold8.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tc_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_FFV_fold8.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tc_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Rg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Rg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Density_fold5.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_FFV_fold5.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_FFV_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Rg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Density_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_FFV_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Rg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Density_fold9.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v32/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/xgb_v3/submission.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/log/permutation_importance_log.xlsx\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/mae_results.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Density/scaler.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Density/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Density/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Density/Density_feature_info.json\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/FFV/scaler.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/FFV/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/FFV/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/FFV/FFV_feature_info.json\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Rg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Rg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Rg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Rg/Rg_feature_info.json\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Tc/scaler.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Tc/Tc_feature_info.json\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Tc/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Tc/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Tg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Tg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Tg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/feature_selection/Tg/Tg_feature_info.json\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Density/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Density/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Density/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Density/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Density/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Density/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Density/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Density/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Density/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Density/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/FFV/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/FFV/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/FFV/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/FFV/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/FFV/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/FFV/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/FFV/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/FFV/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/FFV/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/FFV/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Rg/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Rg/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Rg/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Rg/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Rg/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Rg/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Rg/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Rg/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Rg/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Rg/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tc/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tc/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tc/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tc/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tc/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tc/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tc/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tc/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tc/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tc/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tg/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tg/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tg/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tg/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tg/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tg/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tg/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tg/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tg/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/NeurIPS/fold_residuals/Tg/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Density_fold5_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tc_fold2_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Density_fold3_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/FFV_fold2_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tg_fold4_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Rg_fold4_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Rg_fold7_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Rg_fold6_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tg_fold2_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tg_fold5_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/FFV_fold10_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/FFV_fold3_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Density_fold4_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tc_fold5_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Rg_fold5_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tg_fold8_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tc_fold9_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Density_fold1_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Rg_fold3_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tg_fold7_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/FFV_fold8_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tg_fold10_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/FFV_fold9_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tc_fold3_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tc_fold8_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Density_fold8_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Density_fold10_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tg_fold9_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/FFV_fold5_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tc_fold4_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tg_fold1_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tc_fold10_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/FFV_fold7_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Rg_fold2_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tc_fold6_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/FFV_fold1_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Rg_fold1_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tc_fold1_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/FFV_fold6_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Density_fold2_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tg_fold6_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Rg_fold8_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/FFV_fold4_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Density_fold9_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Density_fold7_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Rg_fold10_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tg_fold3_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Rg_fold9_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Tc_fold7_xgb.joblib\n",
      "/kaggle/input/neurips-2025/xgb_v3/models/Density_fold6_xgb.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/submission.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Density_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tc_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_FFV_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Density_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tc_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Rg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Density_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tc_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tc_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_FFV_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_FFV_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Density_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tc_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Density_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_FFV_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Density_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Rg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_FFV_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Density_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tc_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tc_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_FFV_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Rg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Rg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Rg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Rg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tc_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tc_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_FFV_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tc_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Rg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Rg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Density_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_FFV_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_FFV_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Rg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Density_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_FFV_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Rg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Density_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_20/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/submission.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Density_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tc_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_FFV_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Density_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tc_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Rg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Density_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tc_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tc_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_FFV_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_FFV_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Density_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tc_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Density_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_FFV_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Density_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Rg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_FFV_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Density_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tc_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tc_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_FFV_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Rg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Rg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Rg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Rg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tc_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tc_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_FFV_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tc_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Rg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Rg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Density_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_FFV_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_FFV_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Rg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Density_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_FFV_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Rg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Density_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v38/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/submission.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v9/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/cat_v5/submission.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/log/permutation_importance_log.xlsx\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/mae_results.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Density/scaler.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Density/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Density/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Density/Density_feature_info.json\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/FFV/scaler.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/FFV/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/FFV/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/FFV/FFV_feature_info.json\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Rg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Rg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Rg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Rg/Rg_feature_info.json\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Tc/scaler.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Tc/Tc_feature_info.json\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Tc/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Tc/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Tg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Tg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Tg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/feature_selection/Tg/Tg_feature_info.json\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Density/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Density/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Density/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Density/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Density/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Density/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Density/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Density/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Density/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Density/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/FFV/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/FFV/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/FFV/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/FFV/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/FFV/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/FFV/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/FFV/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/FFV/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/FFV/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/FFV/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Rg/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Rg/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Rg/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Rg/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Rg/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Rg/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Rg/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Rg/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Rg/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Rg/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tc/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tc/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tc/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tc/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tc/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tc/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tc/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tc/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tc/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tc/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tg/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tg/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tg/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tg/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tg/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tg/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tg/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tg/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tg/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/NeurIPS/fold_residuals/Tg/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tc_fold9_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/FFV_fold8_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tc_fold6_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/FFV_fold4_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tg_fold9_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/FFV_fold3_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Density_fold8_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Rg_fold10_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Density_fold9_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Density_fold5_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tc_fold7_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/FFV_fold5_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tc_fold5_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Rg_fold6_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tg_fold1_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Density_fold10_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Rg_fold2_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tc_fold2_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Rg_fold5_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Density_fold3_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Density_fold4_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Rg_fold3_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tc_fold3_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Density_fold7_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/FFV_fold1_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/FFV_fold9_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tg_fold5_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/FFV_fold6_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tc_fold1_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/FFV_fold10_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tc_fold8_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tg_fold3_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tg_fold2_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Density_fold1_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tg_fold10_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Rg_fold1_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Rg_fold9_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/FFV_fold7_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Density_fold2_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tg_fold7_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Rg_fold7_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tc_fold4_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Rg_fold4_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Density_fold6_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tg_fold8_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tc_fold10_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tg_fold4_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/FFV_fold2_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Rg_fold8_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/models/Tg_fold6_catboost.joblib\n",
      "/kaggle/input/neurips-2025/cat_v5/catboost_info/test_error.tsv\n",
      "/kaggle/input/neurips-2025/cat_v5/catboost_info/learn_error.tsv\n",
      "/kaggle/input/neurips-2025/cat_v5/catboost_info/catboost_training.json\n",
      "/kaggle/input/neurips-2025/cat_v5/catboost_info/time_left.tsv\n",
      "/kaggle/input/neurips-2025/cat_v5/catboost_info/learn/events.out.tfevents\n",
      "/kaggle/input/neurips-2025/cat_v5/catboost_info/test/events.out.tfevents\n",
      "/kaggle/input/neurips-2025/stacking_v166/submission.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/mae_results.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Density/scaler.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Density/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Density/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Density/Density_feature_info.json\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/FFV/scaler.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/FFV/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/FFV/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/FFV/FFV_feature_info.json\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Rg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Rg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Rg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Rg/Rg_feature_info.json\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Tc/scaler.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Tc/Tc_feature_info.json\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Tc/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Tc/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Tg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Tg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Tg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v166/NeurIPS/feature_selection/Tg/Tg_feature_info.json\n",
      "/kaggle/input/neurips-2025/stacking_v166/models/Density_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v166/models/Tg_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v166/models/Rg_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v166/models/Tc_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v166/models/FFV_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v166/catboost_info/learn_error.tsv\n",
      "/kaggle/input/neurips-2025/stacking_v166/catboost_info/catboost_training.json\n",
      "/kaggle/input/neurips-2025/stacking_v166/catboost_info/time_left.tsv\n",
      "/kaggle/input/neurips-2025/stacking_v166/catboost_info/learn/events.out.tfevents\n",
      "/kaggle/input/neurips-2025/GATConv_v29/submission.csv\n",
      "/kaggle/input/neurips-2025/GATConv_v29/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/GATConv_v29/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold9.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tc_fold9.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold8.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold7.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold6.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Rg_fold8.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold5.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Density_fold8.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tc_fold5.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tg_fold6.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tc_fold6.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_FFV_fold6.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_FFV_fold9.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Density_fold7.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold8.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tg_fold8.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold6.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tc_fold7.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Density_fold6.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold5.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold6.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Rg_fold9.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_FFV_fold7.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold8.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold9.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold8.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold7.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tg_fold9.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold7.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold9.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tg_fold5.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold5.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Rg_fold7.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tc_fold8.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold5.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_FFV_fold8.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold7.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Rg_fold5.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold6.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold7.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tg_fold7.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Density_fold5.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_FFV_fold5.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold6.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Rg_fold6.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold5.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold9.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold8.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Density_fold9.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold9.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/submission.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v20/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/submission.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Density_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tc_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_FFV_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Density_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tc_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Rg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Density_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tc_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tc_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_FFV_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_FFV_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Density_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tc_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Density_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_FFV_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Density_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Rg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_FFV_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Density_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tc_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tc_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_FFV_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Rg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Rg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Rg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Rg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tc_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tc_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_FFV_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tc_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Rg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Rg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Density_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_FFV_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_FFV_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Rg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Density_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_FFV_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Rg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Density_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_v40/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/lgbm_v6/submission.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/log/permutation_importance_log.xlsx\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/mae_results.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Density/scaler.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Density/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Density/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Density/Density_feature_info.json\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/FFV/scaler.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/FFV/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/FFV/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/FFV/FFV_feature_info.json\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Rg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Rg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Rg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Rg/Rg_feature_info.json\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Tc/scaler.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Tc/Tc_feature_info.json\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Tc/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Tc/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Tg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Tg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Tg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/feature_selection/Tg/Tg_feature_info.json\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Density/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Density/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Density/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Density/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Density/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Density/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Density/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Density/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Density/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Density/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/FFV/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/FFV/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/FFV/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/FFV/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/FFV/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/FFV/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/FFV/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/FFV/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/FFV/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/FFV/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Rg/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Rg/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Rg/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Rg/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Rg/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Rg/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Rg/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Rg/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Rg/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Rg/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tc/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tc/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tc/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tc/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tc/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tc/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tc/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tc/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tc/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tc/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tg/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tg/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tg/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tg/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tg/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tg/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tg/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tg/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tg/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/NeurIPS/fold_residuals/Tg/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Density_fold6_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/FFV_fold3_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Density_fold9_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tc_fold6_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/FFV_fold2_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Rg_fold4_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/FFV_fold7_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Rg_fold10_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Density_fold4_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tc_fold2_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/FFV_fold10_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/FFV_fold6_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tc_fold3_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Rg_fold9_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tg_fold2_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Rg_fold8_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Rg_fold6_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tg_fold7_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Density_fold2_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tg_fold10_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Density_fold8_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Density_fold3_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tg_fold9_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tg_fold6_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tc_fold1_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tc_fold8_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tg_fold8_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tg_fold1_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tc_fold7_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/FFV_fold1_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tc_fold9_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tg_fold3_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tc_fold4_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tg_fold4_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tg_fold5_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Density_fold1_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Rg_fold5_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/FFV_fold9_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Density_fold7_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/FFV_fold4_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/FFV_fold8_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tc_fold5_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Rg_fold3_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Density_fold10_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Tc_fold10_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/FFV_fold5_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Density_fold5_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Rg_fold1_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Rg_fold7_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/lgbm_v6/models/Rg_fold2_lgbm.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/submission.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/log/permutation_importance_log.xlsx\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/mae_results.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Density/scaler.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Density/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Density/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Density/Density_feature_info.json\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/FFV/scaler.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/FFV/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/FFV/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/FFV/FFV_feature_info.json\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Rg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Rg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Rg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Rg/Rg_feature_info.json\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Tc/scaler.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Tc/Tc_feature_info.json\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Tc/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Tc/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Tg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Tg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Tg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/feature_selection/Tg/Tg_feature_info.json\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Density/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Density/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Density/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Density/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Density/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Density/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Density/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Density/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Density/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Density/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/FFV/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/FFV/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/FFV/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/FFV/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/FFV/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/FFV/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/FFV/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/FFV/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/FFV/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/FFV/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Rg/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Rg/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Rg/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Rg/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Rg/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Rg/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Rg/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Rg/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Rg/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Rg/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tc/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tc/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tc/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tc/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tc/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tc/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tc/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tc/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tc/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tc/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tg/fold_7_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tg/fold_5_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tg/fold_9_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tg/fold_8_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tg/fold_2_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tg/fold_1_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tg/fold_10_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tg/fold_3_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tg/fold_6_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/NeurIPS/fold_residuals/Tg/fold_4_val_pred_residuals.csv\n",
      "/kaggle/input/neurips-2025/nn_v4/models/FFV_fold8_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tg_fold8_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tc_fold8_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Rg_fold10_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tg_fold4_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tg_fold7_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/FFV_fold5_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Density_fold8_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tg_fold9_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Density_fold7_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tc_fold3_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Rg_fold4_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Rg_fold7_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tc_fold4_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Density_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/FFV_fold1_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/FFV_fold9_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Rg_fold1_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Rg_fold5_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tg_fold3_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tc_fold6_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Density_fold1_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/FFV_fold3_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Rg_fold6_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/FFV_fold10_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tg_fold10_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tg_fold6_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Rg_fold9_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Density_fold6_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tc_fold10_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/FFV_fold4_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tc_fold5_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tc_fold7_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Density_fold5_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tg_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Rg_fold8_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Density_fold9_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Density_fold4_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tc_fold1_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Rg_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Density_fold10_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Density_fold3_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tg_fold1_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/FFV_fold7_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Rg_fold3_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tc_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tg_fold5_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/FFV_fold6_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/FFV_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/nn_v4/models/Tc_fold9_nn.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/submission.csv\n",
      "/kaggle/input/neurips-2025/GNN_v13/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/GNN_v13/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Density_fold9.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tc_fold9.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_FFV_fold8.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Density_fold7.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tc_fold6.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Rg_fold8.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tg_fold5.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Density_fold8.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tc_fold5.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tg_fold6.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tc_fold6.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_FFV_fold6.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_FFV_fold9.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Density_fold7.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tg_fold8.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tg_fold8.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tg_fold6.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tc_fold7.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Density_fold6.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_FFV_fold5.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Density_fold6.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Rg_fold9.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_FFV_fold7.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Density_fold8.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tc_fold9.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tc_fold8.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_FFV_fold7.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tg_fold9.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Rg_fold7.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Rg_fold9.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tg_fold5.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Rg_fold5.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Rg_fold7.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tc_fold8.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tc_fold5.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_FFV_fold8.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tc_fold7.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Rg_fold5.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Rg_fold6.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tg_fold7.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tg_fold7.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Density_fold5.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_FFV_fold5.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_FFV_fold6.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Rg_fold6.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Density_fold5.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_FFV_fold9.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Rg_fold8.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Density_fold9.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tg_fold9.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/GNN_v13/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/submission.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Density_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tc_fold9.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_FFV_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Density_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tc_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Rg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Density_fold8.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tc_fold5.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tc_fold6.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_FFV_fold6.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_FFV_fold9.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Density_fold7.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tc_fold7.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Density_fold6.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_FFV_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Density_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Rg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_FFV_fold7.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Density_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tc_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tc_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_FFV_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Rg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Rg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Rg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Rg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tc_fold8.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tc_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_FFV_fold8.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tc_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Rg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Rg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Density_fold5.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_FFV_fold5.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_FFV_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Rg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Density_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_FFV_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Rg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Density_fold9.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v28/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/submission.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnn_gh_v1/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/submission.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/submission_hybrid_gnn_final.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/gnn_hybrid_cv_mae_results.csv\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Density_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tc_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_FFV_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_FFV_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_yscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Density_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tc_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Rg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Rg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tc_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_yscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tc_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_FFV_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_FFV_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Density_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_uscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_FFV_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Density_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Density_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_yscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tc_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tc_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_FFV_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_FFV_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Density_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tc_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tg_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tc_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Density_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_FFV_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tc_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Density_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Rg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_FFV_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Density_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_FFV_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tc_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tc_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_FFV_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tg_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Rg_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Rg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_xscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Rg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Rg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Density_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Rg_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_FFV_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Rg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_yscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_xscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Density_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tc_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Density_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tc_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Rg_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tc_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tc_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_FFV_fold8.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tc_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_uscaler_Tc.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Density_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Rg_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Rg_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tc_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Density_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tg_fold7.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_FFV_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tc_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tg_fold7.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Rg_fold4.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_xscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Density_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_FFV_fold5.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Rg_fold1.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_FFV_fold6.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tg_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_FFV_fold0.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Rg_fold6.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Rg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_FFV_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Rg_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tg_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_uscaler_Rg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Density_fold5.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_yscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Rg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tc_fold3.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_FFV_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_xscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_xscaler_Tg.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tc_fold4.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_FFV_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Rg_fold8.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Rg_fold2.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_uscaler_FFV.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Density_fold9.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Tg_fold0.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_config_Density_fold1.json\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tg_fold9.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Density_fold2.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Tg_fold3.pth\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_uscaler_Density.joblib\n",
      "/kaggle/input/neurips-2025/gnh_gh_sv_19/models/gnn/gnn_model_Density_fold3.pth\n",
      "/kaggle/input/neurips-2025/stacking_v165/submission.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/mae_results.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Density/scaler.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Density/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Density/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Density/Density_feature_info.json\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/FFV/scaler.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/FFV/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/FFV/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/FFV/FFV_feature_info.json\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Rg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Rg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Rg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Rg/Rg_feature_info.json\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Tc/scaler.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Tc/Tc_feature_info.json\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Tc/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Tc/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Tg/scaler.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Tg/y_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Tg/X_holdout.csv\n",
      "/kaggle/input/neurips-2025/stacking_v165/NeurIPS/feature_selection/Tg/Tg_feature_info.json\n",
      "/kaggle/input/neurips-2025/stacking_v165/models/Density_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v165/models/Tg_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v165/models/Rg_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v165/models/Tc_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v165/models/FFV_fold2_nn.joblib\n",
      "/kaggle/input/neurips-2025/stacking_v165/catboost_info/learn_error.tsv\n",
      "/kaggle/input/neurips-2025/stacking_v165/catboost_info/catboost_training.json\n",
      "/kaggle/input/neurips-2025/stacking_v165/catboost_info/time_left.tsv\n",
      "/kaggle/input/neurips-2025/stacking_v165/catboost_info/learn/events.out.tfevents\n",
      "/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM/config.json\n",
      "/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM/merges.txt\n",
      "/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM/tokenizer.json\n",
      "/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM/vocab.json\n",
      "/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM/tokenizer_config.json\n",
      "/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM/model.safetensors\n",
      "/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM/special_tokens_map.json\n",
      "/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM/added_tokens.json\n",
      "/kaggle/input/neuripsbl/tc-smiles/Tc_SMILES.csv\n",
      "/kaggle/input/neuripsbl/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/neuripsbl/smiles-extra-data/data_dnst1.xlsx\n",
      "/kaggle/input/neuripsbl/smiles-extra-data/data_tg3.xlsx\n",
      "/kaggle/input/neuripsbl/smiles-extra-data/JCIM_sup_bigsmiles.csv\n",
      "Code B running\n",
      "Code C running\n",
      "Code D running\n",
      "[run_code_d1] train rows=7973  test rows=3  train_extended rows=10080\n",
      "Code E running\n",
      "Code F running\n",
      "Code G running\n",
      "Code H running\n",
      "code I running\n",
      "[CUDA available] True | XGBoost (2, 0)\n",
      "[GPU0 start] Tesla P100-PCIE-16GB | 989/16384 MiB | util 0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sklearnex: sklearn.utils.validation._assert_all_finite: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.utils.validation._assert_all_finite: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.cluster.KMeans.fit: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.model_selection.train_test_split: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.model_selection.train_test_split: running accelerated version on CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NVML Tg iter 0] util 0% | 1136/16384 MiB\n",
      "[NVML Tg iter 50] util 53% | 1136/16384 MiB\n",
      "[NVML Tg iter 100] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 150] util 45% | 1136/16384 MiB\n",
      "[NVML Tg iter 200] util 45% | 1136/16384 MiB\n",
      "[NVML Tg iter 250] util 44% | 1136/16384 MiB\n",
      "[NVML Tg iter 300] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 350] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 400] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 450] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 500] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 550] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 600] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 650] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 700] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 750] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 800] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 850] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 900] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 950] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1000] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1050] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1100] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1150] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 1200] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1250] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 1300] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 1350] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1400] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1450] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1500] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1550] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1600] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1650] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 1700] util 45% | 1136/16384 MiB\n",
      "[NVML Tg iter 1750] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 1800] util 46% | 1136/16384 MiB\n",
      "[NVML Tg iter 1850] util 48% | 1136/16384 MiB\n",
      "[NVML Tg iter 1900] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 1950] util 46% | 1136/16384 MiB\n",
      "[NVML Tg iter 2000] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 2050] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 2100] util 47% | 1136/16384 MiB\n",
      "[NVML Tg iter 2150] util 48% | 1136/16384 MiB\n",
      "[Tg] booster config snippet: cuda\n",
      "[Tg] Holdout MAE: 20.642329\n",
      "[NVML Tg iter 0] util 22% | 1136/16384 MiB\n",
      "[NVML Tg iter 50] util 35% | 1136/16384 MiB\n",
      "[NVML Tg iter 100] util 61% | 1136/16384 MiB\n",
      "[NVML Tg iter 150] util 59% | 1136/16384 MiB\n",
      "[NVML Tg iter 200] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 250] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 300] util 59% | 1136/16384 MiB\n",
      "[NVML Tg iter 350] util 59% | 1136/16384 MiB\n",
      "[NVML Tg iter 400] util 59% | 1136/16384 MiB\n",
      "[NVML Tg iter 450] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 500] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 550] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 600] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 650] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 700] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 750] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 800] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 850] util 59% | 1136/16384 MiB\n",
      "[NVML Tg iter 900] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 950] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 1000] util 53% | 1136/16384 MiB\n",
      "[NVML Tg iter 1050] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 1100] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 1150] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1200] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1250] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1300] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 1350] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1400] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1450] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1500] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1550] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 1600] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1650] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1700] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1750] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1800] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1850] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1900] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 1950] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 2000] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 2050] util 58% | 1136/16384 MiB\n",
      "[NVML Tg iter 2100] util 57% | 1136/16384 MiB\n",
      "[NVML Tg iter 2150] util 58% | 1136/16384 MiB\n",
      "[NVML end Tg] util 13% | 1102/16384 MiB\n",
      "[GPU0 end Tg] Tesla P100-PCIE-16GB | 989/16384 MiB | util 13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sklearnex: sklearn.utils.validation._assert_all_finite: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.utils.validation._assert_all_finite: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.cluster.KMeans.fit: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.model_selection.train_test_split: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.model_selection.train_test_split: running accelerated version on CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NVML FFV iter 0] util 0% | 1150/16384 MiB\n",
      "[NVML FFV iter 50] util 43% | 1150/16384 MiB\n",
      "[NVML FFV iter 100] util 43% | 1150/16384 MiB\n",
      "[NVML FFV iter 150] util 40% | 1150/16384 MiB\n",
      "[NVML FFV iter 200] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 250] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 300] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 350] util 40% | 1150/16384 MiB\n",
      "[NVML FFV iter 400] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 450] util 40% | 1150/16384 MiB\n",
      "[NVML FFV iter 500] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 550] util 40% | 1150/16384 MiB\n",
      "[NVML FFV iter 600] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 650] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 700] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 750] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 800] util 40% | 1150/16384 MiB\n",
      "[NVML FFV iter 850] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 900] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 950] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1000] util 36% | 1150/16384 MiB\n",
      "[NVML FFV iter 1050] util 40% | 1150/16384 MiB\n",
      "[NVML FFV iter 1100] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1150] util 40% | 1150/16384 MiB\n",
      "[NVML FFV iter 1200] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1250] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1300] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1350] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1400] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1450] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1500] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1550] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1600] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1650] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1700] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1750] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1800] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1850] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1900] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 1950] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 2000] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 2050] util 38% | 1150/16384 MiB\n",
      "[NVML FFV iter 2100] util 38% | 1150/16384 MiB\n",
      "[NVML FFV iter 2150] util 39% | 1150/16384 MiB\n",
      "[NVML FFV iter 2200] util 40% | 1150/16384 MiB\n",
      "[FFV] booster config snippet: cuda\n",
      "[FFV] Holdout MAE: 0.003456\n",
      "[NVML FFV iter 0] util 0% | 1148/16384 MiB\n",
      "[NVML FFV iter 50] util 31% | 1148/16384 MiB\n",
      "[NVML FFV iter 100] util 31% | 1148/16384 MiB\n",
      "[NVML FFV iter 150] util 52% | 1148/16384 MiB\n",
      "[NVML FFV iter 200] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 250] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 300] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 350] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 400] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 450] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 500] util 52% | 1148/16384 MiB\n",
      "[NVML FFV iter 550] util 52% | 1148/16384 MiB\n",
      "[NVML FFV iter 600] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 650] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 700] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 750] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 800] util 52% | 1148/16384 MiB\n",
      "[NVML FFV iter 850] util 52% | 1148/16384 MiB\n",
      "[NVML FFV iter 900] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 950] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1000] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1050] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1100] util 52% | 1148/16384 MiB\n",
      "[NVML FFV iter 1150] util 52% | 1148/16384 MiB\n",
      "[NVML FFV iter 1200] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1250] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1300] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1350] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1400] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1450] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1500] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1550] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1600] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1650] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1700] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1750] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1800] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1850] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1900] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 1950] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 2000] util 52% | 1148/16384 MiB\n",
      "[NVML FFV iter 2050] util 52% | 1148/16384 MiB\n",
      "[NVML FFV iter 2100] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 2150] util 51% | 1148/16384 MiB\n",
      "[NVML FFV iter 2200] util 51% | 1148/16384 MiB\n",
      "[NVML end FFV] util 43% | 1104/16384 MiB\n",
      "[GPU0 end FFV] Tesla P100-PCIE-16GB | 991/16384 MiB | util 43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sklearnex: sklearn.utils.validation._assert_all_finite: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.utils.validation._assert_all_finite: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.cluster.KMeans.fit: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.model_selection.train_test_split: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.model_selection.train_test_split: running accelerated version on CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NVML Tc iter 0] util 0% | 1136/16384 MiB\n",
      "[NVML Tc iter 50] util 40% | 1136/16384 MiB\n",
      "[NVML Tc iter 100] util 40% | 1136/16384 MiB\n",
      "[NVML Tc iter 150] util 43% | 1136/16384 MiB\n",
      "[NVML Tc iter 200] util 40% | 1136/16384 MiB\n",
      "[NVML Tc iter 250] util 41% | 1136/16384 MiB\n",
      "[NVML Tc iter 300] util 40% | 1136/16384 MiB\n",
      "[NVML Tc iter 350] util 41% | 1136/16384 MiB\n",
      "[NVML Tc iter 400] util 40% | 1136/16384 MiB\n",
      "[NVML Tc iter 450] util 40% | 1136/16384 MiB\n",
      "[NVML Tc iter 500] util 40% | 1136/16384 MiB\n",
      "[NVML Tc iter 550] util 40% | 1136/16384 MiB\n",
      "[NVML Tc iter 600] util 40% | 1136/16384 MiB\n",
      "[NVML Tc iter 650] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 700] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 750] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 800] util 38% | 1136/16384 MiB\n",
      "[NVML Tc iter 850] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 900] util 38% | 1136/16384 MiB\n",
      "[NVML Tc iter 950] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 1000] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 1050] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 1100] util 40% | 1136/16384 MiB\n",
      "[NVML Tc iter 1150] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 1200] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 1250] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 1300] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 1350] util 39% | 1136/16384 MiB\n",
      "[NVML Tc iter 1400] util 38% | 1136/16384 MiB\n",
      "[NVML Tc iter 1450] util 38% | 1136/16384 MiB\n",
      "[Tc] booster config snippet: cuda\n",
      "[Tc] Holdout MAE: 0.027705\n",
      "[NVML Tc iter 0] util 38% | 1136/16384 MiB\n",
      "[NVML Tc iter 50] util 19% | 1136/16384 MiB\n",
      "[NVML Tc iter 100] util 52% | 1136/16384 MiB\n",
      "[NVML Tc iter 150] util 52% | 1136/16384 MiB\n",
      "[NVML Tc iter 200] util 52% | 1136/16384 MiB\n",
      "[NVML Tc iter 250] util 52% | 1136/16384 MiB\n",
      "[NVML Tc iter 300] util 52% | 1136/16384 MiB\n",
      "[NVML Tc iter 350] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 400] util 52% | 1136/16384 MiB\n",
      "[NVML Tc iter 450] util 52% | 1136/16384 MiB\n",
      "[NVML Tc iter 500] util 52% | 1136/16384 MiB\n",
      "[NVML Tc iter 550] util 52% | 1136/16384 MiB\n",
      "[NVML Tc iter 600] util 50% | 1136/16384 MiB\n",
      "[NVML Tc iter 650] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 700] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 750] util 50% | 1136/16384 MiB\n",
      "[NVML Tc iter 800] util 50% | 1136/16384 MiB\n",
      "[NVML Tc iter 850] util 50% | 1136/16384 MiB\n",
      "[NVML Tc iter 900] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 950] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 1000] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 1050] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 1100] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 1150] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 1200] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 1250] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 1300] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 1350] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 1400] util 51% | 1136/16384 MiB\n",
      "[NVML Tc iter 1450] util 51% | 1136/16384 MiB\n",
      "[NVML end Tc] util 18% | 1104/16384 MiB\n",
      "[GPU0 end Tc] Tesla P100-PCIE-16GB | 991/16384 MiB | util 18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sklearnex: sklearn.utils.validation._assert_all_finite: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.utils.validation._assert_all_finite: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.cluster.KMeans.fit: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.model_selection.train_test_split: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.model_selection.train_test_split: running accelerated version on CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NVML Density iter 0] util 0% | 1138/16384 MiB\n",
      "[NVML Density iter 50] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 100] util 42% | 1138/16384 MiB\n",
      "[NVML Density iter 150] util 40% | 1138/16384 MiB\n",
      "[NVML Density iter 200] util 40% | 1138/16384 MiB\n",
      "[NVML Density iter 250] util 39% | 1138/16384 MiB\n",
      "[NVML Density iter 300] util 39% | 1138/16384 MiB\n",
      "[NVML Density iter 350] util 39% | 1138/16384 MiB\n",
      "[NVML Density iter 400] util 40% | 1138/16384 MiB\n",
      "[NVML Density iter 450] util 39% | 1138/16384 MiB\n",
      "[NVML Density iter 500] util 39% | 1138/16384 MiB\n",
      "[NVML Density iter 550] util 39% | 1138/16384 MiB\n",
      "[NVML Density iter 600] util 40% | 1138/16384 MiB\n",
      "[NVML Density iter 650] util 40% | 1138/16384 MiB\n",
      "[NVML Density iter 700] util 40% | 1138/16384 MiB\n",
      "[NVML Density iter 750] util 39% | 1138/16384 MiB\n",
      "[NVML Density iter 800] util 39% | 1138/16384 MiB\n",
      "[NVML Density iter 850] util 34% | 1138/16384 MiB\n",
      "[NVML Density iter 900] util 34% | 1138/16384 MiB\n",
      "[NVML Density iter 950] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1000] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1050] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1100] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1150] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1200] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1250] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1300] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1350] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1400] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1450] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1500] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1550] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1600] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1650] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1700] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1750] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1800] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1850] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1900] util 18% | 1138/16384 MiB\n",
      "[NVML Density iter 1950] util 18% | 1138/16384 MiB\n",
      "[Density] booster config snippet: cuda\n",
      "[Density] Holdout MAE: 0.033883\n",
      "[NVML Density iter 0] util 14% | 1138/16384 MiB\n",
      "[NVML Density iter 50] util 14% | 1138/16384 MiB\n",
      "[NVML Density iter 100] util 45% | 1138/16384 MiB\n",
      "[NVML Density iter 150] util 50% | 1138/16384 MiB\n",
      "[NVML Density iter 200] util 50% | 1138/16384 MiB\n",
      "[NVML Density iter 250] util 49% | 1138/16384 MiB\n",
      "[NVML Density iter 300] util 49% | 1138/16384 MiB\n",
      "[NVML Density iter 350] util 50% | 1138/16384 MiB\n",
      "[NVML Density iter 400] util 48% | 1138/16384 MiB\n",
      "[NVML Density iter 450] util 48% | 1138/16384 MiB\n",
      "[NVML Density iter 500] util 49% | 1138/16384 MiB\n",
      "[NVML Density iter 550] util 50% | 1138/16384 MiB\n",
      "[NVML Density iter 600] util 50% | 1138/16384 MiB\n",
      "[NVML Density iter 650] util 50% | 1138/16384 MiB\n",
      "[NVML Density iter 700] util 50% | 1138/16384 MiB\n",
      "[NVML Density iter 750] util 49% | 1138/16384 MiB\n",
      "[NVML Density iter 800] util 50% | 1138/16384 MiB\n",
      "[NVML Density iter 850] util 50% | 1138/16384 MiB\n",
      "[NVML Density iter 900] util 50% | 1138/16384 MiB\n",
      "[NVML Density iter 950] util 42% | 1138/16384 MiB\n",
      "[NVML Density iter 1000] util 42% | 1138/16384 MiB\n",
      "[NVML Density iter 1050] util 42% | 1138/16384 MiB\n",
      "[NVML Density iter 1100] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1150] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1200] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1250] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1300] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1350] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1400] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1450] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1500] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1550] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1600] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1650] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1700] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1750] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1800] util 25% | 1138/16384 MiB\n",
      "[NVML Density iter 1850] util 26% | 1138/16384 MiB\n",
      "[NVML Density iter 1900] util 26% | 1138/16384 MiB\n",
      "[NVML Density iter 1950] util 26% | 1138/16384 MiB\n",
      "[NVML end Density] util 17% | 1104/16384 MiB\n",
      "[GPU0 end Density] Tesla P100-PCIE-16GB | 991/16384 MiB | util 17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sklearnex: sklearn.utils.validation._assert_all_finite: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.utils.validation._assert_all_finite: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.cluster.KMeans.fit: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.model_selection.train_test_split: running accelerated version on CPU\n",
      "INFO:sklearnex: sklearn.model_selection.train_test_split: running accelerated version on CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NVML Rg iter 0] util 1% | 1136/16384 MiB\n",
      "[NVML Rg iter 50] util 1% | 1136/16384 MiB\n",
      "[NVML Rg iter 100] util 41% | 1136/16384 MiB\n",
      "[NVML Rg iter 150] util 39% | 1136/16384 MiB\n",
      "[NVML Rg iter 200] util 39% | 1136/16384 MiB\n",
      "[NVML Rg iter 250] util 39% | 1136/16384 MiB\n",
      "[NVML Rg iter 300] util 39% | 1136/16384 MiB\n",
      "[NVML Rg iter 350] util 39% | 1136/16384 MiB\n",
      "[NVML Rg iter 400] util 39% | 1136/16384 MiB\n",
      "[NVML Rg iter 450] util 39% | 1136/16384 MiB\n",
      "[NVML Rg iter 500] util 40% | 1136/16384 MiB\n",
      "[Rg] booster config snippet: cuda\n",
      "[Rg] Holdout MAE: 1.207762\n",
      "[NVML Rg iter 0] util 32% | 1136/16384 MiB\n",
      "[NVML Rg iter 50] util 32% | 1136/16384 MiB\n",
      "[NVML Rg iter 100] util 32% | 1136/16384 MiB\n",
      "[NVML Rg iter 150] util 51% | 1136/16384 MiB\n",
      "[NVML Rg iter 200] util 51% | 1136/16384 MiB\n",
      "[NVML Rg iter 250] util 51% | 1136/16384 MiB\n",
      "[NVML Rg iter 300] util 50% | 1136/16384 MiB\n",
      "[NVML Rg iter 350] util 50% | 1136/16384 MiB\n",
      "[NVML Rg iter 400] util 50% | 1136/16384 MiB\n",
      "[NVML Rg iter 450] util 51% | 1136/16384 MiB\n",
      "[NVML Rg iter 500] util 51% | 1136/16384 MiB\n",
      "[NVML end Rg] util 51% | 1104/16384 MiB\n",
      "[GPU0 end Rg] Tesla P100-PCIE-16GB | 991/16384 MiB | util 51%\n",
      "\n",
      "[Code e FAST] ✅ predictions saved to submission_code_e.csv\n",
      "\n",
      "[Code e FAST] Submission preview:\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  167.167419  0.374778  0.164619  1.129673  22.822590\n",
      "1  1422188626  184.163116  0.376791  0.250968  1.121777  21.160454\n",
      "2  2032016830   84.946556  0.350604  0.224062  1.105318  19.606674\n",
      "[Code e FAST] Finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Code A\n",
    "# ----------------------------\n",
    "def run_code_a():\n",
    "    \"\"\"Wrapped Code A execution\"\"\"\n",
    "    # ===============================================================\n",
    "    # FULL CONTENT OF CODE A GOES HERE\n",
    "    # (all functions, model training, data prep, etc.)\n",
    "    # Deduplicated imports and globals already handled above.\n",
    "    # ===============================================================\n",
    "    print(\"[Code A] Starting...\")\n",
    "\n",
    "    import glob\n",
    "    import os\n",
    "    import time\n",
    "    import random\n",
    "    import json\n",
    "    import hashlib\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import networkx as nx\n",
    "\n",
    "    from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, StackingRegressor\n",
    "    from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.model_selection import train_test_split, KFold\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "\n",
    "    from xgboost import XGBRegressor\n",
    "    from catboost import CatBoostRegressor\n",
    "    from lightgbm import LGBMRegressor\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, MACCSkeys, rdmolops, Lipinski, Crippen\n",
    "    from rdkit.Chem.rdMolDescriptors import CalcNumRotatableBonds\n",
    "    from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\n",
    "\n",
    "    from mordred import Calculator, descriptors as mordred_descriptors\n",
    "\n",
    "    import shap\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "    # torchinfo is optional, only used in show_model_summary\n",
    "    try:\n",
    "        from torchinfo import summary\n",
    "    except ImportError:\n",
    "        summary = None\n",
    "\n",
    "    # Data paths\n",
    "    RDKIT_AVAILABLE = True\n",
    "    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "    # Create the NeurIPS directory if it does not exist\n",
    "    os.makedirs(\"NeurIPS\", exist_ok=True)\n",
    "    class Config:\n",
    "        useAllDataForTraining = False\n",
    "        use_standard_scaler = True  # Set to True to use StandardScaler, False to skip scaling\n",
    "        # Set to True to calculate Mordred descriptors in featurization\n",
    "        use_least_important_features_all_methods = True  # Set to True to call get_least_important_features_all_methods\n",
    "        use_variance_threshold = False  # Set to True to enable VarianceThreshold feature selection\n",
    "        enable_param_tuning = True  # Set to True to enable XGB hyperparameter tuning\n",
    "        debug = False\n",
    "\n",
    "        use_descriptors = False  # Set to True to include RDKit descriptors, False to skip\n",
    "        use_mordred = True\n",
    "        # Control inclusion of AtomPair and TopologicalTorsion fingerprints\n",
    "        use_maccs_fp = False\n",
    "        use_morgan_fp = False\n",
    "        use_atom_pair_fp = False\n",
    "        use_torsion_fp = False\n",
    "        use_chemberta = False\n",
    "        chemberta_pooling = 'max'  # can be 'mean', 'max', 'cls', or 'pooler'\n",
    "        # Include MACCS keys fingerprint\n",
    "\n",
    "        search_nn = True\n",
    "        use_stacking = False\n",
    "        model_name = 'xgb'  \n",
    "        # Options: ['autogluon', xgb', 'catboost', 'lgbm', 'extratrees', 'randomforest', 'tabnet', 'hgbm', 'nn']\n",
    "\n",
    "        # Don't change\n",
    "        # Choose importance method: 'feature_importances_' or 'permutation_importance'\n",
    "        feature_importance_method = 'permutation_importance'\n",
    "        use_cross_validation = True  # Set to False to use a single split for speed\n",
    "        use_pca = False  # Set to True to enable PCA\n",
    "        pca_variance = 0.9999  # Fraction of variance to keep in PCA\n",
    "        use_external_data = True  # Set to True to use external datasets\n",
    "        use_augmentation = False  # Set to True to use augment_dataset\n",
    "        add_gaussian = False  # Set to True to enable Gaussian Mixture Model-based augmentation\n",
    "        random_state = 42\n",
    "\n",
    "\n",
    "        # Number of least important features to drop\n",
    "        # Use a nested dictionary for model- and label-specific n_least_important_features\n",
    "        n_least_important_features = {\n",
    "            'xgb':     {'Tg': 20, 'FFV': 20, 'Tc': 22, 'Density': 19, 'Rg': 19},\n",
    "            'catboost':{'Tg': 15, 'FFV': 15, 'Tc': 18, 'Density': 15, 'Rg': 15},\n",
    "            'lgbm':    {'Tg': 18, 'FFV': 18, 'Tc': 20, 'Density': 17, 'Rg': 17},\n",
    "            'extratrees':{'Tg': 22, 'FFV': 15, 'Tc': 10, 'Density': 25, 'Rg': 5},\n",
    "            'randomforest':{'Tg': 21, 'FFV': 19, 'Tc': 21, 'Density': 18, 'Rg': 18},\n",
    "            'balancedrf':{'Tg': 20, 'FFV': 20, 'Tc': 20, 'Density': 20, 'Rg': 20},\n",
    "        }\n",
    "\n",
    "        # Path for permutation importance log file\n",
    "        permutation_importance_log_path = \"log/permutation_importance_log.xlsx\"\n",
    "\n",
    "        correlation_threshold_value = 0.96\n",
    "        correlation_thresholds = {\n",
    "            \"Tg\": correlation_threshold_value,\n",
    "            \"FFV\": correlation_threshold_value,\n",
    "            \"Tc\": correlation_threshold_value,\n",
    "            \"Density\": correlation_threshold_value,\n",
    "            \"Rg\": correlation_threshold_value\n",
    "        }\n",
    "\n",
    "    # Create a single config instance to use everywhere\n",
    "    config = Config()\n",
    "\n",
    "    if config.debug or config.search_nn:\n",
    "        config.use_cross_validation = False\n",
    "\n",
    "    # --- XGB Hyperparameter Tuning DB Utilities ---\n",
    "    import sqlite3\n",
    "    import hashlib\n",
    "    import json\n",
    "\n",
    "    def init_chemberta():\n",
    "        model_name = \"/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM\"\n",
    "        chemberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        chemberta_model = AutoModel.from_pretrained(model_name)\n",
    "        chemberta_model.eval()\n",
    "        return chemberta_tokenizer, chemberta_model\n",
    "\n",
    "    def get_chemberta_embedding(smiles, embedding_dim=384):\n",
    "        \"\"\"\n",
    "        Returns ChemBERTa embedding for a single SMILES string.\n",
    "        Pads/truncates to embedding_dim if needed.\n",
    "        \"\"\"\n",
    "        if smiles is None or not isinstance(smiles, str) or len(smiles) == 0:\n",
    "            return np.zeros(embedding_dim)\n",
    "        try:\n",
    "            # Add pooling argument with default 'mean'\n",
    "            pooling = getattr(config, 'chemberta_pooling', 'mean')  # can be 'mean', 'max', 'cls', 'pooler'\n",
    "            chemberta_tokenizer, chemberta_model = init_chemberta()\n",
    "            inputs = chemberta_tokenizer([smiles], padding=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = chemberta_model(**inputs)\n",
    "                if pooling == 'pooler' and hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                    emb = outputs.pooler_output.squeeze(0)\n",
    "                elif pooling == 'cls' and hasattr(outputs, 'last_hidden_state'):\n",
    "                    emb = outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "                elif pooling == 'max' and hasattr(outputs, 'last_hidden_state'):\n",
    "                    emb = outputs.last_hidden_state.max(dim=1).values.squeeze(0)\n",
    "                elif pooling == 'mean' and hasattr(outputs, 'last_hidden_state'):\n",
    "                    emb = outputs.last_hidden_state.mean(dim=1).squeeze(0)\n",
    "                else:\n",
    "                    raise ValueError(\"Cannot extract embedding from model output\")\n",
    "                emb_np = emb.cpu().numpy()\n",
    "                # Pad or truncate if needed\n",
    "                if emb_np.shape[0] < embedding_dim:\n",
    "                    emb_np = np.pad(emb_np, (0, embedding_dim - emb_np.shape[0]))\n",
    "                elif emb_np.shape[0] > embedding_dim:\n",
    "                    emb_np = emb_np[:embedding_dim]\n",
    "                return emb_np\n",
    "        except Exception as e:\n",
    "            print(f\"ChemBERTa embedding failed for SMILES '{smiles}': {e}\")\n",
    "            return np.zeros(embedding_dim)\n",
    "        \n",
    "    def init_xgb_tuning_db(db_path=\"xgb_tuning.db\"):\n",
    "        \"\"\"Initialize the XGB tuning database and return all existing results.\"\"\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''CREATE TABLE IF NOT EXISTS xgb_tuning\n",
    "                    (param_hash TEXT PRIMARY KEY, params TEXT, score REAL)''')\n",
    "        c.execute('SELECT params, score FROM xgb_tuning')\n",
    "        results = c.fetchall()\n",
    "        conn.close()\n",
    "        return [(json.loads(params), score) for params, score in results]\n",
    "\n",
    "    def get_param_hash(params):\n",
    "        param_str = json.dumps(params, sort_keys=True)\n",
    "        return hashlib.md5(param_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def check_db_for_params(db_path, param_hash):\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        c = conn.cursor()\n",
    "        c.execute('SELECT score FROM xgb_tuning WHERE param_hash=?', (param_hash,))\n",
    "        result = c.fetchone()\n",
    "        conn.close()\n",
    "        return result is not None\n",
    "\n",
    "    def save_result_to_db(db_path, param_hash, params, score):\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''INSERT OR REPLACE INTO xgb_tuning (param_hash, params, score)\n",
    "                    VALUES (?, ?, ?)''', (param_hash, json.dumps(params, sort_keys=True), score))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    # --- XGB Hyperparameter Grid Search with DB Caching ---\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "    def xgb_grid_search_with_db(X, y, param_grid, db_path=\"xgb_tuning.db\"):\n",
    "        \"\"\"\n",
    "        For each param set in grid, check DB. If not present, train and save result.\n",
    "        param_grid: dict of param lists, e.g. {'max_depth':[3,5], 'learning_rate':[0.01,0.1]}\n",
    "        \"\"\"\n",
    "        tried = 0\n",
    "        best_score = None\n",
    "        best_params = None\n",
    "        # Split X, y into train/val for early stopping\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            param_hash = get_param_hash(params)\n",
    "            if check_db_for_params(db_path, param_hash):\n",
    "                print(f\"Skipping already tried params: {params}\")\n",
    "                continue\n",
    "            # print(f\"Trying params: {json.dumps(params, sort_keys=True)}\")\n",
    "            model = XGBRegressor(**params)\n",
    "            # Provide eval_set for early stopping\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            y_pred = model.predict(X_val)\n",
    "            from sklearn.metrics import mean_absolute_error\n",
    "            score = mean_absolute_error(y_val, y_pred)\n",
    "            print(f\"Result: MAE={score:.6f} for params: {json.dumps(params, sort_keys=True)}\")\n",
    "            # For MAE, lower is better\n",
    "            if (best_score is None) or (score < best_score):\n",
    "                best_score = score\n",
    "                best_params = params.copy()\n",
    "                print(f\"New best MAE: {best_score:.6f} with params: {json.dumps(best_params, sort_keys=True)}\")\n",
    "            save_result_to_db(db_path, param_hash, params, score)\n",
    "            tried += 1\n",
    "        print(f\"Tried {tried} new parameter sets.\")\n",
    "        if best_score is not None:\n",
    "            print(f\"Best score overall: {best_score:.6f} with params: {json.dumps(best_params, sort_keys=True)}\")\n",
    "\n",
    "    from sklearn.linear_model import RidgeCV, ElasticNetCV\n",
    "\n",
    "    def drop_correlated_features(df, threshold=0.95):\n",
    "        \"\"\"\n",
    "        Drops columns in a DataFrame that are highly correlated with other columns.\n",
    "        Only one of each pair of correlated columns is kept.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "            threshold (float): Correlation threshold for dropping columns (default 0.95).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with correlated columns dropped.\n",
    "            list: List of dropped column names.\n",
    "        \"\"\"\n",
    "        corr_matrix = df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "        return df.drop(columns=to_drop), to_drop\n",
    "\n",
    "    def get_canonical_smiles(smiles):\n",
    "            \"\"\"Convert SMILES to canonical form for consistency\"\"\"\n",
    "            if not RDKIT_AVAILABLE:\n",
    "                return smiles\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol:\n",
    "                    return Chem.MolToSmiles(mol, canonical=True)\n",
    "            except:\n",
    "                pass\n",
    "            return smiles\n",
    "\n",
    "    \"\"\"\n",
    "    Load competition data with complete filtering of problematic polymer notation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading competition data...\")\n",
    "    train = pd.read_csv(BASE_PATH + 'train.csv')\n",
    "    test = pd.read_csv(BASE_PATH + 'test.csv')\n",
    "\n",
    "    if config.debug:\n",
    "        print(\"   Debug mode: sampling 1000 training examples\")\n",
    "        train = train.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training data shape: {train.shape}, Test data shape: {test.shape}\")\n",
    "\n",
    "    def clean_and_validate_smiles(smiles):\n",
    "        \"\"\"Completely clean and validate SMILES, removing all problematic patterns\"\"\"\n",
    "        if not isinstance(smiles, str) or len(smiles) == 0:\n",
    "            return None\n",
    "        \n",
    "        # List of all problematic patterns we've seen\n",
    "        bad_patterns = [\n",
    "            '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', \n",
    "            \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n",
    "            # Additional patterns that cause issues\n",
    "            '([R])', '([R1])', '([R2])', \n",
    "        ]\n",
    "        \n",
    "        # Check for any bad patterns\n",
    "        for pattern in bad_patterns:\n",
    "            if pattern in smiles:\n",
    "                return None\n",
    "        \n",
    "        # Additional check: if it contains ] followed by [ without valid atoms, likely polymer notation\n",
    "        if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):\n",
    "            return None\n",
    "        \n",
    "        # Try to parse with RDKit if available\n",
    "        if RDKIT_AVAILABLE:\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    return Chem.MolToSmiles(mol, canonical=True)\n",
    "                else:\n",
    "                    return None\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        # If RDKit not available, return cleaned SMILES\n",
    "        return smiles\n",
    "\n",
    "    # Clean and validate all SMILES\n",
    "    print(\"Cleaning and validating SMILES...\")\n",
    "    train['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\n",
    "    test['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n",
    "\n",
    "    # Remove invalid SMILES\n",
    "    invalid_train = train['SMILES'].isnull().sum()\n",
    "    invalid_test = test['SMILES'].isnull().sum()\n",
    "\n",
    "    print(f\"   Removed {invalid_train} invalid SMILES from training data\")\n",
    "    print(f\"   Removed {invalid_test} invalid SMILES from test data\")\n",
    "\n",
    "    train = train[train['SMILES'].notnull()].reset_index(drop=True)\n",
    "    test = test[test['SMILES'].notnull()].reset_index(drop=True)\n",
    "\n",
    "    print(f\"   Final training samples: {len(train)}\")\n",
    "    print(f\"   Final test samples: {len(test)}\")\n",
    "\n",
    "    def add_extra_data_clean(df_train, df_extra, target):\n",
    "        \"\"\"Add external data with thorough SMILES cleaning\"\"\"\n",
    "        n_samples_before = len(df_train[df_train[target].notnull()])\n",
    "        \n",
    "        print(f\"      Processing {len(df_extra)} {target} samples...\")\n",
    "        \n",
    "        # Clean external SMILES\n",
    "        df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n",
    "        \n",
    "        # Remove invalid SMILES and missing targets\n",
    "        before_filter = len(df_extra)\n",
    "        df_extra = df_extra[df_extra['SMILES'].notnull()]\n",
    "        df_extra = df_extra.dropna(subset=[target])\n",
    "        after_filter = len(df_extra)\n",
    "        \n",
    "        print(f\"      Kept {after_filter}/{before_filter} valid samples\")\n",
    "        \n",
    "        if len(df_extra) == 0:\n",
    "            print(f\"      No valid data remaining for {target}\")\n",
    "            return df_train\n",
    "        \n",
    "        # Group by canonical SMILES and average duplicates\n",
    "        df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "        \n",
    "        cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n",
    "        unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n",
    "\n",
    "        # Fill missing values\n",
    "        filled_count = 0\n",
    "        for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n",
    "            if smile in cross_smiles:\n",
    "                df_train.loc[df_train['SMILES']==smile, target] = \\\n",
    "                    df_extra[df_extra['SMILES']==smile][target].values[0]\n",
    "                filled_count += 1\n",
    "        \n",
    "        # Add unique SMILES\n",
    "        extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n",
    "        if len(extra_to_add) > 0:\n",
    "            for col in TARGETS:\n",
    "                if col not in extra_to_add.columns:\n",
    "                    extra_to_add[col] = np.nan\n",
    "            \n",
    "            extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n",
    "            df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n",
    "\n",
    "        n_samples_after = len(df_train[df_train[target].notnull()])\n",
    "        print(f'      {target}: +{n_samples_after-n_samples_before} samples, +{len(unique_smiles_extra)} unique SMILES')\n",
    "        print(f\"      Filled {filled_count} missing entries in train for {target}\")\n",
    "        print(f\"      Added {len(extra_to_add)} new entries for {target}\")\n",
    "        return df_train\n",
    "\n",
    "    # Load external datasets with robust error handling\n",
    "    print(\"\\n📂 Loading external datasets...\")\n",
    "\n",
    "    external_datasets = []\n",
    "\n",
    "    # Function to safely load datasets\n",
    "    def safe_load_dataset(path, target, processor_func, description):\n",
    "        try:\n",
    "            if path.endswith('.xlsx'):\n",
    "                data = pd.read_excel(path)\n",
    "            else:\n",
    "                data = pd.read_csv(path)\n",
    "            \n",
    "            data = processor_func(data)\n",
    "            external_datasets.append((target, data))\n",
    "            print(f\"   ✅ {description}: {len(data)} samples\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ {description} failed: {str(e)[:100]}\")\n",
    "            return False\n",
    "\n",
    "    # Load each dataset\n",
    "    safe_load_dataset(\n",
    "        '/kaggle/input/tc-smiles/Tc_SMILES.csv',\n",
    "        'Tc',\n",
    "        lambda df: df.rename(columns={'TC_mean': 'Tc'}),\n",
    "        'Tc data'\n",
    "    )\n",
    "\n",
    "    safe_load_dataset(\n",
    "        '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv',\n",
    "        'Tg', \n",
    "        lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,\n",
    "        'TgSS enriched data'\n",
    "    )\n",
    "\n",
    "    safe_load_dataset(\n",
    "        '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv',\n",
    "        'Tg',\n",
    "        lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),\n",
    "        'JCIM Tg data'\n",
    "    )\n",
    "\n",
    "    safe_load_dataset(\n",
    "        '/kaggle/input/smiles-extra-data/data_tg3.xlsx',\n",
    "        'Tg',\n",
    "        lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),\n",
    "        'Xlsx Tg data'\n",
    "    )\n",
    "\n",
    "    safe_load_dataset(\n",
    "        '/kaggle/input/smiles-extra-data/data_dnst1.xlsx',\n",
    "        'Density',\n",
    "        lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n",
    "                    .query('SMILES.notnull() and Density.notnull() and Density != \"nylon\"')\n",
    "                    .assign(Density=lambda x: x['Density'].astype(float) - 0.118),\n",
    "        'Density data'\n",
    "    )\n",
    "\n",
    "    safe_load_dataset(\n",
    "        BASE_PATH + 'train_supplement/dataset4.csv',\n",
    "        'FFV', \n",
    "        lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,\n",
    "        'dataset 4'\n",
    "    )\n",
    "\n",
    "    # Integrate external data\n",
    "    print(\"\\n🔄 Integrating external data...\")\n",
    "    train_extended = train[['SMILES'] + TARGETS].copy()\n",
    "\n",
    "    if getattr(config, \"use_external_data\", True) and  not config.debug:\n",
    "        for target, dataset in external_datasets:\n",
    "            print(f\"   Processing {target} data...\")\n",
    "            train_extended = add_extra_data_clean(train_extended, dataset, target)\n",
    "\n",
    "    print(f\"\\n📊 Final training data:\")\n",
    "    print(f\"   Original samples: {len(train)}\")\n",
    "    print(f\"   Extended samples: {len(train_extended)}\")\n",
    "    print(f\"   Gain: +{len(train_extended) - len(train)} samples\")\n",
    "\n",
    "    for target in TARGETS:\n",
    "        count = train_extended[target].notna().sum()\n",
    "        original_count = train[target].notna().sum() if target in train.columns else 0\n",
    "        gain = count - original_count\n",
    "        print(f\"   {target}: {count:,} samples (+{gain})\")\n",
    "\n",
    "    print(f\"\\n✅ Data integration complete with clean SMILES!\")\n",
    "\n",
    "    def separate_subtables(train_df):\n",
    "        labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "        subtables = {}\n",
    "        for label in labels:\n",
    "            # Filter out NaNs, select columns, reset index\n",
    "            subtables[label] = train_df[train_df[label].notna()][['SMILES', label]].reset_index(drop=True)\n",
    "\n",
    "        # Optional: Debugging\n",
    "        for label in subtables:\n",
    "            print(f\"{label} NaNs per column:\")\n",
    "            print(subtables[label].isna().sum())\n",
    "            print(subtables[label].shape)\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "        return subtables\n",
    "\n",
    "    def augment_smiles_dataset(smiles_list, labels, num_augments=3):\n",
    "        \"\"\"\n",
    "        Augments a list of SMILES strings by generating randomized versions.\n",
    "\n",
    "        Parameters:\n",
    "            smiles_list (list of str): Original SMILES strings.\n",
    "            labels (list or np.array): Corresponding labels.\n",
    "            num_augments (int): Number of augmentations per SMILES.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (augmented_smiles, augmented_labels)\n",
    "        \"\"\"\n",
    "        augmented_smiles = []\n",
    "        augmented_labels = []\n",
    "\n",
    "        for smiles, label in zip(smiles_list, labels):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "            # Add original\n",
    "            augmented_smiles.append(smiles)\n",
    "            augmented_labels.append(label)\n",
    "            # Add randomized versions\n",
    "            for _ in range(num_augments):\n",
    "                rand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n",
    "                augmented_smiles.append(rand_smiles)\n",
    "                augmented_labels.append(label)\n",
    "\n",
    "        return augmented_smiles, np.array(augmented_labels)\n",
    "\n",
    "    mordred_calc = Calculator(mordred_descriptors, ignore_3D=True)\n",
    "    def build_mordred_descriptors(smiles_list):\n",
    "        # Build Mordred descriptors for test\n",
    "        mols_test = [Chem.MolFromSmiles(s) for s in smiles_list]\n",
    "        desc_test = mordred_calc.pandas(mols_test, nproc=1)\n",
    "\n",
    "        # Make columns string & numeric only (no dropping beyond that)\n",
    "        desc_test.columns = desc_test.columns.map(str)\n",
    "        desc_test = desc_test.select_dtypes(include=[np.number]).copy()\n",
    "        desc_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        return desc_test\n",
    "\n",
    "    from rdkit.Chem import Crippen, Lipinski\n",
    "\n",
    "    def smiles_to_combined_fingerprints_with_descriptors(smiles_list):\n",
    "        # Set fingerprint parameters inside the function\n",
    "        radius = 2\n",
    "        n_bits = 128\n",
    "\n",
    "        generator = GetMorganGenerator(radius=radius, fpSize=n_bits) if getattr(Config, \"use_morgan_fp\", True) else None\n",
    "        atom_pair_gen = GetAtomPairGenerator(fpSize=n_bits) if getattr(Config, 'use_atom_pair_fp', False) else None\n",
    "        torsion_gen = GetTopologicalTorsionGenerator(fpSize=n_bits) if getattr(Config, 'use_torsion_fp', False) else None\n",
    "        \n",
    "        fp_len = (n_bits if getattr(Config, 'use_morgan_fp', False) else 0) \\\n",
    "            + (n_bits if getattr(Config, 'use_atom_pair_fp', False) else 0) \\\n",
    "            + (n_bits if getattr(Config, 'use_torsion_fp', False) else 0) \\\n",
    "            + (167 if getattr(Config, 'use_maccs_fp', True) else 0)\n",
    "        if getattr(Config, 'use_chemberta', False):\n",
    "            fp_len += 384\n",
    "            \n",
    "        fingerprints = []\n",
    "        descriptors = []\n",
    "        valid_smiles = []\n",
    "        invalid_indices = []\n",
    "        use_any_fp = getattr(Config, \"use_morgan_fp\", False) or getattr(Config, \"use_atom_pair_fp\", False) or getattr(Config, \"use_torsion_fp\", False) or getattr(Config, \"use_maccs_fp\", False) or getattr(Config, 'use_chemberta', False)\n",
    "\n",
    "        for i, smiles in enumerate(smiles_list):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                # Fingerprints (No change from your code)\n",
    "                if use_any_fp:\n",
    "                    fps = []\n",
    "                    if getattr(Config, \"use_morgan_fp\", True) and generator is not None:\n",
    "                        fps.append(np.array(generator.GetFingerprint(mol)))\n",
    "                    if atom_pair_gen:\n",
    "                        fps.append(np.array(atom_pair_gen.GetFingerprint(mol)))\n",
    "                    if torsion_gen:\n",
    "                        fps.append(np.array(torsion_gen.GetFingerprint(mol)))\n",
    "                    if getattr(Config, 'use_maccs_fp', True):\n",
    "                        fps.append(np.array(MACCSkeys.GenMACCSKeys(mol)))\n",
    "                    if getattr(Config, \"use_chemberta\", False):\n",
    "                        emb = get_chemberta_embedding(smiles)\n",
    "                        fps.append(emb)\n",
    "                    \n",
    "                    combined_fp = np.concatenate(fps)\n",
    "                    fingerprints.append(combined_fp)\n",
    "\n",
    "                if getattr(Config, 'use_descriptors', True):\n",
    "                    descriptor_values = {}\n",
    "                    for name, func in Descriptors.descList:\n",
    "                        try:\n",
    "                            descriptor_values[name] = func(mol)\n",
    "                        except:\n",
    "                            print(f\"Descriptor {name} failed for SMILES at index {i}\")\n",
    "                            descriptor_values[name] = None\n",
    "\n",
    "                    # try:\n",
    "                    # --- Features for Rigidity and Complexity (for Tg, FFV) ---\n",
    "                    try:\n",
    "                        num_heavy_atoms = mol.GetNumHeavyAtoms()\n",
    "                    except Exception as e:\n",
    "                        num_heavy_atoms = 0\n",
    "\n",
    "                    # --- Features for Rigidity and Complexity (for Tg, FFV) ---\n",
    "                    try:\n",
    "                        descriptor_values['NumAromaticRings'] = Lipinski.NumAromaticRings(mol)\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['NumAromaticRings'] = None\n",
    "                    try:\n",
    "                        num_sp3_carbons = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 6 and atom.GetHybridization() == Chem.rdchem.HybridizationType.SP3)\n",
    "                        descriptor_values['FractionCSP3'] = num_sp3_carbons / num_heavy_atoms if num_heavy_atoms > 0 else 0\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['FractionCSP3'] = None\n",
    "\n",
    "                    # --- Features for Bulkiness and Shape (for FFV) ---\n",
    "                    try:\n",
    "                        descriptor_values['MolMR'] = Crippen.MolMR(mol) # Molar Refractivity (volume)\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['MolMR'] = None\n",
    "                    try:\n",
    "                        descriptor_values['LabuteASA'] = Descriptors.LabuteASA(mol) # Accessible surface area\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['LabuteASA'] = None\n",
    "\n",
    "                    # --- Features for Heavy Atoms (for Density) ---\n",
    "                    try:\n",
    "                        descriptor_values['NumFluorine'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 9)\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['NumFluorine'] = None\n",
    "                    try:\n",
    "                        descriptor_values['NumChlorine'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 17)\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['NumChlorine'] = None\n",
    "\n",
    "                    # --- Features for Intermolecular Forces (for Tc) ---\n",
    "                    try:\n",
    "                        descriptor_values['NumHDonors'] = Lipinski.NumHDonors(mol)\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['NumHDonors'] = None\n",
    "                    try:\n",
    "                        descriptor_values['NumHAcceptors'] = Lipinski.NumHAcceptors(mol)\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['NumHAcceptors'] = None\n",
    "\n",
    "                    # --- Features for Branching and Flexibility (for Rg) ---\n",
    "                    try:\n",
    "                        descriptor_values['BalabanJ'] = Descriptors.BalabanJ(mol) # Topological index sensitive to branching\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['BalabanJ'] = None\n",
    "                    try:\n",
    "                        descriptor_values['Kappa2'] = Descriptors.Kappa2(mol) # Molecular shape index\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['Kappa2'] = None\n",
    "                    try:\n",
    "                        descriptor_values['NumRotatableBonds'] = CalcNumRotatableBonds(mol) # Flexibility\n",
    "                    except Exception as e:\n",
    "                        descriptor_values['NumRotatableBonds'] = None\n",
    "                    \n",
    "                    # Graph-based features\n",
    "                    try:\n",
    "                        adj = rdmolops.GetAdjacencyMatrix(mol)\n",
    "                        G = nx.from_numpy_array(adj)\n",
    "                        if nx.is_connected(G):\n",
    "                            descriptor_values['graph_diameter'] = nx.diameter(G)\n",
    "                            descriptor_values['avg_shortest_path'] = nx.average_shortest_path_length(G)\n",
    "                        else:\n",
    "                            descriptor_values['graph_diameter'], descriptor_values['avg_shortest_path'] = 0, 0\n",
    "                        descriptor_values['num_cycles'] = len(list(nx.cycle_basis(G)))\n",
    "                    except:\n",
    "                        print(f\"Graph features failed for SMILES at index {i}\")\n",
    "                        descriptor_values['graph_diameter'], descriptor_values['avg_shortest_path'], descriptor_values['num_cycles'] = None, None, None\n",
    "\n",
    "                    descriptors.append(descriptor_values)\n",
    "                else:\n",
    "                    descriptors.append(None)\n",
    "                valid_smiles.append(smiles)\n",
    "            else:\n",
    "                if use_any_fp: fingerprints.append(np.zeros(fp_len))\n",
    "                if getattr(Config, \"use_chemberta\", False):\n",
    "                    descriptors.append({f'chemberta_emb_{j}': 0.0 for j in range(384)})\n",
    "                else:\n",
    "                    descriptors.append(None)\n",
    "                valid_smiles.append(None)\n",
    "                invalid_indices.append(i)\n",
    "\n",
    "        fingerprints_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fp_len)]) if use_any_fp else pd.DataFrame()\n",
    "        descriptors_df = pd.DataFrame([d for d in descriptors if d is not None]) if any(d is not None for d in descriptors) else pd.DataFrame()\n",
    "\n",
    "        if getattr(Config, 'use_mordred', False):\n",
    "            mordred_df = build_mordred_descriptors(smiles_list)\n",
    "            if descriptors_df.empty:\n",
    "                descriptors_df = mordred_df\n",
    "            else:\n",
    "                descriptors_df = pd.concat([descriptors_df.reset_index(drop=True), mordred_df], axis=1)\n",
    "        \n",
    "        # Keep only unique columns in descriptors_df\n",
    "        if not descriptors_df.empty:\n",
    "            descriptors_df = descriptors_df.loc[:, ~descriptors_df.columns.duplicated()]\n",
    "        return fingerprints_df, descriptors_df, valid_smiles, invalid_indices\n",
    "\n",
    "    required_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n",
    "\n",
    "    # Utility function to combine train and val sets into X_all and y_all\n",
    "    def combine_train_val(X_train, X_val, y_train, y_val):\n",
    "        X_train = pd.DataFrame(X_train) if isinstance(X_train, np.ndarray) else X_train\n",
    "        X_val = pd.DataFrame(X_val) if isinstance(X_val, np.ndarray) else X_val\n",
    "        y_train = pd.Series(y_train) if isinstance(y_train, np.ndarray) else y_train\n",
    "        y_val = pd.Series(y_val) if isinstance(y_val, np.ndarray) else y_val\n",
    "        X_all = pd.concat([X_train, X_val], axis=0)\n",
    "        y_all = pd.concat([y_train, y_val], axis=0)\n",
    "        return X_all, y_all\n",
    "\n",
    "    # --- PCA utility for train/test transformation ---\n",
    "    def apply_pca(X_train, X_test=None, verbose=True):\n",
    "        pca = PCA(n_components=config.pca_variance, svd_solver='full', random_state=getattr(config, 'random_state', 42))\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test) if X_test is not None else None\n",
    "        if verbose:\n",
    "            print(f\"[PCA] Reduced train shape: {X_train.shape} -> {X_train_pca.shape} (kept {pca.n_components_} components, {100*pca.explained_variance_ratio_.sum():.4f}% variance)\")\n",
    "        return X_train_pca, X_test_pca, pca\n",
    "\n",
    "    def augment_dataset(X, y, n_samples=1000, n_components=5, random_state=None):\n",
    "        \"\"\"\n",
    "        Augments a dataset using Gaussian Mixture Models.\n",
    "\n",
    "        Parameters:\n",
    "        - X: pd.DataFrame or np.ndarray — feature matrix\n",
    "        - y: pd.Series or np.ndarray — target values\n",
    "        - n_samples: int — number of synthetic samples to generate\n",
    "        - n_components: int — number of GMM components\n",
    "        - random_state: int — random seed for reproducibility\n",
    "\n",
    "        Returns:\n",
    "        - X_augmented: pd.DataFrame — augmented feature matrix\n",
    "        - y_augmented: pd.Series — augmented target values\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        elif not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X must be a pandas DataFrame or a NumPy array\")\n",
    "\n",
    "        X.columns = X.columns.astype(str)\n",
    "\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = pd.Series(y)\n",
    "        elif not isinstance(y, pd.Series):\n",
    "            raise ValueError(\"y must be a pandas Series or a NumPy array\")\n",
    "\n",
    "        df = X.copy()\n",
    "        df['Target'] = y.values\n",
    "\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "        gmm.fit(df)\n",
    "\n",
    "        synthetic_data, _ = gmm.sample(n_samples)\n",
    "        synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "        augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n",
    "\n",
    "        X_augmented = augmented_df.drop(columns='Target')\n",
    "        y_augmented = augmented_df['Target']\n",
    "\n",
    "        return X_augmented, y_augmented\n",
    "\n",
    "    # --- Outlier Detection Summary Function ---\n",
    "    def display_outlier_summary(y, X=None, name=\"target\", z_thresh=3, iqr_factor=1.5, iso_contamination=0.01, lof_contamination=0.01):\n",
    "        \"\"\"\n",
    "        Display the percentage of data flagged as outlier by Z-score, IQR, Isolation Forest, and LOF.\n",
    "        y: 1D array-like (target or feature)\n",
    "        X: 2D array-like (feature matrix, required for Isolation Forest/LOF)\n",
    "        name: str, name of the variable being checked\n",
    "        \"\"\"\n",
    "        print(f\"\\nOutlier summary for: {name}\")\n",
    "        y = np.asarray(y)\n",
    "        n = len(y)\n",
    "        # Z-score\n",
    "        z_scores = (y - np.mean(y)) / np.std(y)\n",
    "        z_outliers = np.abs(z_scores) > z_thresh\n",
    "        print(f\"Z-score > {z_thresh}: {np.sum(z_outliers)} / {n} ({100*np.mean(z_outliers):.2f}%)\")\n",
    "\n",
    "        # IQR\n",
    "        Q1 = np.percentile(y, 25)\n",
    "        Q3 = np.percentile(y, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - iqr_factor * IQR\n",
    "        upper = Q3 + iqr_factor * IQR\n",
    "        iqr_outliers = (y < lower) | (y > upper)\n",
    "        print(f\"IQR (factor {iqr_factor}): {np.sum(iqr_outliers)} / {n} ({100*np.mean(iqr_outliers):.2f}%)\")\n",
    "\n",
    "        # Isolation Forest (if X provided)\n",
    "        if X is not None:\n",
    "            try:\n",
    "                from sklearn.ensemble import IsolationForest\n",
    "                iso = IsolationForest(contamination=iso_contamination, random_state=42)\n",
    "                iso_out = iso.fit_predict(X)\n",
    "                iso_outliers = iso_out == -1\n",
    "                print(f\"Isolation Forest (contamination={iso_contamination}): {np.sum(iso_outliers)} / {len(iso_outliers)} ({100*np.mean(iso_outliers):.2f}%)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Isolation Forest failed: {e}\")\n",
    "            # Local Outlier Factor\n",
    "            try:\n",
    "                from sklearn.neighbors import LocalOutlierFactor\n",
    "                lof = LocalOutlierFactor(n_neighbors=20, contamination=lof_contamination)\n",
    "                lof_out = lof.fit_predict(X)\n",
    "                lof_outliers = lof_out == -1\n",
    "                print(f\"Local Outlier Factor (contamination={lof_contamination}): {np.sum(lof_outliers)} / {len(lof_outliers)} ({100*np.mean(lof_outliers):.2f}%)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Local Outlier Factor failed: {e}\")\n",
    "        else:\n",
    "            print(\"Isolation Forest/LOF skipped (X not provided)\")\n",
    "\n",
    "\n",
    "    train_df=train_extended\n",
    "    test_df=test\n",
    "    subtables = separate_subtables(train_df)\n",
    "\n",
    "    test_smiles = test_df['SMILES'].tolist()\n",
    "    test_ids = test_df['id'].values\n",
    "    labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "    #labels = ['Tc']\n",
    "\n",
    "    # Save importance_df to Excel log file, one sheet per label\n",
    "    def save_importance_to_excel(importance_df, label, log_path):\n",
    "        import os\n",
    "        from openpyxl import load_workbook\n",
    "        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "        if os.path.exists(log_path):\n",
    "            with pd.ExcelWriter(log_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                importance_df.to_excel(writer, sheet_name=label, index=False)\n",
    "        else:\n",
    "            with pd.ExcelWriter(log_path, engine='openpyxl') as writer:\n",
    "                importance_df.to_excel(writer, sheet_name=label, index=False)\n",
    "\n",
    "    def get_least_important_features_all_methods(X, y, label, model_name=None):\n",
    "        \"\"\"\n",
    "        Remove features in three steps:\n",
    "        1. Remove features with model.feature_importances_ <= 0\n",
    "        2. Remove features with permutation_importance <= 0\n",
    "        3. Remove features with SHAP importance <= 0\n",
    "        Returns a list of features to remove (union of all three criteria).\n",
    "        \"\"\"    \n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=config.random_state)\n",
    "        model_type = (model_name or getattr(config, 'model_name', 'xgb'))\n",
    "        if model_type == 'xgb':\n",
    "            model = XGBRegressor(random_state=config.random_state, n_jobs=-1, verbosity=0, early_stopping_rounds=50, eval_metric=\"mae\", objective=\"reg:absoluteerror\")\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "        elif model_type == 'catboost':\n",
    "            model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, loss_function='MAE', eval_metric='MAE', random_seed=config.random_state, verbose=False)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=50, use_best_model=True)\n",
    "        elif model_type == 'lgbm':\n",
    "            model = LGBMRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, reg_lambda=1.0, objective='mae', random_state=config.random_state, verbose=-1, verbosity=-1)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric='mae', callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
    "        else:\n",
    "            model = XGBRegressor(random_state=config.random_state, n_jobs=-1, verbosity=0, early_stopping_rounds=50, eval_metric=\"rmse\")\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "        feature_names = X_train.columns\n",
    "\n",
    "        # 1. Remove features with model.feature_importances_ <= 0\n",
    "        fi_mask = model.feature_importances_ <= 0\n",
    "        fi_features = set(feature_names[fi_mask])\n",
    "        # Save feature_importances_ to Excel, sorted by importance\n",
    "        fi_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance_mean': model.feature_importances_,\n",
    "            'importance_std': [0]*len(feature_names)\n",
    "        }).sort_values('importance_mean', ascending=False)\n",
    "        save_importance_to_excel(fi_importance_df, label + '_fi', getattr(Config, 'permutation_importance_log_path', 'log/permutation_importance_log.xlsx'))\n",
    "\n",
    "        # 2. Remove features with permutation_importance <= 0\n",
    "        perm_result = permutation_importance(\n",
    "            model, X_valid, y_valid,\n",
    "            n_repeats=1 if config.debug else 10,\n",
    "            random_state=config.random_state,\n",
    "            scoring='neg_mean_absolute_error'\n",
    "        )\n",
    "        # Save permutation importance to Excel, sorted by mean importance descending\n",
    "        perm_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance_mean': perm_result.importances_mean,\n",
    "            'importance_std': perm_result.importances_std\n",
    "        }).sort_values('importance_mean', ascending=False)\n",
    "        save_importance_to_excel(perm_importance_df, label + '_perm', getattr(Config, 'permutation_importance_log_path', 'log/permutation_importance_log.xlsx'))\n",
    "        perm_mask = perm_result.importances_mean <= 0\n",
    "        perm_features = set(feature_names[perm_mask])\n",
    "\n",
    "        # 3. Remove features with SHAP importance <= 0\n",
    "        explainer = shap.Explainer(model, X_valid)\n",
    "        # For LGBM, disable additivity check to avoid ExplainerError\n",
    "        if model_type == 'lgbm':\n",
    "            shap_values = explainer(X_valid, check_additivity=False)\n",
    "        else:\n",
    "            shap_values = explainer(X_valid)\n",
    "        shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "        shap_mask = shap_importance <= 0\n",
    "        shap_features = set(feature_names[shap_mask])\n",
    "        # Save SHAP importance to Excel, sorted by importance\n",
    "        shap_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance_mean': shap_importance,\n",
    "            'importance_std': [0]*len(feature_names)\n",
    "        }).sort_values('importance_mean', ascending=False)\n",
    "        save_importance_to_excel(shap_importance_df, label + '_shap', getattr(Config, 'permutation_importance_log_path', 'log/permutation_importance_log.xlsx'))\n",
    "\n",
    "        # Union of all features to remove\n",
    "        features_to_remove = fi_features | perm_features | shap_features\n",
    "        print(f\"Removed {len(features_to_remove)} features for {label} using all methods (fi: {len(fi_features)}, perm: {len(perm_features)}, shap: {len(shap_features)})\")\n",
    "\n",
    "        return list(features_to_remove)\n",
    "\n",
    "    def get_least_important_features(X, y, label, model_name=None):\n",
    "        # Correct unpacking of train_test_split\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=config.random_state)\n",
    "\n",
    "        if (model_name or getattr(config, 'model_name', 'xgb')) == 'xgb':\n",
    "            model = XGBRegressor(random_state=config.random_state, n_jobs=-1, verbosity=0, early_stopping_rounds=50, eval_metric=\"mae\", objective=\"reg:absoluteerror\")\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "        else:\n",
    "            model = ExtraTreesRegressor(random_state=config.random_state, criterion='absolute_error')\n",
    "            model.fit(X, y)\n",
    "\n",
    "        # Use config.feature_importance_method to choose method\n",
    "        importance_method = getattr(config, 'feature_importance_method', 'feature_importances_')\n",
    "        if importance_method == 'feature_importances_':\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance_mean': model.feature_importances_,\n",
    "                'importance_std': [0]*len(X_train.columns)\n",
    "            })\n",
    "        else:\n",
    "        # model = ExtraTreesRegressor(random_state=config.random_state)\n",
    "            result = permutation_importance(\n",
    "                model, X_valid, y_valid,\n",
    "                n_repeats=30,\n",
    "                random_state=Config.random_state,\n",
    "                scoring='r2'\n",
    "            )\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance_mean': result.importances_mean,\n",
    "                'importance_std': result.importances_std\n",
    "            })\n",
    "\n",
    "        # Use model- and label-specific n_least_important_features\n",
    "        if model_name is None:\n",
    "            model_name_used = getattr(config, 'model_name', 'xgb')\n",
    "        else:\n",
    "            model_name_used = model_name\n",
    "        n = config.n_least_important_features.get(model_name_used, {}).get(label, 5)\n",
    "        # Sort by importance (ascending) and return n least important features\n",
    "        # Remove all features with importance_mean < 0 first\n",
    "        negative_importance = importance_df[importance_df['importance_mean'] <= 0]\n",
    "        num_negative = len(negative_importance)\n",
    "        least_important = negative_importance\n",
    "\n",
    "        # If less than n features removed, remove more to reach n\n",
    "        if num_negative < n:\n",
    "            # Exclude already selected features\n",
    "            remaining = importance_df[~importance_df['feature'].isin(negative_importance['feature'])]\n",
    "            additional = remaining.sort_values(by='importance_mean').head(n - num_negative)\n",
    "            least_important = pd.concat([least_important, additional], ignore_index=True)\n",
    "        else:\n",
    "            # If already removed n or more, just keep the negative ones\n",
    "            least_important = negative_importance\n",
    "\n",
    "        print(f\"Removed {len(least_important)} least important features for {label} (with {num_negative} <= 0)\")\n",
    "\n",
    "        importance_df = importance_df.sort_values(by='importance_mean', ascending=True)\n",
    "\n",
    "        # Mark features to be removed\n",
    "        importance_df['removed'] = importance_df['feature'].isin(least_important['feature'])\n",
    "\n",
    "        save_importance_to_excel(importance_df, label, Config.permutation_importance_log_path)\n",
    "\n",
    "        return least_important['feature'].tolist()\n",
    "\n",
    "    # Save model to disk for this fold using a helper function\n",
    "    def save_model(Model, label, fold, model_name):\n",
    "        model_path = f\"models/{label}_fold{fold+1}_{model_name}\"\n",
    "        try:\n",
    "            if 'torch' in str(type(Model)).lower():\n",
    "                # Save PyTorch model state_dict\n",
    "                model_path += \".pt\"\n",
    "                torch.save(Model.state_dict(), model_path)\n",
    "            else:\n",
    "                # Save scikit-learn model\n",
    "                model_path += \".joblib\"\n",
    "                joblib.dump(Model, model_path)\n",
    "            print(f\"Saved model for {label} fold {fold+1} to {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save model for {label} fold {fold+1}: {e}\")\n",
    "\n",
    "    def train_with_other_models(model_name, label, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Train a regression model using the specified model_name, with hyperparameters\n",
    "        adapted to the data size of the target label.\n",
    "        \"\"\"\n",
    "        print(f\"Training {model_name} model for label: {label}\")\n",
    "        if model_name == 'tabnet':\n",
    "            try:\n",
    "                from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "            except ImportError:\n",
    "                raise ImportError(\"pytorch-tabnet is not installed. Please install it with 'pip install pytorch-tabnet'.\")\n",
    "            \n",
    "            # --- Define TabNet parameters based on label ---\n",
    "            if label in ['Rg', 'Tc']: # Low Data\n",
    "                params = {'n_d': 8, 'n_a': 8, 'n_steps': 3, 'gamma': 1.3, 'lambda_sparse': 1e-4}\n",
    "            elif label == 'FFV': # High Data\n",
    "                params = {'n_d': 24, 'n_a': 24, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 1e-5}\n",
    "            else: # Medium Data\n",
    "                params = {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 1e-5}\n",
    "\n",
    "            Model = TabNetRegressor(**params, seed=42, verbose=0)\n",
    "            Model.fit(\n",
    "                X_train.values, y_train.values.reshape(-1, 1),\n",
    "                eval_set=[(X_val.values, y_val.values.reshape(-1, 1))],\n",
    "                eval_metric=['mae'], # ACTION: Changed from 'rmse' to 'mae'\n",
    "                max_epochs=200, patience=20, batch_size=1024, virtual_batch_size=128\n",
    "            )\n",
    "\n",
    "        elif model_name == 'catboost':\n",
    "            # --- Define CatBoost parameters based on label ---\n",
    "            params = {'iterations': 3000, 'learning_rate': 0.05, 'loss_function': 'MAE', 'eval_metric': 'MAE', 'random_seed': Config.random_state, 'verbose': False}\n",
    "            if label in ['Rg', 'Tc']: # Low Data\n",
    "                params.update({'depth': 5, 'l2_leaf_reg': 7})\n",
    "            elif label == 'FFV': # High Data\n",
    "                params.update({'depth': 7, 'l2_leaf_reg': 2})\n",
    "            else: # Medium Data\n",
    "                params.update({'depth': 6, 'l2_leaf_reg': 3})\n",
    "\n",
    "            Model = CatBoostRegressor(**params)\n",
    "            Model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, use_best_model=True)\n",
    "\n",
    "        elif model_name == 'lgbm':\n",
    "            # --- Define LightGBM parameters based on label ---\n",
    "            params = {'n_estimators': 3000, 'learning_rate': 0.05, 'objective': 'mae', 'random_state': Config.random_state, 'verbose': -1, 'verbosity': -1}\n",
    "            if label in ['Rg', 'Tc']: # Low Data\n",
    "                params.update({'max_depth': 4, 'num_leaves': 20, 'reg_lambda': 5.0})\n",
    "            elif label == 'FFV': # High Data\n",
    "                params.update({'max_depth': 7, 'num_leaves': 40, 'reg_lambda': 1.0})\n",
    "            else: # Medium Data\n",
    "                params.update({'max_depth': 6, 'num_leaves': 31, 'reg_lambda': 1.0})\n",
    "\n",
    "            Model = LGBMRegressor(**params)\n",
    "            Model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mae', callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
    "\n",
    "        elif model_name == 'extratrees':\n",
    "            # --- Define ExtraTrees parameters based on label ---\n",
    "            params = {'n_estimators': 300, 'criterion': 'absolute_error', 'random_state': Config.random_state, 'n_jobs': -1}\n",
    "            if label in ['Rg', 'Tc']: # Low Data: Prevent overfitting by requiring more samples per leaf\n",
    "                params.update({'min_samples_leaf': 3, 'max_features': 0.8})\n",
    "            else: # High/Medium Data\n",
    "                params.update({'min_samples_leaf': 1, 'max_features': 1.0})\n",
    "                \n",
    "            Model = ExtraTreesRegressor(**params)\n",
    "            X_all, y_all = combine_train_val(X_train, X_val, y_train, y_val)\n",
    "            Model.fit(X_all, y_all)\n",
    "\n",
    "        elif model_name == 'randomforest':\n",
    "            # --- Define RandomForest parameters based on label ---\n",
    "            params = {'n_estimators': 1000, 'criterion': 'absolute_error', 'random_state': Config.random_state, 'n_jobs': -1}\n",
    "            if label in ['Rg', 'Tc']: # Low Data\n",
    "                params.update({'min_samples_leaf': 3, 'max_features': 0.8, 'max_depth': 15})\n",
    "            else: # High/Medium Data\n",
    "                params.update({'min_samples_leaf': 1, 'max_features': 1.0, 'max_depth': None})\n",
    "\n",
    "            Model = RandomForestRegressor(**params)\n",
    "            X_all, y_all = combine_train_val(X_train, X_val, y_train, y_val)\n",
    "            Model.fit(X_all, y_all)\n",
    "\n",
    "        elif model_name == 'hgbm':\n",
    "            from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "            # --- Define HGBM parameters based on label ---\n",
    "            params = {'max_iter': 1000, 'learning_rate': 0.05, 'loss': 'absolute_error', 'early_stopping': True, 'random_state': 42} # ACTION: Changed loss to 'absolute_error'\n",
    "            if label in ['Rg', 'Tc']: # Low Data\n",
    "                params.update({'max_depth': 4, 'l2_regularization': 1.0})\n",
    "            elif label == 'FFV': # High Data\n",
    "                params.update({'max_depth': 7, 'l2_regularization': 0.1})\n",
    "            else: # Medium Data\n",
    "                params.update({'max_depth': 6, 'l2_regularization': 0.5})\n",
    "\n",
    "            Model = HistGradientBoostingRegressor(**params)\n",
    "            X_all, y_all = combine_train_val(X_train, X_val, y_train, y_val)\n",
    "            Model.fit(X_all, y_all)\n",
    "\n",
    "        elif model_name == 'nn':\n",
    "            # The 'train_with_nn' function already uses different configs per label, which is excellent!\n",
    "            # Just ensure the loss function inside it is nn.L1Loss()\n",
    "            Model = train_with_nn(label, X_train, X_val, y_train, y_val)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown or unavailable model: {model_name}\")\n",
    "        \n",
    "        return Model\n",
    "\n",
    "    def train_with_autogluon(label, X_train, y_train):\n",
    "        try:\n",
    "            from autogluon.tabular import TabularPredictor\n",
    "        except ImportError:\n",
    "            raise ImportError(\"AutoGluon is not installed. Please install it with 'pip install autogluon'.\")\n",
    "        import pandas as pd\n",
    "        import uuid\n",
    "        # Prepare data for AutoGluon (must be DataFrame with column names)\n",
    "        X_train_df = pd.DataFrame(X_train)\n",
    "        y_train_series = pd.Series(y_train, name=label)\n",
    "        train_data = X_train_df.copy()\n",
    "        train_data[label] = y_train_series.values\n",
    "\n",
    "        unique_path = f\"autogluon_{label}_{int(time.time())}_{uuid.uuid4().hex}\"\n",
    "\n",
    "        hyperparameters = {\n",
    "            \"GBM\": {},\n",
    "            \"CAT\": {},\n",
    "            \"XGB\": {},\n",
    "            \"NN_TORCH\": {},\n",
    "            \"RF\": {},\n",
    "            \"XT\": {}\n",
    "        }\n",
    "\n",
    "        hyperparameter_tune_kwargs = {\n",
    "            \"num_trials\": 50,\n",
    "            \"scheduler\": \"local\",\n",
    "            \"searcher\": \"auto\"\n",
    "        }\n",
    "\n",
    "        time_limit = 300 if getattr(Config, 'debug', False) else 3600\n",
    "\n",
    "        predictor = TabularPredictor(\n",
    "            label=label,\n",
    "            eval_metric=\"mae\",  # Use 'mae' for regression\n",
    "            path=unique_path\n",
    "        ).fit(\n",
    "            train_data,\n",
    "            presets=\"best_quality\",\n",
    "            hyperparameters=hyperparameters,\n",
    "            hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    "            num_bag_folds=5,\n",
    "            num_stack_levels=2,\n",
    "            time_limit=time_limit\n",
    "        )\n",
    "\n",
    "        print(\"\\n[AutoGluon] Leaderboard:\")\n",
    "        leaderboard = predictor.leaderboard(silent=False)\n",
    "\n",
    "        print(\"\\n[AutoGluon] Model Info:\")\n",
    "        print(predictor.info())\n",
    "\n",
    "        print(\"\\n[AutoGluon] Model Names:\")\n",
    "        model_names = leaderboard[\"model\"].tolist()   # <- FIXED here\n",
    "        print(model_names)\n",
    "\n",
    "        # Save feature importance to CSV\n",
    "        fi_df = predictor.feature_importance(train_data)\n",
    "        fi_path = f\"NeurIPS/autogluon_feature_importance_{label}.csv\"  \n",
    "        fi_df.to_csv(fi_path)\n",
    "        print(f\"[AutoGluon] Feature importance saved to {fi_path}\")\n",
    "\n",
    "        return predictor\n",
    "\n",
    "    def train_with_stacking(label, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Trains XGBoost, ExtraTrees, and CatBoost using sklearn's StackingRegressor.\n",
    "        Returns the fitted stacking model and base models.\n",
    "        \"\"\"\n",
    "        estimators = [\n",
    "        ('xgb', XGBRegressor(n_estimators=10000, learning_rate=0.01, max_depth=5, subsample=0.8, colsample_bytree=0.8, gamma=0.1, reg_lambda=1.0, objective=\"reg:absoluteerror\", random_state=Config.random_state, n_jobs=-1)),\n",
    "        ('lgbm', LGBMRegressor(n_estimators=10000, learning_rate=0.01, num_leaves=64, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5, reg_lambda=1.0, max_depth=-1, objective=\"mae\", random_state=Config.random_state, n_jobs=-1, verbose=-1)),\n",
    "            ('cb', CatBoostRegressor(iterations=10000, learning_rate=0.01, depth=7, l2_leaf_reg=5, bagging_temperature=0.8, random_seed=Config.random_state, verbose=0)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=500, max_depth=12, min_samples_leaf=3, random_state=Config.random_state, n_jobs=-1, criterion='absolute_error'))\n",
    "        ]\n",
    "\n",
    "        # Candidate final estimators\n",
    "        final_estimators = {\n",
    "            \"ElasticNet\": ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9], n_alphas=100, max_iter=50000, tol=1e-3, cv=5, n_jobs=-1),\n",
    "            \"CatBoost\": CatBoostRegressor(iterations=3000, learning_rate=0.03, depth=6, l2_leaf_reg=3, random_seed=Config.random_state, verbose=0),\n",
    "        \"LightGBM\": LGBMRegressor(n_estimators=3000, learning_rate=0.03, num_leaves=64, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5, reg_lambda=1.0, max_depth=-1, objective=\"mae\", random_state=Config.random_state, n_jobs=-1, verbose=-1),\n",
    "        \"XGBoost\": XGBRegressor(n_estimators=3000, learning_rate=0.03, max_depth=6, subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, gamma=0.1, objective=\"reg:absoluteerror\", random_state=Config.random_state, n_jobs=-1)\n",
    "        }\n",
    "\n",
    "        final_estimator = final_estimators['ElasticNet']\n",
    "        \n",
    "        stacker = StackingRegressor(estimators=estimators, final_estimator=final_estimator, passthrough=True, cv=5, n_jobs=-1)\n",
    "\n",
    "        stacker.fit(X_train, y_train)\n",
    "        return stacker\n",
    "\n",
    "    def train_with_xgb(label, X_train, y_train, X_val, y_val):\n",
    "        print(f\"Training XGB model for label: {label}\")\n",
    "        if label==\"Tg\": # Medium Data (~1.2k samples)\n",
    "            Model = XGBRegressor(\n",
    "                n_estimators=10000, learning_rate=0.01, max_depth=5, # <-- Reduced from 6\n",
    "                colsample_bytree=1.0, reg_lambda=7.0, gamma=0.1, subsample=0.5, # <-- Increased lambda, reduced subsample\n",
    "                objective=\"reg:absoluteerror\", random_state=Config.random_state, \n",
    "                early_stopping_rounds=50, eval_metric=\"mae\"\n",
    "            )\n",
    "        elif label=='Rg': # Very Low Data (~600 samples), HIGHEST PRIORITY\n",
    "            Model = XGBRegressor(\n",
    "                n_estimators=10000, learning_rate=0.06, max_depth=4, \n",
    "                colsample_bytree=1.0, reg_lambda=10.0, gamma=0.1, subsample=0.6, \n",
    "                objective=\"reg:absoluteerror\", random_state=Config.random_state, \n",
    "                early_stopping_rounds=50, eval_metric=\"mae\"\n",
    "            )\n",
    "        elif label=='FFV': # High Data (~8k samples), LOWEST PRIORITY\n",
    "            Model = XGBRegressor(\n",
    "                n_estimators=10000, learning_rate=0.06, max_depth=7, \n",
    "                colsample_bytree=0.8, reg_lambda=2.0, gamma=0.0, subsample=0.6, \n",
    "                objective=\"reg:absoluteerror\", random_state=Config.random_state, \n",
    "                early_stopping_rounds=50, eval_metric=\"mae\"\n",
    "            )\n",
    "        elif label=='Tc': # Low Data (~900 samples), HIGH PRIORITY\n",
    "            Model = XGBRegressor(\n",
    "                n_estimators=10000, learning_rate=0.01, max_depth=4, \n",
    "                colsample_bytree=0.8, reg_lambda=7.0, gamma=0.0, subsample=0.6, \n",
    "                objective=\"reg:absoluteerror\", random_state=Config.random_state, \n",
    "                early_stopping_rounds=50, eval_metric=\"mae\"\n",
    "            )\n",
    "        elif label=='Density': # Medium Data (~1.2k samples)\n",
    "            Model = XGBRegressor(\n",
    "                n_estimators=10000, learning_rate=0.06, max_depth=5, \n",
    "                colsample_bytree=1.0, reg_lambda=3.0, gamma=0.0, subsample=0.8, \n",
    "                objective=\"reg:absoluteerror\", random_state=Config.random_state, \n",
    "                early_stopping_rounds=50, eval_metric=\"mae\"\n",
    "            )\n",
    "            \n",
    "        print(f\"Model {label} trained with shape: {X_train.shape}, {y_train.shape}\")\n",
    "\n",
    "        Model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        return Model\n",
    "\n",
    "    def preprocess_numerical_features(X, label=None):\n",
    "        # Ensure numeric types\n",
    "        X_num = X.select_dtypes(include=[np.number]).copy()\n",
    "        \n",
    "        # Replace inf/-inf with NaN\n",
    "        X_num.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        \n",
    "        valid_cols = X_num.columns\n",
    "        # # Drop columns with any NaN\n",
    "        # valid_cols = [col for col in X_num.columns if not X_num[col].isnull().any()]\n",
    "        \n",
    "        # dropped_cols = set(X_num.columns) - set(valid_cols)\n",
    "        # if dropped_cols:\n",
    "        #     print(f\"Dropped columns with NaN/Inf for {label}: {list(dropped_cols)}\")\n",
    "\n",
    "        # # Keep only valid columns\n",
    "        # X_num = X_num[valid_cols]\n",
    "        \n",
    "        # Calculate median for each column (for use in test set)\n",
    "        median_values = X_num.median()\n",
    "        # Scale features if enabled\n",
    "        if getattr(Config, 'use_standard_scaler', False):\n",
    "            scaler = StandardScaler()\n",
    "            X_num_scaled = scaler.fit_transform(X_num)\n",
    "            X_num = pd.DataFrame(X_num_scaled, columns=valid_cols, index=X.index)\n",
    "        else:\n",
    "            scaler = None\n",
    "            X_num = X_num.copy()\n",
    "        \n",
    "        # Display categorical features\n",
    "        cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "        if cat_cols:\n",
    "            print(f\"Categorical (non-numeric) features for {label}: {cat_cols}\")\n",
    "        else:\n",
    "            print(f\"No categorical (non-numeric) features for {label}.\")\n",
    "        return X_num, valid_cols, scaler, median_values\n",
    "\n",
    "    def select_features_with_lasso(X, y, label):\n",
    "        \"\"\"\n",
    "        Performs feature selection using Lasso (L1) regularization.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): The input feature matrix.\n",
    "            y (pd.Series or np.array): The target values.\n",
    "            label (str): The name of the target property (e.g., 'Rg', 'Tc').\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing only the selected features.\n",
    "        \"\"\"\n",
    "        # Lasso is sensitive to feature scaling, so we scale the data first.\n",
    "        # We also need to handle any potential NaN values before scaling.\n",
    "        X_filled = X.fillna(X.median())\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_filled)\n",
    "\n",
    "        # Use LassoCV to automatically find the best alpha (regularization strength)\n",
    "        # through cross-validation. This is more robust than picking a single alpha.\n",
    "        lasso_cv = LassoCV(cv=5, random_state=42, n_jobs=-1, max_iter=2000)\n",
    "\n",
    "        # Use SelectFromModel to wrap the LassoCV regressor.\n",
    "        # This will select features where the Lasso coefficient is non-zero.\n",
    "        # The 'threshold=\"median\"' can be a good strategy to select the top 50% of features\n",
    "        # if LassoCV is too lenient and keeps too many. Start with the default (None).\n",
    "        feature_selector = SelectFromModel(lasso_cv, prefit=False, threshold=None)\n",
    "\n",
    "        print(f\"[{label}] Fitting LassoCV to find optimal features...\")\n",
    "        feature_selector.fit(X_scaled, y)\n",
    "\n",
    "        # Get the names of the features that were kept\n",
    "        selected_feature_names = X.columns[feature_selector.get_support()]\n",
    "\n",
    "        print(f\"[{label}] Original number of features: {X.shape[1]}\")\n",
    "        print(f\"[{label}] Features selected by Lasso: {len(selected_feature_names)}\")\n",
    "\n",
    "        # Return the original DataFrame with only the selected columns\n",
    "        return selected_feature_names\n",
    "\n",
    "\n",
    "    def check_inf_nan(X, y, label=None):\n",
    "        \"\"\"\n",
    "        Checks for inf, -inf, and NaN values in X (DataFrame) and y (array/Series).\n",
    "        Prints summary and returns True if any such values are found.\n",
    "        \"\"\"\n",
    "        X_inf = np.isinf(X.values).any()\n",
    "        X_nan = np.isnan(X.values).any()\n",
    "        y_inf = np.isinf(y).any()\n",
    "        y_nan = np.isnan(y).any()\n",
    "        if label is None:\n",
    "            label = \"\"\n",
    "        else:\n",
    "            label = f\" [{label}]\"\n",
    "        if X_inf or X_nan or y_inf or y_nan:\n",
    "            print(f\"⚠️ Detected inf/nan in X or y{label}: X_inf={X_inf}, X_nan={X_nan}, y_inf={y_inf}, y_nan={y_nan}\")\n",
    "            if X_inf:\n",
    "                print(f\"  X columns with inf: {X.columns[np.isinf(X.values).any(axis=0)].tolist()}\")\n",
    "            if X_nan:\n",
    "                print(f\"  X columns with nan: {X.columns[np.isnan(X.values).any(axis=0)].tolist()}\")\n",
    "            if y_inf:\n",
    "                print(\"  y contains inf values.\")\n",
    "            if y_nan:\n",
    "                print(\"  y contains nan values.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"No inf/nan in X or y{label}.\")\n",
    "            return False\n",
    "\n",
    "    # Utility: Display model summary if torchinfo is available\n",
    "    def show_model_summary(model, input_dim, batch_size=32):\n",
    "        try:\n",
    "            from torchinfo import summary\n",
    "            print(summary(model, input_size=(batch_size, input_dim)))\n",
    "        except ImportError:\n",
    "            print(\"torchinfo is not installed. Install it with 'pip install torchinfo' to see model summaries.\")\n",
    "\n",
    "\n",
    "    def train_model(\n",
    "        model,\n",
    "        X_train, X_val, y_train, y_val,\n",
    "        epochs=3000, batch_size=32, lr=1e-3, weight_decay=1e-4, patience=30, verbose=True\n",
    "    ):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "\n",
    "        X_train = np.asarray(X_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        X_val = np.asarray(X_val)\n",
    "        y_val = np.asarray(y_val)\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=verbose)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        epochs_no_improve = 0\n",
    "        use_early_stopping = X_val is not None and y_val is not None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if verbose and (epoch+1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            if use_early_stopping:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_preds = model(X_val_tensor)\n",
    "                    val_loss = criterion(val_preds, y_val_tensor).item()\n",
    "                scheduler.step(val_loss)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = model.state_dict()\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}, best val loss: {best_val_loss:.4f}\")\n",
    "                    if best_model_state is not None:\n",
    "                        model.load_state_dict(best_model_state)\n",
    "                    break\n",
    "\n",
    "        return model\n",
    "\n",
    "    class FeedforwardNet(nn.Module):\n",
    "        def __init__(self, input_dim, neurons, dropouts):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            for i, n in enumerate(neurons):\n",
    "                layers.append(nn.Linear(input_dim, n))\n",
    "                # layers.append(nn.BatchNorm1d(n))\n",
    "                layers.append(nn.ReLU())\n",
    "                if i < len(dropouts) and dropouts[i] > 0:\n",
    "                    layers.append(nn.Dropout(dropouts[i]))\n",
    "                input_dim = n\n",
    "            layers.append(nn.Linear(input_dim, 1))\n",
    "            self.model = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    def train_with_nn(label, X_train, X_val, y_train, y_val):\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "        \n",
    "        if getattr(Config, \"search_nn\", False):\n",
    "            print(f\"--- Starting targeted NN architecture search for label: {label} ---\")\n",
    "            \n",
    "            search_configs_by_label = {\n",
    "                'Low': [ # For Rg, Tc. Simple models with high regularization.\n",
    "                    # --- Single Layer Focus ---\n",
    "                    {\"neurons\": [32], \"dropouts\": [0.3]},\n",
    "                    {\"neurons\": [64], \"dropouts\": [0.4]},\n",
    "                    {\"neurons\": [128], \"dropouts\": [0.5]},\n",
    "                    {\"neurons\": [256], \"dropouts\": [0.5]},\n",
    "\n",
    "                    # --- Two Layer Rectangular Focus (based on Rg's winner) ---\n",
    "                    {\"neurons\": [64, 64], \"dropouts\": [0.4, 0.4]},\n",
    "                    {\"neurons\": [96, 96], \"dropouts\": [0.5, 0.5]},\n",
    "                    {\"neurons\": [128, 128], \"dropouts\": [0.5, 0.5]}, # Previous winner\n",
    "                    {\"neurons\": [192, 192], \"dropouts\": [0.5, 0.5]},\n",
    "                    {\"neurons\": [256, 256], \"dropouts\": [0.5, 0.5]},\n",
    "\n",
    "                    # --- Two Layer Tapering Focus ---\n",
    "                    {\"neurons\": [128, 32], \"dropouts\": [0.5, 0.3]},\n",
    "                    {\"neurons\": [128, 64], \"dropouts\": [0.5, 0.4]},\n",
    "                    {\"neurons\": [256, 64], \"dropouts\": [0.5, 0.4]},\n",
    "                    {\"neurons\": [256, 128], \"dropouts\": [0.5, 0.4]},\n",
    "                    {\"neurons\": [512, 128], \"dropouts\": [0.5, 0.4]},\n",
    "\n",
    "                    # --- Three Layer Focus ---\n",
    "                    {\"neurons\": [64, 64, 64], \"dropouts\": [0.4, 0.4, 0.4]},\n",
    "                    {\"neurons\": [128, 128, 128], \"dropouts\": [0.5, 0.5, 0.5]},\n",
    "                    {\"neurons\": [128, 64, 32], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                    {\"neurons\": [256, 128, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                    {\"neurons\": [256, 64, 32], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                    {\"neurons\": [512, 128, 32], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                ],\n",
    "\n",
    "                'Medium': [ # For Tg, Density. Balanced complexity.\n",
    "                    # --- Two Layer Focus ---\n",
    "                    {\"neurons\": [256, 64], \"dropouts\": [0.4, 0.3]},\n",
    "                    {\"neurons\": [256, 128], \"dropouts\": [0.4, 0.3]},\n",
    "                    {\"neurons\": [512, 64], \"dropouts\": [0.5, 0.3]},\n",
    "                    {\"neurons\": [512, 128], \"dropouts\": [0.5, 0.4]}, # Previous winner\n",
    "                    {\"neurons\": [512, 256], \"dropouts\": [0.5, 0.4]},\n",
    "                    {\"neurons\": [1024, 128], \"dropouts\": [0.5, 0.4]},\n",
    "                    {\"neurons\": [1024, 256], \"dropouts\": [0.5, 0.4]},\n",
    "                    {\"neurons\": [256, 256], \"dropouts\": [0.4, 0.4]},\n",
    "                    {\"neurons\": [512, 512], \"dropouts\": [0.5, 0.5]},\n",
    "\n",
    "                    # --- Three Layer Focus ---\n",
    "                    {\"neurons\": [256, 128, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "                    {\"neurons\": [512, 128, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                    {\"neurons\": [512, 256, 64], \"dropouts\": [0.5, 0.4, 0.2]},\n",
    "                    {\"neurons\": [512, 256, 128], \"dropouts\": [0.5, 0.4, 0.3]}, # Previous winner\n",
    "                    {\"neurons\": [1024, 256, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                    {\"neurons\": [1024, 512, 128], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                    {\"neurons\": [1024, 512, 256], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                    {\"neurons\": [256, 256, 256], \"dropouts\": [0.4, 0.4, 0.4]},\n",
    "                    {\"neurons\": [512, 512, 512], \"dropouts\": [0.5, 0.5, 0.5]},\n",
    "\n",
    "                    # --- Four Layer Focus ---\n",
    "                    {\"neurons\": [512, 256, 128, 64], \"dropouts\": [0.5, 0.4, 0.3, 0.2]},\n",
    "                    {\"neurons\": [1024, 512, 256, 128], \"dropouts\": [0.5, 0.4, 0.3, 0.2]},\n",
    "                ],\n",
    "\n",
    "                'High': [ # For FFV. Exploring width and depth.\n",
    "                    # --- Refining Around Winner ([512, 256]) ---\n",
    "                    {\"neurons\": [512, 128], \"dropouts\": [0.3, 0.2]},\n",
    "                    {\"neurons\": [512, 256], \"dropouts\": [0.3, 0.2]}, # Previous winner\n",
    "                    {\"neurons\": [512, 512], \"dropouts\": [0.3, 0.3]},\n",
    "                    {\"neurons\": [1024, 256], \"dropouts\": [0.4, 0.3]},\n",
    "                    {\"neurons\": [1024, 512], \"dropouts\": [0.4, 0.3]},\n",
    "                    {\"neurons\": [1024, 1024], \"dropouts\": [0.4, 0.4]},\n",
    "                    {\"neurons\": [2048, 512], \"dropouts\": [0.5, 0.4]},\n",
    "                    {\"neurons\": [2048, 1024], \"dropouts\": [0.5, 0.4]},\n",
    "\n",
    "                    # --- Three Layer Focus ---\n",
    "                    {\"neurons\": [512, 256, 128], \"dropouts\": [0.3, 0.2, 0.2]},\n",
    "                    {\"neurons\": [1024, 256, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "                    {\"neurons\": [1024, 512, 128], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "                    {\"neurons\": [1024, 512, 256], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "                    {\"neurons\": [2048, 512, 128], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                    {\"neurons\": [2048, 1024, 512], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                    {\"neurons\": [512, 512, 512], \"dropouts\": [0.3, 0.3, 0.3]},\n",
    "                    {\"neurons\": [1024, 1024, 1024], \"dropouts\": [0.4, 0.4, 0.4]},\n",
    "\n",
    "                    # --- Four+ Layer Focus ---\n",
    "                    {\"neurons\": [512, 256, 256, 128], \"dropouts\": [0.3, 0.2, 0.2, 0.1]},\n",
    "                    {\"neurons\": [1024, 512, 256, 128], \"dropouts\": [0.4, 0.3, 0.2, 0.2]},\n",
    "                    {\"neurons\": [1024, 512, 512, 256], \"dropouts\": [0.4, 0.3, 0.3, 0.2]},\n",
    "                    {\"neurons\": [512, 512, 512, 512], \"dropouts\": [0.3, 0.3, 0.3, 0.3]},\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Determine which set of configs to use\n",
    "            if label in ['Rg', 'Tc']:\n",
    "                configs_to_search = search_configs_by_label['Low']\n",
    "                print(\"Using search space for LOW data targets.\")\n",
    "            elif label == 'FFV':\n",
    "                configs_to_search = search_configs_by_label['High']\n",
    "                print(\"Using search space for HIGH data targets.\")\n",
    "            else: # Tg, Density\n",
    "                configs_to_search = search_configs_by_label['Medium']\n",
    "                print(\"Using search space for MEDIUM data targets.\")\n",
    "\n",
    "            results = []\n",
    "            for i, cfg in enumerate(configs_to_search):\n",
    "                print(f\"\\n---> Searching config {i+1}/{len(configs_to_search)}: Neurons={cfg['neurons']}, Dropouts={cfg['dropouts']}\")\n",
    "                model = FeedforwardNet(input_dim, cfg[\"neurons\"], cfg[\"dropouts\"])\n",
    "                model = train_model(model, X_train, X_val, y_train, y_val, verbose=False) # Turn off verbose for cleaner search logs\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    X_val_np = np.asarray(X_val) if isinstance(X_val, pd.DataFrame) else X_val\n",
    "                    device = next(model.parameters()).device\n",
    "                    X_val_tensor = torch.tensor(X_val_np, dtype=torch.float32).to(device)\n",
    "                    y_pred = model(X_val_tensor).cpu().numpy().flatten()\n",
    "                \n",
    "                val_mae = mean_absolute_error(y_val, y_pred)\n",
    "                print(f\"     Resulting Val MAE: {val_mae:.6f}\")\n",
    "                results.append({\"neurons\": cfg[\"neurons\"], \"dropouts\": cfg[\"dropouts\"], \"val_mae\": val_mae})\n",
    "            \n",
    "            df = pd.DataFrame(results)\n",
    "            print(\"\\n--- Neural Network Search Results ---\")\n",
    "            print(df.sort_values(by='val_mae').to_string(index=False))\n",
    "            \n",
    "            df.to_csv(f\"nn_config_validation_mae_{label}.csv\", index=False)\n",
    "            print(f\"\\nSaved results to nn_config_validation_mae_{label}.csv\")\n",
    "            \n",
    "            best_row = df.loc[df['val_mae'].idxmin()]\n",
    "            print(f\"\\nBest config: neurons={best_row['neurons']}, dropouts={best_row['dropouts']}, Validation MAE: {best_row['val_mae']:.6f}\")\n",
    "            \n",
    "            config = best_row.to_dict()\n",
    "            print(\"Re-training best model on the full training data...\")\n",
    "        \n",
    "        else: # If not searching, use a single pre-defined configuration\n",
    "            best_configs = {\n",
    "                \"Tg\":      {\"neurons\": [256, 128, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "                \"Density\": {\"neurons\": [256, 128, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "                \"FFV\":     {\"neurons\": [512, 256, 128], \"dropouts\": [0.3, 0.2, 0.2]},\n",
    "                \"Tc\":      {\"neurons\": [128, 64], \"dropouts\": [0.4, 0.3]},\n",
    "                \"Rg\":      {\"neurons\": [128, 64], \"dropouts\": [0.4, 0.3]},\n",
    "            }\n",
    "            config = best_configs.get(label)\n",
    "            print(f\"Using pre-defined best config for {label}: Neurons={config['neurons']}, Dropouts={config['dropouts']}\")\n",
    "\n",
    "        # Final model training\n",
    "        best_model = FeedforwardNet(input_dim, config[\"neurons\"], config[\"dropouts\"])\n",
    "        show_model_summary(best_model, input_dim)\n",
    "        best_model = train_model(best_model, X_train, X_val, y_train, y_val, verbose=True)\n",
    "        return best_model\n",
    "\n",
    "    # Utility to set random state everywhere\n",
    "    def set_global_random_seed(seed, config=None):\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        if config is not None:\n",
    "            config.random_state = seed\n",
    "\n",
    "    import hashlib\n",
    "\n",
    "    def stable_hash(obj, max_value=1_000_000):\n",
    "        \"\"\"\n",
    "        Deterministic hash for objects (e.g. labels).\n",
    "        Always returns the same value across runs/machines.\n",
    "        \"\"\"\n",
    "        # Convert to string and encode\n",
    "        s = str(obj).encode(\"utf-8\")\n",
    "        # Use MD5 (fast & deterministic)\n",
    "        h = hashlib.md5(s).hexdigest()\n",
    "        # Convert hex digest to int and limit range\n",
    "        return int(h, 16) % max_value\n",
    "\n",
    "    def train_and_evaluate_models(label, X_main, y_main, splits, nfold, Config):\n",
    "        \"\"\"\n",
    "        Trains models for the given label using the specified configuration.\n",
    "        Returns: models, fold_maes, mean_fold_mae, std_fold_mae\n",
    "        \"\"\"\n",
    "        # Use a prime multiplier for folds\n",
    "        FOLD_PRIME = 9973   # a large prime\n",
    "\n",
    "        models = []\n",
    "        fold_maes = []\n",
    "        mean_fold_mae = None\n",
    "        std_fold_mae = None\n",
    "\n",
    "        # Use stacking only if enabled in config\n",
    "        if getattr(Config, 'use_stacking', False):\n",
    "            Model = train_with_stacking(label, X_main, y_main)\n",
    "            models.append(Model)\n",
    "            save_model(Model, label, 1, Config.model_name)\n",
    "        elif Config.model_name in ['autogluon']:\n",
    "            Model = train_with_autogluon(label, X_main, y_main)\n",
    "            models.append(Model)\n",
    "            # save_model(Model, label, 1, Config.model_name)\n",
    "        else:\n",
    "            for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "                print(f\"\\n--- Fold {fold+1}/{nfold} ---\")\n",
    "                # Set a different random seed for each fold for best possible result\n",
    "                # Use a deterministic but varied seed: base + fold + hash(label)\n",
    "                base_seed = getattr(Config, 'random_state', 42)\n",
    "                label_hash = stable_hash(label)   # replaces abs(hash(label)) % 10000\n",
    "                fold_seed = base_seed + fold * FOLD_PRIME  + label_hash\n",
    "\n",
    "                set_global_random_seed(fold_seed, config=Config)\n",
    "\n",
    "                # Robustly handle both DataFrame and ndarray\n",
    "                if isinstance(X_main, np.ndarray):\n",
    "                    X_train, X_val = X_main[train_idx], X_main[val_idx]\n",
    "                else:\n",
    "                    X_train, X_val = X_main.iloc[train_idx], X_main.iloc[val_idx]\n",
    "                if isinstance(y_main, np.ndarray):\n",
    "                    y_train, y_val = y_main[train_idx], y_main[val_idx]\n",
    "                else:\n",
    "                    y_train, y_val = y_main.iloc[train_idx], y_main.iloc[val_idx]\n",
    "\n",
    "                if Config.model_name == 'xgb':\n",
    "                    Model = train_with_xgb(label, X_train, y_train, X_val, y_val)\n",
    "                elif Config.model_name in ['catboost', 'lgbm', 'extratrees', 'randomforest', 'balancedrf', 'tabnet', 'hgbm', 'autogluon', 'nn']:\n",
    "                    Model = train_with_other_models(Config.model_name, label, X_train, y_train, X_val, y_val)\n",
    "                else:\n",
    "                    assert False, \"No model present. Set Config.use_train_with_xgb = True to train a model.\"\n",
    "\n",
    "                # Save model for later holdout prediction\n",
    "                models.append(Model)\n",
    "                save_model(Model, label, fold, Config.model_name)\n",
    "\n",
    "                # Predict on validation set for this fold\n",
    "                if hasattr(Model, 'forward') and not hasattr(Model, 'predict'):\n",
    "                    Model.eval()\n",
    "                    X_val_np = np.asarray(X_val) if isinstance(X_val, pd.DataFrame) else X_val\n",
    "                    device = next(Model.parameters()).device\n",
    "                    with torch.no_grad():\n",
    "                        X_val_tensor = torch.tensor(X_val_np, dtype=torch.float32).to(device)\n",
    "                        y_val_pred = Model(X_val_tensor).cpu().numpy().flatten()\n",
    "                else:\n",
    "                    y_val_pred = Model.predict(X_val)\n",
    "\n",
    "                fold_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "                print(f\"Fold {fold+1} MAE (on validation set): {fold_mae}\")\n",
    "                fold_maes.append(fold_mae)\n",
    "                # Save y_val, y_val_pred, and residuals in sorted order for each fold\n",
    "                residuals = y_val - y_val_pred\n",
    "                results_df = pd.DataFrame({\n",
    "                    'y_val': y_val,\n",
    "                    'y_val_pred': y_val_pred,\n",
    "                    'residual': residuals\n",
    "                })\n",
    "                results_df = results_df.sort_values(by='residual', ascending=False).reset_index(drop=True)\n",
    "                os.makedirs(f'NeurIPS/fold_residuals/{label}', exist_ok=True)\n",
    "                results_df.to_csv(f'NeurIPS/fold_residuals/{label}/fold_{fold+1}_val_pred_residuals.csv', index=False)\n",
    "            mean_fold_mae = np.mean(fold_maes)\n",
    "            std_fold_mae = np.std(fold_maes)\n",
    "            print(f\"{label} 5-Fold CV mean_absolute_error (on validation sets): {mean_fold_mae} ± {std_fold_mae}\")\n",
    "\n",
    "        return models, fold_maes, mean_fold_mae, std_fold_mae\n",
    "\n",
    "    def save_feature_selection_info(label, kept_columns, least_important_features, correlated_features_dropped, scaler, X_holdout, y_holdout, median_values):\n",
    "        holdout_dir = f\"NeurIPS/feature_selection/{label}\"\n",
    "        os.makedirs(holdout_dir, exist_ok=True)\n",
    "        feature_info = {\n",
    "            \"kept_columns\": list(kept_columns),\n",
    "            \"least_important_features\": list(least_important_features),\n",
    "            \"correlated_features_dropped\": list(correlated_features_dropped),\n",
    "        }\n",
    "        # Save median_values if provided\n",
    "        if median_values is not None:\n",
    "            if hasattr(median_values, 'to_dict'):\n",
    "                feature_info[\"median_values\"] = median_values.to_dict()\n",
    "            else:\n",
    "                feature_info[\"median_values\"] = median_values\n",
    "\n",
    "        # Save X_holdout and y_holdout for this label\n",
    "        X_holdout_path = os.path.join(holdout_dir, \"X_holdout.csv\")\n",
    "        y_holdout_path = os.path.join(holdout_dir, \"y_holdout.csv\")\n",
    "        pd.DataFrame(X_holdout).to_csv(X_holdout_path, index=False)\n",
    "        pd.DataFrame({\"y_holdout\": y_holdout}).to_csv(y_holdout_path, index=False)\n",
    "\n",
    "        feature_info_path = os.path.join(holdout_dir, f\"{label}_feature_info.json\")\n",
    "        with open(feature_info_path, \"w\") as f:\n",
    "            json.dump(feature_info, f, indent=2)\n",
    "\n",
    "        # Save scaler object\n",
    "        scaler_path = os.path.join(holdout_dir, \"scaler.joblib\")\n",
    "        if scaler is not None:\n",
    "            joblib.dump(scaler, scaler_path)\n",
    "\n",
    "    def load_feature_selection_info(label, base_dir):\n",
    "        \"\"\"\n",
    "        Loads feature selection info saved by save_feature_selection_info for a given label.\n",
    "        Returns a dict with keys: kept_columns, least_important_features, correlated_features_dropped, scaler, X_holdout, y_holdout.\n",
    "        \"\"\"\n",
    "\n",
    "        holdout_dir = os.path.join(base_dir, f\"NeurIPS/feature_selection/{label}\")\n",
    "        feature_info_path = os.path.join(holdout_dir, f\"{label}_feature_info.json\")\n",
    "        X_holdout_path = os.path.join(holdout_dir, \"X_holdout.csv\")\n",
    "        y_holdout_path = os.path.join(holdout_dir, \"y_holdout.csv\")\n",
    "\n",
    "        if not os.path.exists(feature_info_path):\n",
    "            raise FileNotFoundError(f\"Feature info file not found: {feature_info_path}\")\n",
    "\n",
    "        with open(feature_info_path, \"r\") as f:\n",
    "            feature_info = json.load(f)\n",
    "\n",
    "        X_holdout = pd.read_csv(X_holdout_path)\n",
    "        y_holdout = pd.read_csv(y_holdout_path)[\"y_holdout\"].values\n",
    "\n",
    "        # Note: scaler is not restored as an object (only its params or type string is saved)\n",
    "        # If you need the actual scaler object, you must save it with joblib or pickle\n",
    "\n",
    "        # Load scaler object\n",
    "        scaler_path = os.path.join(holdout_dir, \"scaler.joblib\")\n",
    "        if os.path.exists(scaler_path):\n",
    "            scaler = joblib.load(scaler_path)\n",
    "        else:\n",
    "            scaler = None\n",
    "\n",
    "        # Try to load median_values if present in feature_info, else set to empty Series\n",
    "        if \"median_values\" in feature_info:\n",
    "            median_values = pd.Series(feature_info[\"median_values\"])\n",
    "        else:\n",
    "            median_values = pd.Series(dtype=float)\n",
    "        return {\n",
    "            \"kept_columns\": feature_info.get(\"kept_columns\", []),\n",
    "            \"least_important_features\": feature_info.get(\"least_important_features\", []),\n",
    "            \"correlated_features_dropped\": feature_info.get(\"correlated_features_dropped\", []),\n",
    "            \"X_holdout\": X_holdout,\n",
    "            \"y_holdout\": y_holdout,\n",
    "            \"scaler\": scaler,\n",
    "            \"median_values\": median_values\n",
    "        }\n",
    "\n",
    "    def load_models_for_label(label, models_dir=\"models\"):\n",
    "        \"\"\"\n",
    "        Loads all models for a given label from the specified directory.\n",
    "        Model filenames must start with the label (e.g., 'Tg_fold1_xgb.joblib').\n",
    "        Returns a list of loaded models.\n",
    "        \"\"\"\n",
    "\n",
    "        models = []\n",
    "        if not os.path.exists(models_dir):\n",
    "            print(f\"Models directory '{models_dir}' does not exist.\")\n",
    "            return models\n",
    "\n",
    "        # Match both .joblib and .pt (for torch) files\n",
    "        pattern_joblib = os.path.join(models_dir, f\"{label}_*.joblib\")\n",
    "        pattern_pt = os.path.join(models_dir, f\"{label}_*.pt\")\n",
    "        model_files = glob.glob(pattern_joblib) + glob.glob(pattern_pt)\n",
    "        if not model_files:\n",
    "            print(f\"No models found for label '{label}' in '{models_dir}'.\")\n",
    "            return models\n",
    "\n",
    "        for model_file in sorted(model_files):\n",
    "            if model_file.endswith(\".joblib\"):\n",
    "                try:\n",
    "                    model = joblib.load(model_file)\n",
    "                    models.append(model)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load model {model_file}: {e}\")\n",
    "            elif model_file.endswith(\".pt\"):\n",
    "                # Torch model loading requires model class and architecture\n",
    "                print(f\"Skipping torch model {model_file} (requires model class definition).\")\n",
    "                # You can implement torch loading here if needed\n",
    "        print(f\"Loaded {len(models)} models for label '{label}'.\")\n",
    "        return models\n",
    "\n",
    "    output_df = pd.DataFrame({\n",
    "        'id': test_ids\n",
    "    })\n",
    "\n",
    "    # --- Store and display mean_absolute_error for each label ---\n",
    "    mae_results = []\n",
    "\n",
    "    def prepare_label_data(label, subtables, Config):\n",
    "        print(f\"Processing label: {label}\")\n",
    "        print(subtables[label].head())\n",
    "        print(subtables[label].shape)\n",
    "        original_smiles = subtables[label]['SMILES'].tolist()\n",
    "        original_labels = subtables[label][label].values\n",
    "\n",
    "        # Canonicalize SMILES and deduplicate at molecule level before augmentation\n",
    "        canonical_smiles = [get_canonical_smiles(s) for s in original_smiles]\n",
    "        smiles_label_df = pd.DataFrame({\n",
    "            'SMILES': canonical_smiles,\n",
    "            'label': original_labels\n",
    "        })\n",
    "        before_dedup = len(smiles_label_df)\n",
    "        smiles_label_df = smiles_label_df.drop_duplicates(subset=['SMILES'], keep='first').reset_index(drop=True)\n",
    "        after_dedup = len(smiles_label_df)\n",
    "        num_dropped = before_dedup - after_dedup\n",
    "        print(f\"Dropped {num_dropped} duplicate SMILES rows for {label} before augmentation.\")\n",
    "        original_smiles = smiles_label_df['SMILES'].tolist()\n",
    "        original_labels = smiles_label_df['label'].values\n",
    "\n",
    "        if Config.use_augmentation and not Config.debug:\n",
    "            print(f\"SMILES before augmentation: {len(original_smiles)}\")\n",
    "            smiles_aug, labels_aug = augment_smiles_dataset(original_smiles, original_labels, num_augments=1)\n",
    "            print(f\"SMILES after augmentation: {len(smiles_aug)} (increase: {len(smiles_aug) - len(original_smiles)})\")\n",
    "            original_smiles, original_labels = smiles_aug, labels_aug\n",
    "\n",
    "        # After augmentation, deduplicate again at molecule level (canonical SMILES)\n",
    "        canonical_smiles_aug = [get_canonical_smiles(s) for s in original_smiles]\n",
    "        smiles_label_aug_df = pd.DataFrame({\n",
    "            'SMILES': canonical_smiles_aug,\n",
    "            'label': original_labels\n",
    "        })\n",
    "        smiles_label_aug_df = smiles_label_aug_df.drop_duplicates(subset=['SMILES'], keep='first').reset_index(drop=True)\n",
    "        original_smiles = smiles_label_aug_df['SMILES'].tolist()\n",
    "        original_labels = smiles_label_aug_df['label'].values\n",
    "\n",
    "        fp_df, descriptor_df, valid_smiles, invalid_indices = smiles_to_combined_fingerprints_with_descriptors(original_smiles)\n",
    "\n",
    "        print(f\"Invalid indices for {label}: {invalid_indices}\")\n",
    "        y = np.delete(original_labels, invalid_indices)\n",
    "        print(fp_df.shape)\n",
    "        fp_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if not descriptor_df.empty:\n",
    "            X = pd.DataFrame(descriptor_df)\n",
    "            X, kept_columns, scaler, median_values = preprocess_numerical_features(X, label)\n",
    "            X.reset_index(drop=True, inplace=True)\n",
    "            if not fp_df.empty:\n",
    "                X = pd.concat([X, fp_df], axis=1)\n",
    "        else:\n",
    "            kept_columns = []\n",
    "            scaler = None\n",
    "            X = fp_df\n",
    "\n",
    "        # Remove duplicate rows in X and corresponding values in y (feature-level duplicates)\n",
    "        X_dup = X.duplicated(keep='first')\n",
    "        if X_dup.any():\n",
    "            print(f\"Found {X_dup.sum()} duplicate rows in X for {label}, removing them.\")\n",
    "            X = X[~X_dup]\n",
    "            y = y[~X_dup]\n",
    "        print(f\"After concat: {X.shape}\")\n",
    "        # Fill NaN in train with median from train\n",
    "        # Only fill NaN with median if using neural network\n",
    "        if Config.model_name == 'nn':\n",
    "            X = X.fillna(median_values)\n",
    "        check_inf_nan(X, y, label)\n",
    "\n",
    "        # display_outlier_summary(y, X=X, name=label, z_thresh=3, iqr_factor=1.5, iso_contamination=0.01, lof_contamination=0.01)\n",
    "\n",
    "        # Drop least important features from X and test\n",
    "\n",
    "        least_important_features = []\n",
    "        if getattr(Config, 'use_least_important_features_all_methods', False):\n",
    "            for i in range(4):\n",
    "                print(f\"Iteration {i+1} for least important feature removal on {label}\")\n",
    "                least_important_feature = get_least_important_features_all_methods(X, y, label, model_name=Config.model_name)\n",
    "                least_important_features.extend(least_important_feature)\n",
    "                if len(least_important_feature) > 0:\n",
    "                    print(f\"label: {label} Dropping least important features: {least_important_feature}\")\n",
    "                    X = X.drop(columns=least_important_feature)\n",
    "                    print(f\"After dropping least important features: {X.shape}\")\n",
    "\n",
    "        check_inf_nan(X, y, label)\n",
    "        # Drop highly correlated features using label-specific correlation threshold from Config\n",
    "        correlation_threshold = Config.correlation_thresholds.get(label, 1.0)\n",
    "        if correlation_threshold < 1.0:\n",
    "            X, correlated_features_dropped = drop_correlated_features(pd.DataFrame(X), threshold=correlation_threshold)\n",
    "        else:\n",
    "            correlated_features_dropped = []\n",
    "\n",
    "        print(f\"After correlation cut (threshold={correlation_threshold}): {X.shape}, dropped columns: {correlated_features_dropped}\")\n",
    "        print(f\"After dropping correlated features: {X.shape}\")\n",
    "        check_inf_nan(X, y, label)\n",
    "\n",
    "        if getattr(Config, 'use_variance_threshold', False):\n",
    "            threshold = 0.01\n",
    "            selector = VarianceThreshold(threshold=threshold)\n",
    "            X_sel = selector.fit_transform(X)\n",
    "            # Get mask of selected features\n",
    "            selected_cols_variance = X.columns[selector.get_support()]\n",
    "            # Convert back to DataFrame with column names\n",
    "            X = pd.DataFrame(X_sel, columns=selected_cols_variance, index=X.index)\n",
    "            print(f\"After variance cut: {X.shape}\")\n",
    "            print(f'Type of X: {type(X)}')\n",
    "\n",
    "        if Config.add_gaussian and not Config.debug:\n",
    "            n_samples = 1000\n",
    "            X, y = augment_dataset(X, y, n_samples=n_samples)\n",
    "            print(f\"After augment cut: {X.shape}\")\n",
    "\n",
    "        # --- Hold out 10% for final MAE calculation ---\n",
    "        # X_main, X_holdout, y_main, y_holdout = train_test_split(X, y, test_size=0.1, random_state=Config.random_state)\n",
    "        # Bin y for stratification\n",
    "        y_bins = pd.qcut(y, q=5, duplicates='drop', labels=False)\n",
    "        X_main, X_holdout, y_main, y_holdout = train_test_split(X, y, test_size=0.10, random_state=Config.random_state, stratify=y_bins)\n",
    "        if Config.useAllDataForTraining == True:\n",
    "            X_main = X\n",
    "            y_main = y\n",
    "        # --- Optionally apply PCA ---\n",
    "        if getattr(Config, 'use_pca', False):\n",
    "            X_main, X_holdout, pca = apply_pca(X_main, X_holdout, verbose=True)\n",
    "        else:\n",
    "            pca = None\n",
    "\n",
    "        # --- Cross-Validation or Single Split (for speed) ---\n",
    "        fold_maes = []\n",
    "        test_preds = []\n",
    "        val_preds = np.zeros(len(y_main))\n",
    "        if getattr(Config, 'use_cross_validation', True):\n",
    "            nfold = 10\n",
    "            from sklearn.model_selection import StratifiedKFold\n",
    "            skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=Config.random_state)\n",
    "            # For regression, bin y_main for stratification\n",
    "            y_bins = pd.qcut(y_main, q=nfold, duplicates='drop', labels=False)\n",
    "            splits = skf.split(X_main, y_bins)\n",
    "        else:\n",
    "            # Use a single split: 80% train, 20% val\n",
    "            train_idx, val_idx = train_test_split(\n",
    "                np.arange(len(X_main)), test_size=0.2, random_state=Config.random_state\n",
    "            )\n",
    "            splits = [(train_idx, val_idx)]\n",
    "            nfold = 1\n",
    "\n",
    "        return {\n",
    "            \"X_main\": X_main,\n",
    "            \"X_holdout\": X_holdout,\n",
    "            \"y_main\": y_main,\n",
    "            \"y_holdout\": y_holdout,\n",
    "            \"kept_columns\": kept_columns,\n",
    "            \"scaler\": scaler,\n",
    "            \"median_values\": median_values,\n",
    "            \"least_important_features\": least_important_features,\n",
    "            \"correlated_features_dropped\": correlated_features_dropped,\n",
    "            \"selector\": selector if getattr(Config, 'use_variance_threshold', False) else None,\n",
    "            \"selected_cols_variance\": selected_cols_variance if getattr(Config, 'use_variance_threshold', False) else None,\n",
    "            \"pca\": pca,\n",
    "            \"splits\": splits,\n",
    "            \"nfold\": nfold,\n",
    "            \"fold_maes\": fold_maes,\n",
    "            \"test_preds\": test_preds,\n",
    "            \"val_preds\": val_preds\n",
    "        }\n",
    "\n",
    "    def load_label_data(label, model_dir=None):\n",
    "        if model_dir is not None:\n",
    "            # Load model and data for the specified label\n",
    "            model_path = os.path.join(model_dir, f\"{label}_model.pkl\")\n",
    "            data_path = os.path.join(model_dir, f\"{label}_data.pkl\")\n",
    "            model = joblib.load(model_path)\n",
    "            data = joblib.load(data_path)\n",
    "            return model, data\n",
    "        return None, None\n",
    "\n",
    "    def train_or_predict(train_model=True, model_dir=None):\n",
    "        for label in labels:\n",
    "            if train_model:\n",
    "                print(f\"\\n=== Training/Predicting for label: {label} ===\")\n",
    "                label_data = prepare_label_data(label, subtables, config)\n",
    "                X_main = label_data[\"X_main\"]\n",
    "                X_holdout = label_data[\"X_holdout\"]\n",
    "                y_main = label_data[\"y_main\"]\n",
    "                y_holdout = label_data[\"y_holdout\"]\n",
    "                kept_columns = label_data[\"kept_columns\"]\n",
    "                scaler = label_data[\"scaler\"]\n",
    "                median_values = label_data[\"median_values\"]\n",
    "                least_important_features = label_data[\"least_important_features\"]\n",
    "                correlated_features_dropped = label_data[\"correlated_features_dropped\"]\n",
    "                selector = label_data[\"selector\"]\n",
    "                selected_cols_variance = label_data[\"selected_cols_variance\"]\n",
    "                pca = label_data[\"pca\"]\n",
    "                splits = label_data[\"splits\"]\n",
    "                nfold = label_data[\"nfold\"]\n",
    "                fold_maes = label_data[\"fold_maes\"]\n",
    "                test_preds = label_data[\"test_preds\"]\n",
    "                val_preds = label_data[\"val_preds\"]\n",
    "\n",
    "                # --- Save feature selection info for this label ---\n",
    "                save_feature_selection_info(label, kept_columns, least_important_features, correlated_features_dropped, scaler, X_holdout, y_holdout, median_values)  \n",
    "                os.makedirs('models', exist_ok=True)\n",
    "                models = []\n",
    "\n",
    "                # labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "                # --- Hyperparameter tuning for this label ---\n",
    "                if Config.enable_param_tuning:\n",
    "                    if label == 'Tg':\n",
    "                        db_path = f\"xgb_tuning_{label}.db\"\n",
    "                        init_xgb_tuning_db(db_path)\n",
    "                        # Example param_grid (customize as needed)\n",
    "                        param_grid = {\n",
    "                            'n_estimators': [3000],\n",
    "                            'max_depth': [4, 5, 6, 7],\n",
    "                            'learning_rate': [0.001, 0.01, 0.06, 0.1],\n",
    "                            'subsample': [0.6, 0.8, 1.0],\n",
    "                            'colsample_bytree': [0.8, 1.0],\n",
    "                            'gamma': [0, 0.1],\n",
    "                            'reg_lambda': [1.0, 5.0, 10.0],\n",
    "                            'early_stopping_rounds': [50],\n",
    "                            'objective': [\"reg:squarederror\"],\n",
    "                            'eval_metric': [\"rmse\"]\n",
    "                        }\n",
    "                        # You must define X_train and y_train for tuning here\n",
    "                        xgb_grid_search_with_db(X_main, y_main, param_grid, db_path=db_path)\n",
    "                    else:\n",
    "                        continue\n",
    "                # FIXME save features_keep\n",
    "                # Ensure only features present in X_main are kept\n",
    "\n",
    "                # if label in ['Tg', 'Tc', 'Density', 'Rg']:\n",
    "                #     features_keep = select_features_with_lasso(X_main, y_main, label)\n",
    "                #     X_main = X_main[features_keep]\n",
    "                models, fold_maes, mean_fold_mae, std_fold_mae = train_and_evaluate_models(label, X_main, y_main, splits, nfold, Config)\n",
    "            else:\n",
    "                print(f\"\\n=== Loading models and data for label: {label} ===\")\n",
    "                mean_fold_mae, std_fold_mae = None, None\n",
    "                feature_info = load_feature_selection_info(label, model_dir)\n",
    "                kept_columns = feature_info[\"kept_columns\"]\n",
    "                least_important_features = feature_info[\"least_important_features\"]\n",
    "                correlated_features_dropped = feature_info[\"correlated_features_dropped\"]\n",
    "                scaler = feature_info.get(\"scaler\", None)\n",
    "                X_holdout = feature_info[\"X_holdout\"]\n",
    "                y_holdout = feature_info[\"y_holdout\"]\n",
    "                median_values = feature_info[\"median_values\"]\n",
    "                selector = None\n",
    "                selected_cols_variance = None\n",
    "                pca = None\n",
    "\n",
    "                models = load_models_for_label(label, os.path.join(model_dir, 'models'))\n",
    "                test_preds = []\n",
    "\n",
    "            # Prepare test set once\n",
    "            fp_df, descriptor_df, valid_smiles, invalid_indices = smiles_to_combined_fingerprints_with_descriptors(test_smiles)\n",
    "            # median_values = label_data[\"median_values\"]\n",
    "            if not descriptor_df.empty:\n",
    "                # Safely align test columns to training columns. \n",
    "                # This adds any missing columns and fills them with NaN.\n",
    "                descriptor_df = descriptor_df.reindex(columns=kept_columns)\n",
    "                if Config.model_name == 'nn':\n",
    "                    # Fill NaN in test with median from train\n",
    "                    descriptor_df = descriptor_df.fillna(median_values)\n",
    "                # Scale test set using the same scaler and kept_columns, then convert to DataFrame\n",
    "                if getattr(Config, 'use_standard_scaler', False) and scaler is not None:\n",
    "                    descriptor_df = pd.DataFrame(scaler.transform(descriptor_df), columns=kept_columns, index=descriptor_df.index)\n",
    "                descriptor_df.reset_index(drop=True, inplace=True)\n",
    "                if not fp_df.empty:\n",
    "                    fp_df = fp_df.reset_index(drop=True)\n",
    "                    test = pd.concat([descriptor_df, fp_df], axis=1)\n",
    "                else:\n",
    "                    test = descriptor_df\n",
    "            else:\n",
    "                test = fp_df\n",
    "\n",
    "            if len(least_important_features) > 0:\n",
    "                test = test.drop(columns=least_important_features)\n",
    "            if len(correlated_features_dropped) > 0:\n",
    "                print(f\"Dropping correlated columns from test: {correlated_features_dropped}\")\n",
    "                test = test.drop(correlated_features_dropped, axis=1, errors='ignore')\n",
    "            if getattr(Config, 'use_variance_threshold', False):\n",
    "                test_sel = selector.transform(test)\n",
    "                # Convert back to DataFrame\n",
    "                test = pd.DataFrame(test_sel, columns=selected_cols_variance, index=test.index)\n",
    "            # Optionally apply PCA to test set if enabled\n",
    "            if getattr(Config, 'use_pca', False):\n",
    "                test = pca.transform(test)\n",
    "            # if label in ['Tg', 'Tc', 'Density', 'Rg']:\n",
    "            #     X_holdout = X_holdout[features_keep]\n",
    "            #     test = test[features_keep]\n",
    "            # --- Holdout set evaluation with all trained models ---\n",
    "            holdout_maes = []\n",
    "            for i, Model in enumerate(models):\n",
    "                is_torch_model = hasattr(Model, 'forward') and not hasattr(Model, 'predict')\n",
    "\n",
    "                if is_torch_model:\n",
    "                    Model.eval()\n",
    "                    X_holdout_np = np.asarray(X_holdout) if isinstance(X_holdout, pd.DataFrame) else X_holdout\n",
    "                    test_np = np.asarray(test) if isinstance(test, pd.DataFrame) else test\n",
    "                    device = next(Model.parameters()).device\n",
    "                    with torch.no_grad():\n",
    "                        X_holdout_tensor = torch.tensor(X_holdout_np, dtype=torch.float32).to(device)\n",
    "                        test_tensor = torch.tensor(test_np, dtype=torch.float32).to(device)\n",
    "                        y_holdout_pred = Model(X_holdout_tensor).detach().cpu().numpy().flatten()\n",
    "                        y_test_pred = Model(test_tensor).detach().cpu().numpy().flatten()\n",
    "                else:\n",
    "                    y_holdout_pred = Model.predict(X_holdout)\n",
    "                    y_test_pred = Model.predict(test)\n",
    "\n",
    "                holdout_mae = mean_absolute_error(y_holdout, y_holdout_pred)\n",
    "                print(f\"Model {i+1} holdout MAE: {holdout_mae}\")\n",
    "                holdout_maes.append(holdout_mae)\n",
    "\n",
    "                if isinstance(y_test_pred, pd.Series):\n",
    "                    y_test_pred = y_test_pred.values.flatten()\n",
    "                else:\n",
    "                    y_test_pred = y_test_pred.flatten()        \n",
    "                test_preds.append(y_test_pred)\n",
    "\n",
    "            mean_holdout_mae = np.mean(holdout_maes)\n",
    "            std_holdout_mae = np.std(holdout_maes)\n",
    "            print(f\"{label} Holdout MAE (mean ± std over all models): {mean_holdout_mae:.5f} ± {std_holdout_mae:.5f}\")\n",
    "\n",
    "            mae_results.append({\n",
    "                'label': label,\n",
    "                'fold_mae_mean': mean_fold_mae,\n",
    "                'fold_mae_std': std_fold_mae,\n",
    "                'holdout_mae_mean': mean_holdout_mae,\n",
    "                'holdout_mae_std': std_holdout_mae\n",
    "            })\n",
    "\n",
    "            # Average test predictions across folds\n",
    "            test_preds = np.array(test_preds)\n",
    "            y_pred = np.mean(test_preds, axis=0)\n",
    "            print(y_pred)\n",
    "            new_column_name = label\n",
    "            output_df[new_column_name] = y_pred\n",
    "\n",
    "        # Save MAE results to CSV and display\n",
    "        mae_df = pd.DataFrame(mae_results)\n",
    "        mae_df.to_csv('NeurIPS/mae_results.csv', index=False)\n",
    "        print(\"\\nMean Absolute Error for each label:\")\n",
    "        print(mae_df)\n",
    "\n",
    "    # train_or_predict()\n",
    "\n",
    "    # output_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "    # output_df = pd.DataFrame({\n",
    "    #     'id': test_ids\n",
    "    # })\n",
    "    # MODEL_DIR1 = '/kaggle/input/neurips-2025/nn_v4'\n",
    "    # train_or_predict(train_model=False, model_dir=MODEL_DIR1)\n",
    "    # # output_dfs.append(train_or_predict(output_df, train_model=False, model_dir=MODEL_DIR1))\n",
    "    # print(output_df)\n",
    "    # output_dfs.append(output_df.copy())\n",
    "\n",
    "\n",
    "    output_df = pd.DataFrame({\n",
    "        'id': test_ids\n",
    "    })\n",
    "    MODEL_DIR1 = '/kaggle/input/neurips-2025/xgb_v3'\n",
    "    train_or_predict(train_model=False, model_dir=MODEL_DIR1)\n",
    "    print(output_df)\n",
    "    output_dfs.append(output_df.copy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"[Code A] Finished.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Code B\n",
    "# ----------------------------\n",
    "def run_code_b():\n",
    "    \"\"\"Wrapped Code B execution\"\"\"\n",
    "    # ===============================================================\n",
    "    # FULL CONTENT OF CODE B GOES HERE\n",
    "    # (all functions, model training, submission build, etc.)\n",
    "    # Deduplicated imports and globals already handled above.\n",
    "    # ===============================================================\n",
    "    print(\"\\n[Code B] Starting...\")\n",
    "    \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import joblib\n",
    "    import os\n",
    "    import torch\n",
    "    from torch_geometric.data import Data\n",
    "    from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "    from torch_geometric.nn import GCNConv, GINEConv, global_mean_pool, global_max_pool\n",
    "    import torch.nn.functional as F\n",
    "    import warnings\n",
    "    import json\n",
    "    import torch\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    import json\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "\n",
    "    RDKIT_AVAILABLE = True\n",
    "    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "    os.makedirs(\"NeurIPS\", exist_ok=True)\n",
    "    class Config:\n",
    "        debug = False\n",
    "        use_cross_validation = True  # Set to False to use a single split for speed\n",
    "        use_external_data = True  # Set to True to use external datasets\n",
    "        random_state = 42\n",
    "\n",
    "    # Create a single config instance to use everywhere\n",
    "    config = Config()\n",
    "\n",
    "    \"\"\"\n",
    "    Load competition data with complete filtering of problematic polymer notation\n",
    "    \"\"\"\n",
    "    print(\"Loading competition data...\")\n",
    "    train = pd.read_csv(BASE_PATH + 'train.csv')\n",
    "    test = pd.read_csv(BASE_PATH + 'test.csv')\n",
    "\n",
    "    if config.debug:\n",
    "        print(\"   Debug mode: sampling 1000 training examples\")\n",
    "        train = train.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training data shape: {train.shape}, Test data shape: {test.shape}\")\n",
    "\n",
    "    def clean_and_validate_smiles(smiles):\n",
    "        \"\"\"Completely clean and validate SMILES, removing all problematic patterns\"\"\"\n",
    "        if not isinstance(smiles, str) or len(smiles) == 0:\n",
    "            return None\n",
    "        \n",
    "        # List of all problematic patterns we've seen\n",
    "        bad_patterns = [\n",
    "            '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', \n",
    "            \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n",
    "            # Additional patterns that cause issues\n",
    "            '([R])', '([R1])', '([R2])', \n",
    "        ]\n",
    "        \n",
    "        for pattern in bad_patterns:\n",
    "            if pattern in smiles:\n",
    "                return None\n",
    "        \n",
    "        # Additional check: if it contains ] followed by [ without valid atoms, likely polymer notation\n",
    "        if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):\n",
    "            return None\n",
    "        \n",
    "        # Try to parse with RDKit if available\n",
    "        if RDKIT_AVAILABLE:\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    return Chem.MolToSmiles(mol, canonical=True)\n",
    "                else:\n",
    "                    return None\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        # If RDKit not available, return cleaned SMILES\n",
    "        return smiles\n",
    "\n",
    "    # Clean and validate all SMILES\n",
    "    print(\"Cleaning and validating SMILES...\")\n",
    "    train['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\n",
    "    test['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n",
    "\n",
    "    # Remove invalid SMILES\n",
    "    invalid_train = train['SMILES'].isnull().sum()\n",
    "    invalid_test = test['SMILES'].isnull().sum()\n",
    "\n",
    "    print(f\"   Removed {invalid_train} invalid SMILES from training data\")\n",
    "    print(f\"   Removed {invalid_test} invalid SMILES from test data\")\n",
    "\n",
    "    train = train[train['SMILES'].notnull()].reset_index(drop=True)\n",
    "    test = test[test['SMILES'].notnull()].reset_index(drop=True)\n",
    "\n",
    "    print(f\"   Final training samples: {len(train)}\")\n",
    "    print(f\"   Final test samples: {len(test)}\")\n",
    "\n",
    "    def add_extra_data_clean(df_train, df_extra, target):\n",
    "        \"\"\"Add external data with thorough SMILES cleaning\"\"\"\n",
    "        n_samples_before = len(df_train[df_train[target].notnull()])\n",
    "        \n",
    "        print(f\"      Processing {len(df_extra)} {target} samples...\")\n",
    "        \n",
    "        # Clean external SMILES\n",
    "        df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n",
    "        \n",
    "        # Remove invalid SMILES and missing targets\n",
    "        before_filter = len(df_extra)\n",
    "        df_extra = df_extra[df_extra['SMILES'].notnull()]\n",
    "        df_extra = df_extra.dropna(subset=[target])\n",
    "        after_filter = len(df_extra)\n",
    "        \n",
    "        print(f\"      Kept {after_filter}/{before_filter} valid samples\")\n",
    "        \n",
    "        if len(df_extra) == 0:\n",
    "            print(f\"      No valid data remaining for {target}\")\n",
    "            return df_train\n",
    "        \n",
    "        # Group by canonical SMILES and average duplicates\n",
    "        df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "        \n",
    "        cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n",
    "        unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n",
    "\n",
    "        # Fill missing values\n",
    "        filled_count = 0\n",
    "        for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n",
    "            if smile in cross_smiles:\n",
    "                df_train.loc[df_train['SMILES']==smile, target] = \\\n",
    "                    df_extra[df_extra['SMILES']==smile][target].values[0]\n",
    "                filled_count += 1\n",
    "        \n",
    "        # Add unique SMILES\n",
    "        extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n",
    "        if len(extra_to_add) > 0:\n",
    "            for col in TARGETS:\n",
    "                if col not in extra_to_add.columns:\n",
    "                    extra_to_add[col] = np.nan\n",
    "            \n",
    "            extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n",
    "            df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n",
    "\n",
    "        n_samples_after = len(df_train[df_train[target].notnull()])\n",
    "        print(f'      {target}: +{n_samples_after-n_samples_before} samples, +{len(unique_smiles_extra)} unique SMILES')\n",
    "        print(f\"      Filled {filled_count} missing entries in train for {target}\")\n",
    "        print(f\"      Added {len(extra_to_add)} new entries for {target}\")\n",
    "        return df_train\n",
    "\n",
    "    # Load external datasets with robust error handling\n",
    "    print(\"\\n📂 Loading external datasets...\")\n",
    "\n",
    "    external_datasets = []\n",
    "\n",
    "    # Function to safely load datasets\n",
    "    def safe_load_dataset(path, target, processor_func, description):\n",
    "        try:\n",
    "            if path.endswith('.xlsx'):\n",
    "                data = pd.read_excel(path)\n",
    "            else:\n",
    "                data = pd.read_csv(path)\n",
    "            \n",
    "            data = processor_func(data)\n",
    "            external_datasets.append((target, data))\n",
    "            print(f\"   ✅ {description}: {len(data)} samples\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ {description} failed: {str(e)[:100]}\")\n",
    "            return False\n",
    "\n",
    "    # Load each dataset\n",
    "    safe_load_dataset(\n",
    "        '/kaggle/input/tc-smiles/Tc_SMILES.csv',\n",
    "        'Tc',\n",
    "        lambda df: df.rename(columns={'TC_mean': 'Tc'}),\n",
    "        'Tc data'\n",
    "    )\n",
    "\n",
    "    safe_load_dataset(\n",
    "        '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv',\n",
    "        'Tg', \n",
    "        lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,\n",
    "        'TgSS enriched data'\n",
    "    )\n",
    "\n",
    "    safe_load_dataset(\n",
    "        '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv',\n",
    "        'Tg',\n",
    "        lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),\n",
    "        'JCIM Tg data'\n",
    "    )\n",
    "\n",
    "    safe_load_dataset(\n",
    "        '/kaggle/input/smiles-extra-data/data_tg3.xlsx',\n",
    "        'Tg',\n",
    "        lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),\n",
    "        'Xlsx Tg data'\n",
    "    )\n",
    "\n",
    "    safe_load_dataset(\n",
    "        '/kaggle/input/smiles-extra-data/data_dnst1.xlsx',\n",
    "        'Density',\n",
    "        lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n",
    "                    .query('SMILES.notnull() and Density.notnull() and Density != \"nylon\"')\n",
    "                    .assign(Density=lambda x: x['Density'].astype(float) - 0.118),\n",
    "        'Density data'\n",
    "    )\n",
    "\n",
    "    safe_load_dataset(\n",
    "        BASE_PATH + 'train_supplement/dataset4.csv',\n",
    "        'FFV', \n",
    "        lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,\n",
    "        'dataset 4'\n",
    "    )\n",
    "\n",
    "    # Integrate external data\n",
    "    print(\"\\n🔄 Integrating external data...\")\n",
    "    train_extended = train[['SMILES'] + TARGETS].copy()\n",
    "\n",
    "    if getattr(config, \"use_external_data\", True) and  not config.debug:\n",
    "        for target, dataset in external_datasets:\n",
    "            print(f\"   Processing {target} data...\")\n",
    "            train_extended = add_extra_data_clean(train_extended, dataset, target)\n",
    "\n",
    "    print(f\"\\n📊 Final training data:\")\n",
    "    print(f\"   Original samples: {len(train)}\")\n",
    "    print(f\"   Extended samples: {len(train_extended)}\")\n",
    "    print(f\"   Gain: +{len(train_extended) - len(train)} samples\")\n",
    "\n",
    "    for target in TARGETS:\n",
    "        count = train_extended[target].notna().sum()\n",
    "        original_count = train[target].notna().sum() if target in train.columns else 0\n",
    "        gain = count - original_count\n",
    "        print(f\"   {target}: {count:,} samples (+{gain})\")\n",
    "\n",
    "    print(f\"\\n✅ Data integration complete with clean SMILES!\")\n",
    "\n",
    "    def separate_subtables(train_df):\n",
    "        labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "        subtables = {}\n",
    "        for label in labels:\n",
    "            # Filter out NaNs, select columns, reset index\n",
    "            subtables[label] = train_df[train_df[label].notna()][['SMILES', label]].reset_index(drop=True)\n",
    "\n",
    "        return subtables\n",
    "\n",
    "    def augment_smiles_dataset(smiles_list, labels, num_augments=3):\n",
    "        \"\"\"\n",
    "        Augments a list of SMILES strings by generating randomized versions.\n",
    "\n",
    "        Parameters:\n",
    "            smiles_list (list of str): Original SMILES strings.\n",
    "            labels (list or np.array): Corresponding labels.\n",
    "            num_augments (int): Number of augmentations per SMILES.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (augmented_smiles, augmented_labels)\n",
    "        \"\"\"\n",
    "        augmented_smiles = []\n",
    "        augmented_labels = []\n",
    "\n",
    "        for smiles, label in zip(smiles_list, labels):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "            # Add original\n",
    "            augmented_smiles.append(smiles)\n",
    "            augmented_labels.append(label)\n",
    "            # Add randomized versions\n",
    "            for _ in range(num_augments):\n",
    "                rand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n",
    "                augmented_smiles.append(rand_smiles)\n",
    "                augmented_labels.append(label)\n",
    "\n",
    "        return augmented_smiles, np.array(augmented_labels)\n",
    "\n",
    "    required_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n",
    "\n",
    "    def augment_dataset(X, y, n_samples=1000, n_components=5, random_state=None):\n",
    "        \"\"\"\n",
    "        Augments a dataset using Gaussian Mixture Models.\n",
    "\n",
    "        Parameters:\n",
    "        - X: pd.DataFrame or np.ndarray — feature matrix\n",
    "        - y: pd.Series or np.ndarray — target values\n",
    "        - n_samples: int — number of synthetic samples to generate\n",
    "        - n_components: int — number of GMM components\n",
    "        - random_state: int — random seed for reproducibility\n",
    "\n",
    "        Returns:\n",
    "        - X_augmented: pd.DataFrame — augmented feature matrix\n",
    "        - y_augmented: pd.Series — augmented target values\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        elif not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X must be a pandas DataFrame or a NumPy array\")\n",
    "\n",
    "        X.columns = X.columns.astype(str)\n",
    "\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = pd.Series(y)\n",
    "        elif not isinstance(y, pd.Series):\n",
    "            raise ValueError(\"y must be a pandas Series or a NumPy array\")\n",
    "\n",
    "        df = X.copy()\n",
    "        df['Target'] = y.values\n",
    "\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "        gmm.fit(df)\n",
    "\n",
    "        synthetic_data, _ = gmm.sample(n_samples)\n",
    "        synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "        augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n",
    "\n",
    "        X_augmented = augmented_df.drop(columns='Target')\n",
    "        y_augmented = augmented_df['Target']\n",
    "\n",
    "        return X_augmented, y_augmented\n",
    "\n",
    "\n",
    "    train_df=train_extended\n",
    "    test_df=test\n",
    "    subtables = separate_subtables(train_df)\n",
    "\n",
    "    test_smiles = test_df['SMILES'].tolist()\n",
    "    test_ids = test_df['id'].values\n",
    "    labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # --- GNN MODEL AND DATA PREPARATION ---\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # A dictionary to map atom symbols to integer indices for the GNN\n",
    "    ATOM_MAP = {\n",
    "        'C': 0, 'N': 1, 'O': 2, 'F': 3, 'P': 4, 'S': 5, 'Cl': 6, 'Br': 7, 'I': 8, 'H': 9,\n",
    "        # --- NEWLY ADDED SYMBOLS ---\n",
    "        'Si': 10, # Silicon\n",
    "        'Na': 11, # Sodium\n",
    "        '*' : 12, # Wildcard atom\n",
    "        # --- NEWLY ADDED SYMBOLS ---\n",
    "        'B': 13,  # Boron\n",
    "        'Ge': 14, # Germanium\n",
    "        'Sn': 15, # Tin\n",
    "        'Se': 16, # Selenium\n",
    "        'Te': 17, # Tellurium\n",
    "        'Ca': 18, # Calcium\n",
    "        'Cd': 19, # Cadmium\n",
    "    }\n",
    "\n",
    "    def smiles_to_graph(smiles_str: str, y_val=None):\n",
    "        \"\"\"\n",
    "        Converts a SMILES string to a graph, adding selected global\n",
    "        molecular features to each node's feature vector.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles_str)\n",
    "            if mol is None: return None\n",
    "\n",
    "            # 1. Calculate global features once per molecule\n",
    "            global_features = [\n",
    "                Descriptors.MolWt(mol),\n",
    "                Descriptors.TPSA(mol),\n",
    "                Descriptors.NumRotatableBonds(mol),\n",
    "                Descriptors.MolLogP(mol)\n",
    "            ]\n",
    "\n",
    "            node_features = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                # Initialize atom-specific features (one-hot encoding)\n",
    "                atom_features = [0] * len(ATOM_MAP)\n",
    "                symbol = atom.GetSymbol()\n",
    "                if symbol in ATOM_MAP:\n",
    "                    atom_features[ATOM_MAP[symbol]] = 1\n",
    "\n",
    "                # Add other standard atom features\n",
    "                atom_features.extend([\n",
    "                    atom.GetAtomicNum(),\n",
    "                    atom.GetTotalDegree(),\n",
    "                    atom.GetFormalCharge(),\n",
    "                    atom.GetTotalNumHs(),\n",
    "                    int(atom.GetIsAromatic())\n",
    "                ])\n",
    "                \n",
    "                # 2. Append the global features to each atom's feature vector\n",
    "                atom_features.extend(global_features)\n",
    "                \n",
    "                node_features.append(atom_features)\n",
    "            \n",
    "            if not node_features: return None\n",
    "            x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "            edge_indices, edge_attrs = [], []\n",
    "            for bond in mol.GetBonds():\n",
    "                i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                edge_indices.extend([(i, j), (j, i)])\n",
    "                bond_type = bond.GetBondTypeAsDouble()\n",
    "                edge_attrs.extend([[bond_type], [bond_type]])\n",
    "\n",
    "            if not edge_indices:\n",
    "                edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "                edge_attr = torch.empty((0, 1), dtype=torch.float)\n",
    "            else:\n",
    "                edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "                edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "            if y_val is not None:\n",
    "                y_tensor = torch.tensor([[y_val]], dtype=torch.float)\n",
    "                return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y_tensor)\n",
    "            else:\n",
    "                return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    from rdkit.Chem import Descriptors\n",
    "\n",
    "    # A dictionary mapping labels to their most important global features from XGBoost\n",
    "    LABEL_SPECIFIC_FEATURES = {\n",
    "        'Tg': [\n",
    "            \"HallKierAlpha\", # Topological charge index\n",
    "            \"MolLogP\",       # Lipophilicity\n",
    "            \"NumRotatableBonds\", # Flexibility\n",
    "            \"TPSA\",          # Polarity\n",
    "        ],\n",
    "        'FFV': [\n",
    "            \"NHOHCount\",     # Count of NH and OH groups (H-bonding)\n",
    "            \"NumRotatableBonds\",\n",
    "            \"MolWt\",         # Size\n",
    "            \"TPSA\",\n",
    "        ],\n",
    "        'Tc': [\n",
    "            \"MolLogP\",\n",
    "            \"NumValenceElectrons\",\n",
    "            \"SPS\",           # Molecular shape index\n",
    "            \"MolWt\",\n",
    "        ],\n",
    "        'Density': [\n",
    "            \"MolWt\",\n",
    "            \"MolMR\",         # Molar refractivity (related to volume)\n",
    "            \"FractionCSP3\",  # Proportion of sp3 hybridized carbons (related to saturation)\n",
    "            \"NumHeteroatoms\",\n",
    "        ],\n",
    "        'Rg': [\n",
    "            \"HallKierAlpha\",\n",
    "            \"MolWt\",\n",
    "            \"NumValenceElectrons\",\n",
    "            \"qed\",           # Quantitative Estimation of Drug-likeness\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # A helper dictionary to easily call RDKit functions from their string names\n",
    "    RDKIT_DESC_CALCULATORS = {name: func for name, func in Descriptors.descList}\n",
    "    RDKIT_DESC_CALCULATORS['qed'] = Descriptors.qed # Add qed as it's not in the default list\n",
    "\n",
    "    from rdkit import Chem\n",
    "    import numpy as np\n",
    "\n",
    "    # This ATOM_MAP dictionary must be defined globally in your script (it already is)\n",
    "    # ATOM_MAP = {'C': 0, 'N': 1, ...}\n",
    "\n",
    "    def smiles_to_graph_label_specific(smiles_str: str, label: str, y_val=None):\n",
    "        \"\"\"\n",
    "        (BASELINE VERSION - SIMPLE FEATURES)\n",
    "        - This is the original hybrid GNN featurizer that produced your best score.\n",
    "        - Node Features (x): Atom one-hot (20) + 5 atom features = 25 features.\n",
    "        - Edge Features (edge_attr): Bond type as double = 1 feature.\n",
    "        - Global Features (u): Label-specific descriptors are stored separately in 'data.u'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles_str)\n",
    "            if mol is None: \n",
    "                return None\n",
    "\n",
    "            # --- 1. Calculate and store label-specific GLOBAL features ---\n",
    "            global_features = []\n",
    "            features_to_calculate = LABEL_SPECIFIC_FEATURES.get(label, [])\n",
    "            \n",
    "            for feature_name in features_to_calculate:\n",
    "                calculator_func = RDKIT_DESC_CALCULATORS.get(feature_name)\n",
    "                if calculator_func:\n",
    "                    try:\n",
    "                        val = calculator_func(mol)\n",
    "                        # Ensure value is valid, replace inf/nan with 0\n",
    "                        global_features.append(val if np.isfinite(val) else 0.0)\n",
    "                    except Exception as e:\n",
    "                        global_features.append(0.0)\n",
    "                else:\n",
    "                    global_features.append(0.0)\n",
    "\n",
    "            # --- 2. Create Node Features (SIMPLE) ---\n",
    "            node_features = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                # One-Hot Symbol (len 20, from global ATOM_MAP)\n",
    "                atom_features = [0] * len(ATOM_MAP)\n",
    "                symbol = atom.GetSymbol()\n",
    "                if symbol in ATOM_MAP:\n",
    "                    atom_features[ATOM_MAP[symbol]] = 1\n",
    "\n",
    "                # Standard Features (len 5)\n",
    "                atom_features.extend([\n",
    "                    atom.GetAtomicNum(),\n",
    "                    atom.GetTotalDegree(),\n",
    "                    atom.GetFormalCharge(),\n",
    "                    atom.GetTotalNumHs(),\n",
    "                    int(atom.GetIsAromatic())\n",
    "                ])\n",
    "                # Total features = 25\n",
    "                node_features.append(atom_features)\n",
    "            \n",
    "            if not node_features: return None\n",
    "            x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "            # --- 3. Create Edge Features (SIMPLE) ---\n",
    "            edge_indices, edge_attrs = [], []\n",
    "            for bond in mol.GetBonds():\n",
    "                i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                edge_indices.extend([(i, j), (j, i)])\n",
    "                bond_type = bond.GetBondTypeAsDouble()\n",
    "                edge_attrs.extend([[bond_type], [bond_type]]) # 1-dim feature\n",
    "\n",
    "            if not edge_indices:\n",
    "                edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "                edge_attr = torch.empty((0, 1), dtype=torch.float) # Shape (0, 1)\n",
    "            else:\n",
    "                edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "                edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "            # --- 4. Create Data Object ---\n",
    "            data_obj = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            data_obj.u = torch.tensor([global_features], dtype=torch.float) # Store globals in 'u'\n",
    "\n",
    "            if y_val is not None:\n",
    "                data_obj.y = torch.tensor([[y_val]], dtype=torch.float)\n",
    "            \n",
    "            return data_obj\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catch any other unexpected molecule-level errors\n",
    "            print(f\"CRITICAL ERROR converting SMILES '{smiles_str}': {e}\")\n",
    "            return None\n",
    "                \n",
    "    class GNNModel(torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Defines the Graph Neural Network architecture.\n",
    "        \"\"\"\n",
    "        def __init__(self, num_node_features, hidden_channels=128):\n",
    "            super(GNNModel, self).__init__()\n",
    "            torch.manual_seed(42)\n",
    "            \n",
    "            self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "            self.conv2 = GCNConv(hidden_channels, hidden_channels * 2)\n",
    "            self.conv3 = GCNConv(hidden_channels * 2, hidden_channels * 4)\n",
    "            self.lin = torch.nn.Linear(hidden_channels * 4, 1)\n",
    "\n",
    "        def forward(self, data):\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            \n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = F.relu(self.conv2(x, edge_index))\n",
    "            x = self.conv3(x, edge_index)\n",
    "            x = global_max_pool(x, batch) # Aggregate node features to get a graph-level embedding\n",
    "            x = F.dropout(x, p=0.25, training=self.training)\n",
    "            x = self.lin(x)\n",
    "            \n",
    "            return x\n",
    "\n",
    "\n",
    "    def predict_with_gnn(trained_model, test_smiles):\n",
    "        \"\"\"\n",
    "        Uses a pre-trained GNN model to make predictions on a list of test SMILES.\n",
    "        \"\"\"\n",
    "        if trained_model is None:\n",
    "            print(\"Prediction skipped because the GNN model is invalid.\")\n",
    "            return np.full(len(test_smiles), np.nan)\n",
    "\n",
    "        print(\"--- Making predictions with trained GNN... ---\")\n",
    "        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Convert test SMILES to graph data\n",
    "        test_data_list = [smiles_to_graph(s) for s in test_smiles]\n",
    "        \n",
    "        # We need to keep track of which original indices are valid\n",
    "        valid_indices = [i for i, data in enumerate(test_data_list) if data is not None]\n",
    "        valid_test_data = [data for data in test_data_list if data is not None]\n",
    "\n",
    "        if not valid_test_data:\n",
    "            print(\"Warning: No valid test molecules could be converted to graphs.\")\n",
    "            return np.full(len(test_smiles), np.nan)\n",
    "            \n",
    "        test_loader = PyGDataLoader(valid_test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "        trained_model.eval()\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                data = data.to(DEVICE)\n",
    "                out = trained_model(data)\n",
    "                all_preds.append(out.cpu())\n",
    "\n",
    "        # Combine predictions from all batches\n",
    "        test_preds_tensor = torch.cat(all_preds, dim=0).numpy().flatten()\n",
    "        \n",
    "        # Create a full-sized prediction array and fill in the values at their original positions\n",
    "        final_predictions = np.full(len(test_smiles), np.nan)\n",
    "        if len(test_preds_tensor) == len(valid_indices):\n",
    "            final_predictions[valid_indices] = test_preds_tensor\n",
    "        else:\n",
    "            print(f\"Warning: Mismatch in GNN prediction count. This can happen with invalid SMILES.\")\n",
    "            fill_count = min(len(valid_indices), len(test_preds_tensor))\n",
    "            final_predictions[valid_indices[:fill_count]] = test_preds_tensor[:fill_count]\n",
    "\n",
    "        return final_predictions\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    def save_gnn_model(model, label, model_dir=\"models/gnn\"):\n",
    "        \"\"\"\n",
    "        (MODIFIED) Saves the GNN model state_dict and its full constructor config.\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            print(f\"Skipping save for {label}, model is None.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        model_path = os.path.join(model_dir, f\"gnn_model_{label}.pth\")\n",
    "        config_path = os.path.join(model_dir, f\"gnn_config_{label}.json\")\n",
    "\n",
    "        # Save the model parameters (the weights)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        # Save the full configuration dictionary\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(model.config_args, f, indent=4)\n",
    "            \n",
    "        print(f\"Saved final model for {label} to {model_path}\")\n",
    "\n",
    "\n",
    "    def load_gnn_model(label, model_dir=\"models/gnn\"):\n",
    "        \"\"\"\n",
    "        (MODIFIED) Loads a saved GNN model using its full config file.\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(model_dir, f\"gnn_model_{label}.pth\")\n",
    "        config_path = os.path.join(model_dir, f\"gnn_config_{label}.json\")\n",
    "        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        if not os.path.exists(model_path) or not os.path.exists(config_path):\n",
    "            print(f\"Warning: Model or config file not found for {label}. Cannot load model.\")\n",
    "            return None\n",
    "\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        try:\n",
    "            # Re-initialize the model using all saved config args via dictionary unpacking\n",
    "            model = TaskSpecificGNN(**config).to(DEVICE)\n",
    "            \n",
    "            # Load the saved model weights\n",
    "            model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "            model.eval() # Set model to evaluation mode\n",
    "            print(f\"Successfully loaded saved model for {label} from {model_path}\")\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR loading model for {label}: {e}\")\n",
    "            print(\"This may be due to a mismatch between the saved model and the current model class definition.\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "    def create_dynamic_mlp(input_dim, layer_list, dropout_list):\n",
    "        \"\"\"\n",
    "        Helper function to dynamically build the task-specific MLP.\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        for neurons, dropout in zip(layer_list, dropout_list):\n",
    "            layers.append(torch.nn.Linear(current_dim, neurons))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(dropout))\n",
    "            current_dim = neurons\n",
    "            \n",
    "        # Add the final single-output prediction layer\n",
    "        layers.append(torch.nn.Linear(current_dim, 1))\n",
    "        \n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "    class TaskSpecificGNN(torch.nn.Module):\n",
    "        def __init__(self, num_node_features, num_edge_features, num_global_features,\n",
    "                    hidden_channels_gnn, mlp_neurons, mlp_dropouts, heads=8):\n",
    "            super().__init__()\n",
    "            torch.manual_seed(42)\n",
    "\n",
    "            # --- 1. GNN Backbone (Using GATConv, No BatchNorm) ---\n",
    "            self.convs = torch.nn.ModuleList()\n",
    "\n",
    "            # Layer 1\n",
    "            self.convs.append(\n",
    "                GATConv(num_node_features, hidden_channels_gnn, heads=heads,\n",
    "                        edge_dim=num_edge_features)\n",
    "            )\n",
    "\n",
    "            # Layer 2\n",
    "            self.convs.append(\n",
    "                GATConv(hidden_channels_gnn * heads, hidden_channels_gnn * 2, heads=heads,\n",
    "                        edge_dim=num_edge_features)\n",
    "            )\n",
    "\n",
    "            # Layer 3 (Final GNN layer)\n",
    "            self.convs.append(\n",
    "                GATConv(hidden_channels_gnn * 2 * heads, hidden_channels_gnn * 4, heads=heads,\n",
    "                        concat=False, edge_dim=num_edge_features)\n",
    "            )\n",
    "\n",
    "            gnn_output_dim = hidden_channels_gnn * 4\n",
    "\n",
    "            # --- 2. Readout Head ---\n",
    "            combined_feature_size = gnn_output_dim + num_global_features\n",
    "\n",
    "            self.readout_mlp = create_dynamic_mlp(\n",
    "                input_dim=combined_feature_size,\n",
    "                layer_list=mlp_neurons,\n",
    "                dropout_list=mlp_dropouts\n",
    "            )\n",
    "\n",
    "            # --- 3. Store config for saving/loading ---\n",
    "            self.config_args = {\n",
    "                'num_node_features': num_node_features,\n",
    "                'num_edge_features': num_edge_features,\n",
    "                'num_global_features': num_global_features,\n",
    "                'hidden_channels_gnn': hidden_channels_gnn,\n",
    "                'mlp_neurons': mlp_neurons,\n",
    "                'mlp_dropouts': mlp_dropouts,\n",
    "                'heads': heads\n",
    "            }\n",
    "\n",
    "        def forward(self, data):\n",
    "            x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch\n",
    "\n",
    "            # GNN Layers with ReLU and Dropout\n",
    "            x = F.relu(self.convs[0](x, edge_index, edge_attr))\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "            x = F.relu(self.convs[1](x, edge_index, edge_attr))\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "            x = F.relu(self.convs[2](x, edge_index, edge_attr))\n",
    "\n",
    "            # Readout\n",
    "            graph_embedding = global_mean_pool(x, batch)\n",
    "            combined_features = torch.cat([graph_embedding, u], dim=1)\n",
    "            output = self.readout_mlp(combined_features)\n",
    "\n",
    "            return output\n",
    "            \n",
    "    # This is a new helper, just to make scaling code cleaner inside the loops\n",
    "    def scale_graph_features(data_list, u_scaler, x_scaler, atom_map_len):\n",
    "        \"\"\"Applies fitted scalers in-place to a list of Data objects.\"\"\"\n",
    "        try:\n",
    "            for data in data_list:\n",
    "                # 1. Scale global features (u)\n",
    "                data.u = torch.tensor(u_scaler.transform(data.u.numpy()), dtype=torch.float)\n",
    "                \n",
    "                # 2. Scale continuous part of node features (x)\n",
    "                x_one_hot = data.x[:, :atom_map_len]\n",
    "                x_continuous = data.x[:, atom_map_len:]\n",
    "                \n",
    "                x_continuous_scaled = x_scaler.transform(x_continuous.numpy())\n",
    "                x_continuous_scaled_tensor = torch.tensor(x_continuous_scaled, dtype=torch.float)\n",
    "                \n",
    "                # Recombine scaled features\n",
    "                data.x = torch.cat([x_one_hot, x_continuous_scaled_tensor], dim=1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR applying scalers: {e}. Check feature dimensions. AtomMapLen={atom_map_len}\")\n",
    "            raise e\n",
    "        return data_list\n",
    "\n",
    "\n",
    "    def train_gnn_model(label, train_data_list, val_data_list, mlp_neurons, mlp_dropouts, epochs=300): # Increased default epochs\n",
    "        \"\"\"\n",
    "        (REVISED)\n",
    "        - Accepts both train and val data lists.\n",
    "        - Implements ReduceLROnPlateau scheduler based on val_loss.\n",
    "        - Implements Early Stopping based on val_loss patience.\n",
    "        \"\"\"\n",
    "        print(f\"--- Training GNN for label: {label} ---\")\n",
    "        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        if not train_data_list:\n",
    "            print(f\"Warning: Empty train data list passed for {label}.\")\n",
    "            return None\n",
    "        if not val_data_list:\n",
    "            print(f\"Warning: Empty validation data list passed for {label}.\")\n",
    "            return None\n",
    "\n",
    "        # drop_last=True is important for training stability, prevents variance from tiny final batches.\n",
    "        train_loader = PyGDataLoader(train_data_list, batch_size=32, shuffle=True, drop_last=True) \n",
    "        val_loader = PyGDataLoader(val_data_list, batch_size=32, shuffle=False) # No shuffle/drop for val\n",
    "\n",
    "        # Get feature dimensions from the first data object\n",
    "        first_data = train_data_list[0]\n",
    "        num_node_features = first_data.x.shape[1]\n",
    "        num_global_features = first_data.u.shape[1]\n",
    "        num_edge_features = first_data.edge_attr.shape[1]\n",
    "        \n",
    "        print(f\"Model Features (Scaled): Nodes={num_node_features}, Edges={num_edge_features}, Global={num_global_features}\")\n",
    "\n",
    "        model = TaskSpecificGNN(  # This should be your (no-BN) model class\n",
    "            num_node_features=num_node_features,\n",
    "            num_edge_features=num_edge_features,\n",
    "            num_global_features=num_global_features,\n",
    "            hidden_channels_gnn=128, \n",
    "            mlp_neurons=mlp_neurons,\n",
    "            mlp_dropouts=mlp_dropouts\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        criterion = torch.nn.L1Loss() \n",
    "\n",
    "        # --- 1. ADD SCHEDULER ---\n",
    "        # This will cut the LR by half (factor=0.5) if val loss doesn't improve for 10 epochs (patience=10)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "        # --- 2. ADD EARLY STOPPING VARS ---\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        PATIENCE_EPOCHS = 30  # Stop training if val loss doesn't improve for 30 straight epochs\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            for data in train_loader:\n",
    "                if data.x.shape[0] <= 1: # Skip batches with one node (can happen)\n",
    "                    continue\n",
    "                data = data.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "                loss = criterion(out, data.y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                total_train_loss += loss.item() * data.num_graphs\n",
    "            \n",
    "            if len(train_loader.dataset) == 0:\n",
    "                avg_train_loss = 0\n",
    "            else:\n",
    "                avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "\n",
    "            # --- 3. ADD VALIDATION LOOP (INSIDE EPOCH LOOP) ---\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    data = data.to(DEVICE)\n",
    "                    out = model(data)\n",
    "                    loss = criterion(out, data.y)\n",
    "                    total_val_loss += loss.item() * data.num_graphs\n",
    "            \n",
    "            if len(val_loader.dataset) == 0:\n",
    "                avg_val_loss = 0\n",
    "            else:\n",
    "                avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                print(f\"Epoch: {epoch:03d}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "            # --- 4. SCHEDULER & EARLY STOPPING LOGIC ---\n",
    "            scheduler.step(avg_val_loss) # Feed validation loss to the scheduler\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= PATIENCE_EPOCHS and epoch > 50: # Give it at least 50 epochs to warm up\n",
    "                print(f\"--- Early stopping triggered at epoch {epoch} ---\")\n",
    "                break\n",
    "                \n",
    "        print(f\"--- GNN training for {label} complete. Best Val Loss: {best_val_loss:.6f} ---\")\n",
    "        return model\n",
    "\n",
    "    def predict_with_gnn(trained_model, test_smiles, label, u_scaler, x_scaler, atom_map_len):\n",
    "        \"\"\"\n",
    "        (MODIFIED for Full Scaling)\n",
    "        - Requires both u_scaler (global) and x_scaler (node) to transform features.\n",
    "        - Returns SCALED predictions.\n",
    "        \"\"\"\n",
    "        if trained_model is None or u_scaler is None or x_scaler is None:\n",
    "            print(f\"Prediction skipped for {label} due to missing model or scaler.\")\n",
    "            return np.full(len(test_smiles), np.nan)\n",
    "\n",
    "        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # 1. Featurize test data (features are NOT scaled yet)\n",
    "        test_data_list = [smiles_to_graph_label_specific(s, label, y_val=None) for s in test_smiles]\n",
    "        \n",
    "        valid_indices = [i for i, data in enumerate(test_data_list) if data is not None]\n",
    "        valid_test_data = [data for data in test_data_list if data is not None]\n",
    "\n",
    "        if not valid_test_data:\n",
    "            print(f\"Warning: No valid test molecules could be converted for {label}.\")\n",
    "            return np.full(len(test_smiles), np.nan)\n",
    "            \n",
    "        # 2. Apply fitted scalers to all valid test features\n",
    "        try:\n",
    "            valid_test_data = scale_graph_features(valid_test_data, u_scaler, x_scaler, atom_map_len)\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR applying scalers during prediction: {e}.\")\n",
    "            return np.full(len(test_smiles), np.nan)\n",
    "\n",
    "        test_loader = PyGDataLoader(valid_test_data, batch_size=32, shuffle=False) \n",
    "\n",
    "        trained_model.eval()\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                data = data.to(DEVICE)\n",
    "                out = trained_model(data)\n",
    "                all_preds.append(out.cpu())\n",
    "\n",
    "        test_preds_tensor = torch.cat(all_preds, dim=0).numpy().flatten()\n",
    "        \n",
    "        # Fill predictions array (these are SCALED predictions)\n",
    "        final_predictions = np.full(len(test_smiles), np.nan)\n",
    "        if len(test_preds_tensor) == len(valid_indices):\n",
    "            final_predictions[valid_indices] = test_preds_tensor\n",
    "        else:\n",
    "            print(f\"Warning: Mismatch in GNN prediction count for {label}.\")\n",
    "            fill_count = min(len(valid_indices), len(test_preds_tensor))\n",
    "            final_predictions[valid_indices[:fill_count]] = test_preds_tensor[:fill_count]\n",
    "\n",
    "        return final_predictions # These predictions are on the SCALED range\n",
    "\n",
    "\n",
    "    def train_or_predict_gnn(train_model=True, model_dir=\"models/gnn\", n_splits=10):\n",
    "        \"\"\"\n",
    "        (FINAL COMPLETE VERSION)\n",
    "        - All data hardening (coerce, filter) and RobustScaler logic is included.\n",
    "        - CV loop is modified to create a val_data_list.\n",
    "        - Calls the new, optimized train_gnn_model with scheduler/early stopping.\n",
    "        - Correctly passes all arguments (config['neurons'], config['dropouts']) to fix the TypeError.\n",
    "        \"\"\"\n",
    "        \n",
    "        ATOM_MAP_LEN = 20  # Make sure this matches your global ATOM_MAP\n",
    "        \n",
    "        # Plausible physical ranges to filter catastrophic outliers BEFORE scaling\n",
    "        VALID_RANGES = {\n",
    "            'Tg':      (-100, 500),  \n",
    "            'FFV':     (0.01, 0.99), \n",
    "            'Tc':      (0, 1000),    \n",
    "            'Density': (0.1, 3.0),   \n",
    "            'Rg':      (0.1, 200)    \n",
    "        }\n",
    "\n",
    "        # MLP configs for the GNN readout head\n",
    "        best_configs = {\n",
    "            # Classic funnel, slightly lower final dropout\n",
    "            \"Tg\":      {\"neurons\": [512, 256, 128], \"dropouts\": [0.5, 0.4, 0.2]},\n",
    "            # Original wide funnel for this complex feature\n",
    "            \"Density\": {\"neurons\": [1024, 256, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "            # Even wider and deeper, with strong regularization for presumed complexity\n",
    "            \"FFV\":     {\"neurons\": [1024, 512, 64], \"dropouts\": [0.6, 0.5, 0.4]},\n",
    "            # Slightly deeper than the simplest model to capture more features\n",
    "            \"Tc\":      {\"neurons\": [128, 64], \"dropouts\": [0.4, 0.3]},\n",
    "            # A gentle funnel instead of a pure block to encourage feature compression\n",
    "            \"Rg\":      {\"neurons\": [128, 64, 64], \"dropouts\": [0.4, 0.3, 0.3]},\n",
    "        }\n",
    "        default_config = {\"neurons\": [128, 64], \"dropouts\": [0.3, 0.3]}\n",
    "\n",
    "        output_df = pd.DataFrame({'id': test_df['id']})\n",
    "        cv_mae_results = []\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        warnings.filterwarnings(\"ignore\", \"Mean of empty slice\", RuntimeWarning)\n",
    "\n",
    "        for label in labels: \n",
    "            print(f\"\\n{'='*20} Processing GNN for label: {label} {'='*20}\")\n",
    "            \n",
    "            config = best_configs.get(label, default_config)\n",
    "            print(f\"Using MLP Config: Neurons={config['neurons']}, Dropouts={config['dropouts']}\")\n",
    "            \n",
    "            ensemble_models = []\n",
    "            y_scaler_path = os.path.join(model_dir, f\"gnn_yscaler_{label}.joblib\")\n",
    "            u_scaler_path = os.path.join(model_dir, f\"gnn_uscaler_{label}.joblib\")\n",
    "            x_scaler_path = os.path.join(model_dir, f\"gnn_xscaler_{label}.joblib\")\n",
    "            \n",
    "            if train_model:\n",
    "                # --- START DATA HARDENING ---\n",
    "                all_smiles_raw = subtables[label]['SMILES']\n",
    "                all_y_raw = subtables[label][label] \n",
    "                \n",
    "                all_y_numeric = pd.to_numeric(all_y_raw, errors='coerce')\n",
    "                original_count = len(all_y_numeric)\n",
    "\n",
    "                valid_min, valid_max = VALID_RANGES.get(label, (-np.inf, np.inf))\n",
    "                valid_mask = (all_y_numeric >= valid_min) & (all_y_numeric <= valid_max) & (all_y_numeric.notna())\n",
    "                \n",
    "                all_y = all_y_numeric[valid_mask].reset_index(drop=True)\n",
    "                all_smiles = all_smiles_raw[valid_mask].reset_index(drop=True)\n",
    "                \n",
    "                print(f\"FILTERING: Coerced {original_count} rows. Kept {len(all_y)} valid rows within range ({valid_min}, {valid_max}).\")\n",
    "                \n",
    "                if len(all_y) < (2 * n_splits): \n",
    "                    print(f\"CRITICAL: Not enough valid data ({len(all_y)}) to train for {label} with {n_splits} splits. Skipping.\")\n",
    "                    continue\n",
    "                # --- END DATA HARDENING ---\n",
    "\n",
    "                # --- 1. FIT Y-SCALER (ROBUST) ---\n",
    "                print(\"Using RobustScaler for Y-Scaler.\")\n",
    "                y_scaler = RobustScaler()  \n",
    "                all_y_scaled = y_scaler.fit_transform(all_y.values.reshape(-1, 1)).flatten()\n",
    "                joblib.dump(y_scaler, y_scaler_path)\n",
    "                print(f\"Saved Y-Scaler for {label}\")\n",
    "\n",
    "                # --- 2. FIT INPUT SCALERS (ROBUST) ---\n",
    "                print(\"Pre-computing all graph features to fit input scalers...\")\n",
    "                all_train_graphs_raw = [smiles_to_graph_label_specific(s, label, None) for s in all_smiles]\n",
    "                \n",
    "                # Sync graph list with all data (skipping any SMILES that fail featurization)\n",
    "                all_train_graphs_synced = []\n",
    "                all_y_scaled_synced = [] \n",
    "                all_y_original_synced = [] # Also sync original Y for the CV split\n",
    "                all_smiles_synced = []     # Also sync SMILES for the CV split\n",
    "                \n",
    "                for i, graph in enumerate(all_train_graphs_raw):\n",
    "                    if graph is not None:\n",
    "                        all_train_graphs_synced.append(graph)\n",
    "                        all_y_scaled_synced.append(all_y_scaled[i]) \n",
    "                        all_y_original_synced.append(all_y[i]) # Keep the original, unscaled, clean Y\n",
    "                        all_smiles_synced.append(all_smiles[i]) # Keep the matching SMILES\n",
    "                \n",
    "                all_train_graphs = all_train_graphs_synced \n",
    "                all_y_scaled = np.array(all_y_scaled_synced)\n",
    "                all_y_original_df = pd.Series(all_y_original_synced) # Store as Series for .iloc\n",
    "                all_smiles_df = pd.Series(all_smiles_synced)         # Store as Series for .iloc\n",
    "\n",
    "                if not all_train_graphs:\n",
    "                    print(f\"CRITICAL: No valid training graphs could be featurized for {label}. Skipping.\")\n",
    "                    continue\n",
    "                    \n",
    "                all_u_data = np.concatenate([d.u.numpy() for d in all_train_graphs], axis=0)\n",
    "                print(\"Using RobustScaler for U-Scaler.\")\n",
    "                u_scaler = RobustScaler().fit(all_u_data)  # Use RobustScaler\n",
    "                joblib.dump(u_scaler, u_scaler_path)\n",
    "                print(f\"Saved U-Scaler for {label}\")\n",
    "\n",
    "                all_x_data = torch.cat([d.x for d in all_train_graphs], dim=0)\n",
    "                all_x_continuous = all_x_data[:, ATOM_MAP_LEN:].numpy()\n",
    "                print(\"Using RobustScaler for X-Scaler.\")\n",
    "                x_scaler = RobustScaler().fit(all_x_continuous)  # Use RobustScaler\n",
    "                joblib.dump(x_scaler, x_scaler_path)\n",
    "                print(f\"Saved X-Scaler for {label}\")\n",
    "\n",
    "                # --- 3. APPLY SCALERS ---\n",
    "                all_data_objects_scaled = scale_graph_features(all_train_graphs, u_scaler, x_scaler, ATOM_MAP_LEN)\n",
    "                for i, data_obj in enumerate(all_data_objects_scaled):\n",
    "                    data_obj.y = torch.tensor([[all_y_scaled[i]]], dtype=torch.float)\n",
    "                \n",
    "                # --- 4. K-FOLD CV LOOP (MODIFIED) ---\n",
    "                kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "                fold_val_scores = []\n",
    "                fold_indices_gen = kf.split(all_data_objects_scaled) # Split the synced, valid, scaled data\n",
    "\n",
    "                for fold, (train_idx, val_idx) in enumerate(fold_indices_gen):\n",
    "                    print(f\"\\n--- Fold {fold+1}/{n_splits} for {label} ---\")\n",
    "                    \n",
    "                    train_data_list = [all_data_objects_scaled[i] for i in train_idx]\n",
    "                    val_data_list = [all_data_objects_scaled[i] for i in val_idx] # <-- CREATE VAL LIST\n",
    "                    \n",
    "                    val_smiles_list = all_smiles_df.iloc[val_idx].tolist()\n",
    "                    y_val_original = all_y_original_df.iloc[val_idx].values \n",
    "\n",
    "                    fold_model = train_gnn_model(\n",
    "                        label,\n",
    "                        train_data_list, # Pass train data\n",
    "                        val_data_list,   # <-- Pass val data\n",
    "                        config['neurons'],    # <-- PASSES mlp_neurons\n",
    "                        config['dropouts'],   # <-- PASSES mlp_dropouts (FIXES ERROR)\n",
    "                        epochs=300       # <-- Train longer (will stop early)\n",
    "                    )\n",
    "                    \n",
    "                    if fold_model:\n",
    "                        print(\"Running final validation prediction on the best model...\")\n",
    "                        val_preds_scaled = predict_with_gnn(fold_model, val_smiles_list, label, u_scaler, x_scaler, ATOM_MAP_LEN)\n",
    "                        \n",
    "                        train_y_scaled_median = 0.0 # RobustScaler median is 0\n",
    "                        val_preds_scaled_filled = pd.Series(val_preds_scaled).fillna(train_y_scaled_median)\n",
    "                        \n",
    "                        val_preds_original = y_scaler.inverse_transform(\n",
    "                            val_preds_scaled_filled.values.reshape(-1, 1)\n",
    "                        ).flatten()\n",
    "\n",
    "                        mae = mean_absolute_error(y_val_original, val_preds_original)\n",
    "                        print(f\"✅ Fold {fold+1} Validation MAE (Original Scale): {mae:.4f}\")\n",
    "                        fold_val_scores.append(mae)\n",
    "                        \n",
    "                        model_save_name = f\"{label}_fold{fold}\"\n",
    "                        save_gnn_model(fold_model, model_save_name, model_dir)\n",
    "                        ensemble_models.append(fold_model)\n",
    "                    else:\n",
    "                        print(f\"Warning: Training failed for Fold {fold+1}. Model will be skipped.\")\n",
    "                \n",
    "                if fold_val_scores:\n",
    "                    avg_cv_mae = np.mean(fold_val_scores)\n",
    "                    print(f\"\\n{'*'*10} Average CV MAE for {label} (Original Scale): {avg_cv_mae:.4f} {'*'*10}\")\n",
    "                    cv_mae_results.append({'label': label, 'avg_cv_mae': avg_cv_mae})\n",
    "\n",
    "            else:\n",
    "                # --- PREDICTION-ONLY MODE ---\n",
    "                print(f\"Loading {n_splits} models and ALL 3 RobustScalers for {label} ensemble...\")\n",
    "                model_path = '/kaggle/input/neurips-2025/GATConv_v29/models/gnn/'\n",
    "                try:\n",
    "                    y_scaler = joblib.load(f'{model_path}gnn_yscaler_{label}.joblib')\n",
    "                    u_scaler = joblib.load(f'{model_path}gnn_uscaler_{label}.joblib')\n",
    "                    x_scaler = joblib.load(f'{model_path}gnn_xscaler_{label}.joblib')\n",
    "                    print(\"Loaded Y, U, and X RobustScalers.\")\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"CRITICAL: Scaler files not found for {label}. Cannot make predictions.\")\n",
    "                    continue\n",
    "\n",
    "                for fold in range(n_splits):\n",
    "                    loaded_model = load_gnn_model(f\"{label}_fold{fold}\", model_path.rstrip('/'))\n",
    "                    if loaded_model:\n",
    "                        ensemble_models.append(loaded_model)\n",
    "                \n",
    "                if not ensemble_models: print(f\"Warning: No models found for label {label}.\")\n",
    "                else: print(f\"Successfully loaded {len(ensemble_models)} models for ensemble.\")\n",
    "\n",
    "\n",
    "            # --- ENSEMBLE PREDICTION STEP (Test Set) ---\n",
    "            test_smiles = test_df['SMILES'].tolist()\n",
    "            \n",
    "            if ensemble_models and y_scaler and u_scaler and x_scaler:\n",
    "                print(f\"Making ensemble (scaled) predictions for {label} using {len(ensemble_models)} models...\")\n",
    "                all_fold_preds_scaled = []\n",
    "                for model in ensemble_models:\n",
    "                    fold_test_preds_scaled = predict_with_gnn(model, test_smiles, label, u_scaler, x_scaler, ATOM_MAP_LEN)\n",
    "                    all_fold_preds_scaled.append(fold_test_preds_scaled)\n",
    "                \n",
    "                preds_stack_scaled = np.stack(all_fold_preds_scaled)\n",
    "                final_ensemble_preds_scaled = np.nanmean(preds_stack_scaled, axis=0) \n",
    "                pred_series_scaled = pd.Series(final_ensemble_preds_scaled)\n",
    "                \n",
    "                pred_series_scaled_filled = pred_series_scaled.fillna(0.0) # Impute with scaled median (0.0)\n",
    "\n",
    "                final_preds_original = y_scaler.inverse_transform(\n",
    "                    pred_series_scaled_filled.values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "                \n",
    "                output_df[label] = final_preds_original\n",
    "                \n",
    "            else:\n",
    "                print(f\"No models or scalers available for {label}. Filling with (filtered) training median.\")\n",
    "                # Robust median fallback logic\n",
    "                fallback_median = 0.0\n",
    "                try:\n",
    "                    if 'all_y' in locals() and not all_y.empty:\n",
    "                        fallback_median = all_y.median()\n",
    "                    else: \n",
    "                        print(\"Loading data to calculate fallback median...\")\n",
    "                        fb_y_raw = subtables[label][label]\n",
    "                        fb_y_num = pd.to_numeric(fb_y_raw, errors='coerce')\n",
    "                        valid_min, valid_max = VALID_RANGES.get(label, (-np.inf, np.inf))\n",
    "                        fb_mask = (fb_y_num >= valid_min) & (fb_y_num <= valid_max) & (fb_y_num.notna())\n",
    "                        fallback_median = fb_y_num[fb_mask].median()\n",
    "                    print(f\"Using filtered median fallback: {fallback_median}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting median, falling back to 0: {e}\")\n",
    "                    fallback_median = 0.0 \n",
    "                    \n",
    "                output_df[label] = fallback_median\n",
    "\n",
    "        # --- Display final CV MAE summary ---\n",
    "        if train_model and cv_mae_results:\n",
    "            print(\"\\n\" + \"=\"*40)\n",
    "            print(\"📊 HYBRID GNN 5-Fold CV MAE Summary (Original Scale):\")\n",
    "            print(\"=\"*40)\n",
    "            mae_df = pd.DataFrame(cv_mae_results)\n",
    "            print(mae_df.to_string(index=False))\n",
    "            mae_df.to_csv(\"gnn_hybrid_cv_mae_results.csv\", index=False)\n",
    "            print(\"\\nCV results saved to gnn_hybrid_cv_mae_results.csv\")\n",
    "\n",
    "        submission_path = 'submission_hybrid_gnn_final.csv'\n",
    "        output_df.to_csv(submission_path, index=False)\n",
    "        print(f\"\\n✅ GNN Ensemble predictions (Original Scale) saved to {submission_path}\")\n",
    "        \n",
    "        warnings.filterwarnings(\"default\", \"Mean of empty slice\", RuntimeWarning)\n",
    "        \n",
    "        return output_df\n",
    "\n",
    "    # To train the models and then predict:\n",
    "    gnn_submission_df = train_or_predict_gnn(train_model=False)\n",
    "\n",
    "    output_dfs.append(gnn_submission_df)\n",
    "\n",
    "    print(\"\\nGNN Submission Preview:\")\n",
    "    print(gnn_submission_df.head())    \n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"[Code B] Finished.\")\n",
    "\n",
    "\n",
    "# Code C (FAST: SMILES TF-IDF + RidgeCV)\n",
    "# --------------------------------------\n",
    "def run_code_c():\n",
    "    \"\"\"Fast Code C: SMILES char n-grams + RidgeCV per target.\"\"\"\n",
    "    print(\"\\n[Code C FAST] Starting (TF-IDF + RidgeCV)...\")\n",
    "\n",
    "    import os, warnings, numpy as np, pandas as pd\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import joblib\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # --- paths / constants ---\n",
    "    try:\n",
    "        BASE = BASE_PATH\n",
    "    except NameError:\n",
    "        BASE = \"/kaggle/input/neurips-open-polymer-prediction-2025/\"\n",
    "    MODEL_DIR = \"models_c_fast\"\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        TARGETS_ = TARGETS\n",
    "    except NameError:\n",
    "        TARGETS_ = ['Tg','FFV','Tc','Density','Rg']\n",
    "\n",
    "    VALID_RANGES = {'Tg':(-100,500),'FFV':(0.01,0.99),'Tc':(0,1000),'Density':(0.1,3.0),'Rg':(0.1,200)}\n",
    "    DEFAULTS = {\"Tg\":150.0, \"FFV\":0.35, \"Tc\":200.0, \"Density\":1.10, \"Rg\":20.0}\n",
    "\n",
    "    # --- load data (preserve all test ids) ---\n",
    "    train = pd.read_csv(os.path.join(BASE, \"train.csv\"))\n",
    "    test  = pd.read_csv(os.path.join(BASE, \"test.csv\"))\n",
    "    test_ids_all = test[\"id\"].values\n",
    "\n",
    "    def clean_smiles_lenient(s):\n",
    "        if not isinstance(s, str): return \"\"\n",
    "        s = s.strip()\n",
    "        return s if any(ch.isalpha() for ch in s) else \"\"\n",
    "\n",
    "    train[\"SMILES\"] = train[\"SMILES\"].apply(clean_smiles_lenient)\n",
    "    test[\"SMILES\"]  = test[\"SMILES\"].apply(clean_smiles_lenient)\n",
    "    train = train[train[\"SMILES\"]!=\"\"].reset_index(drop=True)\n",
    "    test[\"SMILES\"]  = test[\"SMILES\"].fillna(\"\")\n",
    "\n",
    "    # --- vectorizer: reuse if exists, else fit on train+test SMILES union ---\n",
    "    vec_path = os.path.join(MODEL_DIR, \"tfidf_char.pkl\")\n",
    "    if os.path.exists(vec_path):\n",
    "        vectorizer = joblib.load(vec_path)\n",
    "        print(\"[Code C FAST] Loaded TF-IDF vectorizer.\")\n",
    "    else:\n",
    "        print(\"[Code C FAST] Fitting TF-IDF vectorizer...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            analyzer=\"char\",\n",
    "            ngram_range=(2,5),\n",
    "            min_df=3,\n",
    "            max_features=150_000,\n",
    "            sublinear_tf=True,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        vectorizer.fit(pd.concat([train[\"SMILES\"], test[\"SMILES\"]], axis=0).values)\n",
    "        joblib.dump(vectorizer, vec_path)\n",
    "\n",
    "    X_test = vectorizer.transform(test[\"SMILES\"].values)\n",
    "\n",
    "    output_df = pd.DataFrame({\"id\": test_ids_all})\n",
    "\n",
    "    # small CV for alpha picking; runs very fast\n",
    "    alphas = np.array([0.1, 0.3, 1.0, 3.0, 10.0], dtype=float)\n",
    "\n",
    "    for label in TARGETS_:\n",
    "        print(f\"\\n[Code C FAST] === {label} ===\")\n",
    "        y_raw = pd.to_numeric(train.get(label), errors=\"coerce\")\n",
    "        vmin, vmax = VALID_RANGES.get(label, (-np.inf, np.inf))\n",
    "        m = (y_raw.notna()) & (y_raw >= vmin) & (y_raw <= vmax)\n",
    "\n",
    "        if not m.any():\n",
    "            med = DEFAULTS.get(label, 0.0)\n",
    "            print(f\"   No valid training rows → fallback {med}\")\n",
    "            output_df[label] = med\n",
    "            continue\n",
    "\n",
    "        X_train = vectorizer.transform(train.loc[m, \"SMILES\"].values)\n",
    "        y_train = y_raw.loc[m].astype(float).values\n",
    "\n",
    "        # Scale y for stability, inverse later\n",
    "        y_scaler = RobustScaler()\n",
    "        y_scaled = y_scaler.fit_transform(y_train.reshape(-1,1)).ravel()\n",
    "\n",
    "        model_path = os.path.join(MODEL_DIR, f\"ridge_{label}.pkl\")\n",
    "        ysc_path   = os.path.join(MODEL_DIR, f\"yscaler_{label}.pkl\")\n",
    "\n",
    "        if os.path.exists(model_path) and os.path.exists(ysc_path):\n",
    "            model = joblib.load(model_path)\n",
    "            y_scaler = joblib.load(ysc_path)\n",
    "            print(\"   Loaded model + scaler.\")\n",
    "        else:\n",
    "            print(\"   Training RidgeCV...\")\n",
    "            cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            model = RidgeCV(alphas=alphas, cv=cv, scoring=\"neg_mean_absolute_error\")\n",
    "            model.fit(X_train, y_scaled)\n",
    "            joblib.dump(model, model_path)\n",
    "            joblib.dump(y_scaler, ysc_path)\n",
    "            # quick CV MAE estimate (on training folds scores)\n",
    "            if hasattr(model, 'cv_values_'):\n",
    "                pass\n",
    "            else:\n",
    "                # quick holdout via CV.split one pass (optional)\n",
    "                pass\n",
    "            print(f\"   Chosen alpha: {getattr(model, 'alpha_', 'n/a')}\")\n",
    "\n",
    "        preds_scaled = model.predict(X_test)\n",
    "        preds = y_scaler.inverse_transform(preds_scaled.reshape(-1,1)).ravel()\n",
    "\n",
    "        # sanity: if degenerate, fall back to median\n",
    "        if len(preds) != len(test_ids_all) or np.allclose(preds, preds[0]):\n",
    "            med = float(np.median(y_train)) if len(y_train) else DEFAULTS.get(label, 0.0)\n",
    "            print(f\"   Degenerate predictions → fallback {med}\")\n",
    "            output_df[label] = med\n",
    "        else:\n",
    "            output_df[label] = preds\n",
    "\n",
    "    sub_path = \"submission_code_c_fast.csv\"\n",
    "    output_df.to_csv(sub_path, index=False)\n",
    "    print(f\"\\n[Code C FAST] ✅ predictions saved to {sub_path}\")\n",
    "\n",
    "    # return to global merger\n",
    "    try:\n",
    "        output_dfs.append(output_df)\n",
    "    except NameError:\n",
    "        globals()[\"output_dfs\"] = [output_df]\n",
    "\n",
    "    print(\"\\n[Code C FAST] Submission preview:\")\n",
    "    print(output_df.head())\n",
    "    print(\"[Code C FAST] Finished.\\n\")\n",
    "\n",
    "def run_code_d():\n",
    "    \n",
    "    \n",
    "    print(\"[Code D FAST] Started.\\n\")\n",
    "    # --- local wheels as in your baseline1 ---\n",
    "    !pip install /kaggle/input/betterback/rdkit-2025-3-3-cp311/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl -q\n",
    "    !pip install mordred --no-index --find-links=file:///kaggle/input/betterback/mordred-1-2-0-py3-none-any/mordred-1-2-0-py3-none-any -q\n",
    "    \n",
    "    import os, gc, warnings, math, random, time\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    from typing import Dict, Tuple, Optional, List\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    \n",
    "    # oneDAL speed patch for any sklearn bits we use\n",
    "    try:\n",
    "        from sklearnex import patch_sklearn\n",
    "        patch_sklearn()\n",
    "        print(\"[speed] sklearnex patch active\")\n",
    "    except Exception as e:\n",
    "        print(\"[speed] sklearnex skipped:\", e)\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    from rdkit import Chem\n",
    "    from mordred import Calculator, descriptors\n",
    "    \n",
    "    SEED = 42\n",
    "    random.seed(SEED); np.random.seed(SEED)\n",
    "    \n",
    "    # ---------- IO ----------\n",
    "    def _read_csv_fast(path: str):\n",
    "        try:\n",
    "            return pd.read_csv(path, low_memory=False, engine=\"pyarrow\")\n",
    "        except Exception:\n",
    "            return pd.read_csv(path, low_memory=False)\n",
    "    \n",
    "    tg      = _read_csv_fast('/kaggle/input/betterback/modred-dataset/modred-dataset/desc_tg.csv')\n",
    "    tc      = _read_csv_fast('/kaggle/input/betterback/modred-dataset/modred-dataset/desc_tc.csv')\n",
    "    rg      = _read_csv_fast('/kaggle/input/betterback/modred-dataset/modred-dataset/desc_rg.csv')\n",
    "    ffv     = _read_csv_fast('/kaggle/input/betterback/modred-dataset/modred-dataset/desc_ffv.csv')\n",
    "    density = _read_csv_fast('/kaggle/input/betterback/modred-dataset/modred-dataset/desc_de.csv')\n",
    "    test    = _read_csv_fast('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "    ID = test['id'].values\n",
    "    \n",
    "    def _clean_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        keep = ['id'] if 'id' in df.columns else []\n",
    "        num  = df.select_dtypes(include=[np.number])\n",
    "        out  = pd.concat([df[keep], num], axis=1) if keep else num.copy()\n",
    "        const_cols = [c for c in out.columns if c != 'id' and out[c].nunique() <= 1]\n",
    "        if const_cols: out.drop(columns=const_cols, inplace=True)\n",
    "        return out\n",
    "    \n",
    "    tg      = _clean_numeric(tg)\n",
    "    tc      = _clean_numeric(tc)\n",
    "    rg      = _clean_numeric(rg)\n",
    "    ffv     = _clean_numeric(ffv)\n",
    "    density = _clean_numeric(density)\n",
    "    \n",
    "    # ---------- Mordred descriptors for TEST (cached) ----------\n",
    "    # ---------- Mordred descriptors for TEST (cached, SAFE single-thread) ----------\n",
    "    import os\n",
    "    os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "    \n",
    "    mols_test = [Chem.MolFromSmiles(s) for s in test.SMILES]\n",
    "    desc_cache_path = '/kaggle/working/desc_test.parquet'\n",
    "    \n",
    "    def _compute_mordred_safe(mols, batch=256):\n",
    "        calc = Calculator(descriptors, ignore_3D=True)\n",
    "        pieces = []\n",
    "        n = len(mols)\n",
    "        for i in range(0, n, batch):\n",
    "            chunk = mols[i:i+batch]\n",
    "            df_chunk = calc.pandas(chunk, nproc=1)  # single-thread\n",
    "            df_chunk.columns = df_chunk.columns.map(str)\n",
    "            pieces.append(df_chunk)\n",
    "            print(f\"[mordred] computed {min(i+batch, n)}/{n}\", flush=True)\n",
    "        df = pd.concat(pieces, axis=0, ignore_index=True)\n",
    "        df = df.select_dtypes(include=[np.number]).copy()\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        return df\n",
    "    \n",
    "    if os.path.exists(desc_cache_path):\n",
    "        print(\"[mordred] using cached parquet\")\n",
    "        desc_test = pd.read_parquet(desc_cache_path)\n",
    "    else:\n",
    "        print(\"[mordred] computing descriptors (single-threaded batches)...\")\n",
    "        desc_test = _compute_mordred_safe(mols_test, batch=256)\n",
    "        desc_test.to_parquet(desc_cache_path)\n",
    "        print(\"[mordred] cached to\", desc_cache_path)\n",
    "    \n",
    "    desc_test = desc_test.astype(np.float32)\n",
    "    \n",
    "    TRAIN = {'Tg': tg, 'FFV': ffv, 'Tc': tc, 'Density': density, 'Rg': rg}\n",
    "    TARGETS = ['Tg','FFV','Tc','Density','Rg']\n",
    "    \n",
    "    # ---------- Utils ----------\n",
    "    def prepare_Xy(train_df: pd.DataFrame, test_df: pd.DataFrame, target: str, skew_gate=True):\n",
    "        assert target in train_df.columns, f\"{target} missing\"\n",
    "        cols = [c for c in train_df.columns if c not in ('id', target) and c in test_df.columns]\n",
    "        if not cols: raise RuntimeError(f\"No common numeric features for {target}.\")\n",
    "        y_all = train_df[target].astype(float)\n",
    "        mask  = y_all.notna()\n",
    "        y = y_all.loc[mask].values.astype(np.float32)\n",
    "        X = train_df.loc[mask, cols].copy()\n",
    "        T = test_df[cols].copy()\n",
    "    \n",
    "        X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        T.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        med = X.median(); mean = X.mean(); skew = X.skew().fillna(0)\n",
    "        for c in cols:\n",
    "            fillv = med[c] if (skew_gate and abs(float(skew[c])) > 1.0) else mean[c]\n",
    "            if X[c].isna().any(): X[c] = X[c].fillna(fillv)\n",
    "            if T[c].isna().any(): T[c] = T[c].fillna(fillv)\n",
    "        return X.astype(np.float32), y.astype(np.float32), T.astype(np.float32)\n",
    "    \n",
    "    def percentile_clip(y: np.ndarray, qlo=0.5, qhi=99.5):\n",
    "        lo, hi = np.percentile(y, [qlo, qhi])\n",
    "        return float(lo), float(hi)\n",
    "    \n",
    "    def stratified_holdout_indices(y: np.ndarray, test_size=0.2, bins=12, seed=SEED):\n",
    "        # Create label bins for stratification (heatmap code's \"no leakage\" concept)\n",
    "        q = np.quantile(y, np.linspace(0, 1, bins+1))\n",
    "        q[0], q[-1] = -1e12, 1e12\n",
    "        y_bins = np.digitize(y, q[1:-1])\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "        tr_idx, va_idx = next(sss.split(np.zeros_like(y_bins), y_bins))\n",
    "        return tr_idx, va_idx\n",
    "    \n",
    "    # ---------- Optional tabular MixUp-lite (OFF by default) ----------\n",
    "    def mixup_tabular(X: np.ndarray, y: np.ndarray, alpha=0.15, frac=0.15, seed=SEED):\n",
    "        if frac <= 0: return X, y\n",
    "        rng = np.random.default_rng(seed)\n",
    "        n = X.shape[0]; m = int(n * frac)\n",
    "        if m <= 0: return X, y\n",
    "        i = rng.choice(n, size=m, replace=False)\n",
    "        j = rng.choice(n, size=m, replace=False)\n",
    "        lam = rng.beta(alpha, alpha, size=m).astype(np.float32)\n",
    "        X_new = lam[:, None]*X[i] + (1-lam)[:, None]*X[j]\n",
    "        y_new = lam*y[i] + (1-lam)*y[j]\n",
    "        X_out = np.vstack([X, X_new.astype(np.float32)])\n",
    "        y_out = np.concatenate([y, y_new.astype(np.float32)])\n",
    "        return X_out, y_out\n",
    "    \n",
    "    # ---------- Models ----------\n",
    "    def make_cat(seed=SEED):\n",
    "        return CatBoostRegressor(\n",
    "            task_type='GPU', devices='0',\n",
    "            loss_function='RMSE', eval_metric='MAE',\n",
    "            iterations=1600, learning_rate=0.035, depth=7, l2_leaf_reg=5.0,\n",
    "            bootstrap_type='Bernoulli', subsample=0.85,\n",
    "            od_type='Iter', od_wait=80, border_count=64,\n",
    "            random_seed=seed, verbose=False, allow_writing_files=False,\n",
    "            nan_mode='Min', grow_policy='SymmetricTree',\n",
    "            gpu_ram_part=0.85, metric_period=100, thread_count=-1\n",
    "        )\n",
    "    \n",
    "    def xgb_fit_predict(X_tr, y_tr, X_va, y_va, X_te, seed=SEED,\n",
    "                        nrounds=3200, esr=120):\n",
    "        dtr = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dva = xgb.DMatrix(X_va, label=y_va)\n",
    "        dte = xgb.DMatrix(X_te)\n",
    "        params = {\n",
    "            \"max_depth\": 8, \"eta\": 0.03, \"subsample\": 0.85, \"colsample_bytree\": 0.8,\n",
    "            \"lambda\": 2.0, \"tree_method\":\"gpu_hist\", \"predictor\":\"gpu_predictor\",\n",
    "            \"objective\":\"reg:squarederror\", \"eval_metric\":\"mae\", \"seed\": seed\n",
    "        }\n",
    "        bst = xgb.train(params, dtr, num_boost_round=nrounds,\n",
    "                        evals=[(dtr,\"train\"),(dva,\"valid\")],\n",
    "                        early_stopping_rounds=esr, verbose_eval=False)\n",
    "        p_va = bst.predict(dva, iteration_range=(0, bst.best_iteration+1)).astype(np.float32)\n",
    "        p_te = bst.predict(dte, iteration_range=(0, bst.best_iteration+1)).astype(np.float32)\n",
    "        return p_va, p_te\n",
    "    \n",
    "    # ---------- Per-target training + blend ----------\n",
    "    def train_blend_target(train_df: pd.DataFrame, target: str, test_df: pd.DataFrame,\n",
    "                           use_mixup=False) -> Tuple[np.ndarray, Dict[str, float]]:\n",
    "        X, y, T = prepare_Xy(train_df, test_df, target, skew_gate=True)\n",
    "        lo, hi = percentile_clip(y, 0.5, 99.5)\n",
    "    \n",
    "        # stratified holdout (heatmap \"no pre-shuffle\" idea)\n",
    "        tr_idx, va_idx = stratified_holdout_indices(y, test_size=0.2, bins=12, seed=SEED)\n",
    "        X_tr, y_tr = X.iloc[tr_idx].values, y[tr_idx]\n",
    "        X_va, y_va = X.iloc[va_idx].values, y[va_idx]\n",
    "    \n",
    "        # Optional MixUp-lite on tabular (default OFF for speed)\n",
    "        if use_mixup:\n",
    "            X_tr, y_tr = mixup_tabular(X_tr, y_tr, alpha=0.2, frac=0.12, seed=SEED)\n",
    "    \n",
    "        # --- CatBoost (2 seeds) ---\n",
    "        cat_maes = []; cat_te_list = []\n",
    "        for sd in (SEED, SEED+1):\n",
    "            cat = make_cat(sd)\n",
    "            cat.fit(Pool(X_tr, y_tr), eval_set=Pool(X_va, y_va), use_best_model=True, verbose=False)\n",
    "            p_va = np.clip(cat.predict(X_va).astype(np.float32), lo, hi)\n",
    "            cat_maes.append(float(np.mean(np.abs(y_va - p_va))))\n",
    "            cat_te_list.append(np.clip(cat.predict(T).astype(np.float32), lo, hi))\n",
    "        p_cat_va_mean = np.mean(cat_maes)\n",
    "        p_cat_te_mean = np.mean(np.stack(cat_te_list, axis=1), axis=1)\n",
    "    \n",
    "        # --- XGBoost (2 seeds) ---\n",
    "        xgb_maes = []; xgb_te_list = []\n",
    "        for sd in (SEED, SEED+1):\n",
    "            p_va, p_te = xgb_fit_predict(X_tr, y_tr, X_va, y_va, T, seed=sd, nrounds=3200, esr=120)\n",
    "            p_va = np.clip(p_va, lo, hi)\n",
    "            xgb_maes.append(float(np.mean(np.abs(y_va - p_va))))\n",
    "            xgb_te_list.append(np.clip(p_te, lo, hi))\n",
    "        p_xgb_va_mean = np.mean(xgb_maes)\n",
    "        p_xgb_te_mean = np.mean(np.stack(xgb_te_list, axis=1), axis=1)\n",
    "    \n",
    "        # --- Blend weight search on the SAME holdout (0..1 step 0.05) ---\n",
    "        best_w, best_mae = 0.5, 1e9\n",
    "        # To evaluate blend MAE on holdout, recompute holdout preds from best Cat/XGB (we already have means)\n",
    "        # Use single-seed holdout preds for fair weighting (seed-avg would be close; this keeps it fast):\n",
    "        # quickly recompute one set:\n",
    "        cat = make_cat(SEED); cat.fit(Pool(X_tr, y_tr), eval_set=Pool(X_va, y_va), use_best_model=True, verbose=False)\n",
    "        p_cat_va = np.clip(cat.predict(X_va).astype(np.float32), lo, hi)\n",
    "        p_cat_va = p_cat_va.astype(np.float32)\n",
    "    \n",
    "        p_xgb_va, _ = xgb_fit_predict(X_tr, y_tr, X_va, y_va, X_va, seed=SEED, nrounds=3200, esr=120)\n",
    "        p_xgb_va = np.clip(p_xgb_va.astype(np.float32), lo, hi)\n",
    "    \n",
    "        for w in np.linspace(0.0, 1.0, 21):\n",
    "            pv = w*p_cat_va + (1.0-w)*p_xgb_va\n",
    "            mae = float(np.mean(np.abs(y_va - pv)))\n",
    "            if mae < best_mae:\n",
    "                best_mae, best_w = mae, float(w)\n",
    "    \n",
    "        # final blended TEST preds using seed-averaged test preds for each model\n",
    "        p_te = best_w*p_cat_te_mean + (1.0-best_w)*p_xgb_te_mean\n",
    "    \n",
    "        report = {\n",
    "            'cat_mae': p_cat_va_mean,\n",
    "            'xgb_mae': p_xgb_va_mean,\n",
    "            'blend_mae': best_mae,\n",
    "            'w_cat': best_w\n",
    "        }\n",
    "        print(f\"[{target}] MAE — Cat(mean): {p_cat_va_mean:.6f} | XGB(mean): {p_xgb_va_mean:.6f} | \"\n",
    "              f\"Blend(w={best_w:.2f}): {best_mae:.6f}\")\n",
    "        return p_te.astype(np.float32), report\n",
    "    \n",
    "    # ---------- Driver ----------\n",
    "    # ---------- Driver ----------\n",
    "    def main(use_mixup=False):\n",
    "        metrics = []\n",
    "        preds = {}\n",
    "        for t in TARGETS:\n",
    "            p, m = train_blend_target(TRAIN[t], t, desc_test, use_mixup=use_mixup)\n",
    "            preds[t] = p; metrics.append((t, m['blend_mae']))\n",
    "    \n",
    "        print(\"Local holdout MAE:\", \" \".join([f\"{t}: {mae:.6f}\" for t, mae in metrics]), flush=True)\n",
    "    \n",
    "        # build submission as output_df instead of sub\n",
    "        output_df = pd.DataFrame({\n",
    "            'id': ID,\n",
    "            'Tg':      preds['Tg'],\n",
    "            'FFV':     preds['FFV'],\n",
    "            'Tc':      preds['Tc'],\n",
    "            'Density': preds['Density'],\n",
    "            'Rg':      preds['Rg'],\n",
    "        })\n",
    "        output_df['FFV'] = output_df['FFV'].clip(0.0, 1.0)\n",
    "        output_df['Density'] = output_df['Density'].clip(lower=0.0)\n",
    "    \n",
    "        # === your requested save + append style ===\n",
    "        sub_path = \"submission_code_d.csv\"\n",
    "        output_df.to_csv(sub_path, index=False)\n",
    "        print(f\"\\n[Code C FAST] ✅ predictions saved to {sub_path}\")\n",
    "    \n",
    "        # return to global merger\n",
    "        try:\n",
    "            output_dfs.append(output_df)\n",
    "        except NameError:\n",
    "            globals()[\"output_dfs\"] = [output_df]\n",
    "    \n",
    "        print(\"\\n[Code C FAST] Submission preview:\")\n",
    "        print(output_df.head())\n",
    "        print(\"[Code C FAST] Finished.\\n\")\n",
    "    \n",
    "    \n",
    "    main(use_mixup=False)  # set True to try small tabular MixUp-lite\n",
    "    print(\"[Code D FAST] Finished.\\n\")\n",
    "\n",
    "\n",
    "# --- cross-function globals (published across steps) ---\n",
    "train = None\n",
    "test = None\n",
    "train_extended = None\n",
    "separate_subtables = None\n",
    "augment_smiles_dataset = None\n",
    "smiles_to_combined_fingerprints_with_descriptors = None\n",
    "smiles_to_combined_fingerprints_with_descriptorsOriginal = None\n",
    "make_smile_canonical = None\n",
    "filters = None\n",
    "augment_dataset = None\n",
    "\n",
    "import builtins  # avoid clash with our driver named `range`\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Code A\n",
    "# ----------------------------\n",
    "def run_code_a1():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "def run_code_b1():\n",
    "    print(\"Code B running\")\n",
    "    # Kaggle notebook magic (leave as-is)\n",
    "    !pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl -q\n",
    "\n",
    "\n",
    "def run_code_c1():\n",
    "    print(\"Code C running\")\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, rdMolDescriptors, AllChem, Fragments, Lipinski\n",
    "    from rdkit.Chem import rdmolops\n",
    "    BASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\n",
    "    RDKIT_AVAILABLE = True\n",
    "    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "    def get_canonical_smiles(smiles):\n",
    "        if not RDKIT_AVAILABLE:\n",
    "            return smiles\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return smiles\n",
    "\n",
    "\n",
    "def run_code_d1():\n",
    "    print(\"Code D running\")\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from rdkit import Chem\n",
    "\n",
    "    global train, test, train_extended, RDKIT_AVAILABLE, TARGETS\n",
    "\n",
    "    BASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\n",
    "    RDKIT_AVAILABLE = True\n",
    "    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "    train = pd.read_csv(BASE_PATH + 'train.csv')\n",
    "    test = pd.read_csv(BASE_PATH + 'test.csv')\n",
    "\n",
    "    def clean_and_validate_smiles(smiles):\n",
    "        if not isinstance(smiles, str) or len(smiles) == 0:\n",
    "            return None\n",
    "        bad_patterns = [\n",
    "            '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]',\n",
    "            \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n",
    "            '([R])', '([R1])', '([R2])',\n",
    "        ]\n",
    "        for pattern in bad_patterns:\n",
    "            if pattern in smiles:\n",
    "                return None\n",
    "        if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):\n",
    "            return None\n",
    "        if RDKIT_AVAILABLE:\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    return Chem.MolToSmiles(mol, canonical=True)\n",
    "                else:\n",
    "                    return None\n",
    "            except Exception:\n",
    "                return None\n",
    "        return smiles\n",
    "\n",
    "    train['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\n",
    "    test['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n",
    "    train = train[train['SMILES'].notnull()].reset_index(drop=True)\n",
    "    test = test[test['SMILES'].notnull()].reset_index(drop=True)\n",
    "\n",
    "    def add_extra_data_clean(df_train, df_extra, target):\n",
    "        df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n",
    "        df_extra = df_extra[df_extra['SMILES'].notnull()]\n",
    "        df_extra = df_extra.dropna(subset=[target])\n",
    "        if len(df_extra) == 0:\n",
    "            return df_train\n",
    "        df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "        cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n",
    "        unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n",
    "        for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n",
    "            if smile in cross_smiles:\n",
    "                df_train.loc[df_train['SMILES'] == smile, target] = \\\n",
    "                    df_extra[df_extra['SMILES'] == smile][target].values[0]\n",
    "        extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n",
    "        if len(extra_to_add) > 0:\n",
    "            for col in TARGETS:\n",
    "                if col not in extra_to_add.columns:\n",
    "                    extra_to_add[col] = np.nan\n",
    "            extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n",
    "            df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n",
    "        return df_train\n",
    "\n",
    "    external_datasets = []\n",
    "\n",
    "    def safe_load_dataset(path, target, processor_func, description):\n",
    "        try:\n",
    "            if path.endswith('.xlsx'):\n",
    "                data = pd.read_excel(path)\n",
    "            else:\n",
    "                data = pd.read_csv(path)\n",
    "            data = processor_func(data)\n",
    "            external_datasets.append((target, data))\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    safe_load_dataset('/kaggle/input/tc-smiles/Tc_SMILES.csv','Tc',\n",
    "                      lambda df: df.rename(columns={'TC_mean': 'Tc'}),'Tc data')\n",
    "    safe_load_dataset('/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv','Tg',\n",
    "                      lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,'TgSS enriched data')\n",
    "    safe_load_dataset('/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv','Tg',\n",
    "                      lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),'JCIM Tg data')\n",
    "    safe_load_dataset('/kaggle/input/smiles-extra-data/data_tg3.xlsx','Tg',\n",
    "                      lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),'Xlsx Tg data')\n",
    "    safe_load_dataset('/kaggle/input/smiles-extra-data/data_dnst1.xlsx','Density',\n",
    "                      lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES','Density']]\n",
    "                                .query('SMILES.notnull() and Density.notnull() and Density != \"nylon\"')\n",
    "                                .assign(Density=lambda x: x['Density'].astype(float) - 0.118),'Density data')\n",
    "    safe_load_dataset('/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv','FFV',\n",
    "                      lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,'dataset 4')\n",
    "\n",
    "    train_extended = train[['SMILES'] + TARGETS].copy()\n",
    "    for target, dataset in external_datasets:\n",
    "        train_extended = add_extra_data_clean(train_extended, dataset, target)\n",
    "\n",
    "    print(f\"[run_code_d1] train rows={len(train)}  test rows={len(test)}  train_extended rows={len(train_extended)}\")\n",
    "\n",
    "\n",
    "def run_code_e1():\n",
    "    print(\"Code E running\")\n",
    "    def separate_subtables(train_df):\n",
    "        labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "        subtables = {}\n",
    "        for label in labels:\n",
    "            subtables[label] = train_df[['SMILES', label]][train_df[label].notna()]\n",
    "        return subtables\n",
    "    globals()[\"separate_subtables\"] = separate_subtables\n",
    "\n",
    "\n",
    "def run_code_f1():\n",
    "    print(\"Code F running\")\n",
    "    import numpy as np\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, MACCSkeys\n",
    "    from rdkit.Chem import rdmolops\n",
    "    from rdkit.Chem.rdMolDescriptors import CalcTPSA, CalcNumRotatableBonds\n",
    "    from rdkit.Chem.Descriptors import MolWt, MolLogP\n",
    "    from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
    "    import networkx as nx\n",
    "\n",
    "    def augment_smiles_dataset(smiles_list, labels, num_augments=3):\n",
    "        augmented_smiles = []\n",
    "        augmented_labels = []\n",
    "        for smiles, label in zip(smiles_list, labels):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "            augmented_smiles.append(smiles)\n",
    "            augmented_labels.append(label)\n",
    "            for _ in builtins.range(num_augments):  # avoid driver clash\n",
    "                rand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n",
    "                augmented_smiles.append(rand_smiles)\n",
    "                augmented_labels.append(label)\n",
    "        return augmented_smiles, np.array(augmented_labels)\n",
    "\n",
    "    def smiles_to_combined_fingerprints_with_descriptors(smiles_list, radius=2, n_bits=128):\n",
    "        generator = GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "        fingerprints, descriptors, valid_smiles, invalid_indices = [], [], [], []\n",
    "        for i, smiles in enumerate(smiles_list):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                morgan_fp = generator.GetFingerprint(mol)\n",
    "                maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "                combined_fp = np.concatenate([np.array(morgan_fp), np.array(maccs_fp)])\n",
    "                fingerprints.append(combined_fp)\n",
    "                descriptor_values = {}\n",
    "                for name, func in Descriptors.descList:\n",
    "                    try:\n",
    "                        descriptor_values[name] = func(mol)\n",
    "                    except Exception:\n",
    "                        descriptor_values[name] = None\n",
    "                descriptor_values['MolWt'] = MolWt(mol)\n",
    "                descriptor_values['LogP'] = MolLogP(mol)\n",
    "                descriptor_values['TPSA'] = CalcTPSA(mol)\n",
    "                descriptor_values['RotatableBonds'] = CalcNumRotatableBonds(mol)\n",
    "                descriptor_values['NumAtoms'] = mol.GetNumAtoms()\n",
    "                descriptor_values['SMILES'] = smiles\n",
    "                try:\n",
    "                    adj = rdmolops.GetAdjacencyMatrix(mol)\n",
    "                    import numpy as _np\n",
    "                    G = nx.from_numpy_array(adj)\n",
    "                    if nx.is_connected(G):\n",
    "                        descriptor_values['graph_diameter'] = nx.diameter(G)\n",
    "                        descriptor_values['avg_shortest_path'] = nx.average_shortest_path_length(G)\n",
    "                    else:\n",
    "                        descriptor_values['graph_diameter'] = 0\n",
    "                        descriptor_values['avg_shortest_path'] = 0\n",
    "                    descriptor_values['num_cycles'] = len(list(nx.cycle_basis(G)))\n",
    "                except Exception:\n",
    "                    descriptor_values['graph_diameter'] = None\n",
    "                    descriptor_values['avg_shortest_path'] = None\n",
    "                    descriptor_values['num_cycles'] = None\n",
    "                descriptors.append(descriptor_values)\n",
    "                valid_smiles.append(smiles)\n",
    "            else:\n",
    "                import numpy as _np\n",
    "                fingerprints.append(_np.zeros(n_bits + 167))\n",
    "                descriptors.append({})\n",
    "                valid_smiles.append(None)\n",
    "                invalid_indices.append(i)\n",
    "        return __import__('numpy').array(fingerprints), descriptors, valid_smiles, invalid_indices\n",
    "\n",
    "    def smiles_to_combined_fingerprints_with_descriptorsOriginal(smiles_list, radius=2, n_bits=128):\n",
    "        generator = GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "        fingerprints, descriptors, valid_smiles, invalid_indices = [], [], [], []\n",
    "        for i, smiles in enumerate(smiles_list):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                morgan_fp = generator.GetFingerprint(mol)\n",
    "                maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "                combined_fp = __import__('numpy').concatenate([__import__('numpy').array(morgan_fp),\n",
    "                                                               __import__('numpy').array(maccs_fp)])\n",
    "                fingerprints.append(combined_fp)\n",
    "                descriptor_values = {}\n",
    "                for name, func in Descriptors.descList:\n",
    "                    try:\n",
    "                        descriptor_values[name] = func(mol)\n",
    "                    except Exception:\n",
    "                        descriptor_values[name] = None\n",
    "                descriptor_values['MolWt'] = MolWt(mol)\n",
    "                descriptor_values['LogP'] = MolLogP(mol)\n",
    "                descriptor_values['TPSA'] = CalcTPSA(mol)\n",
    "                descriptor_values['RotatableBonds'] = CalcNumRotatableBonds(mol)\n",
    "                descriptor_values['NumAtoms'] = mol.GetNumAtoms()\n",
    "                descriptor_values['SMILES'] = smiles\n",
    "                descriptors.append(descriptor_values)\n",
    "                valid_smiles.append(smiles)\n",
    "            else:\n",
    "                fingerprints.append(__import__('numpy').zeros(167))\n",
    "                descriptors.append({})\n",
    "                valid_smiles.append(None)\n",
    "                invalid_indices.append(i)\n",
    "        return __import__('numpy').array(fingerprints), descriptors, valid_smiles, invalid_indices\n",
    "\n",
    "    def make_smile_canonical(smile):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smile)\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except Exception:\n",
    "            return __import__('numpy').nan\n",
    "\n",
    "    globals()[\"augment_smiles_dataset\"] = augment_smiles_dataset\n",
    "    globals()[\"smiles_to_combined_fingerprints_with_descriptors\"] = smiles_to_combined_fingerprints_with_descriptors\n",
    "    globals()[\"smiles_to_combined_fingerprints_with_descriptorsOriginal\"] = smiles_to_combined_fingerprints_with_descriptorsOriginal\n",
    "    globals()[\"make_smile_canonical\"] = make_smile_canonical\n",
    "\n",
    "\n",
    "def run_code_g1():\n",
    "    print(\"Code G running\")\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors\n",
    "    from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\n",
    "    from rdkit.Chem import MACCSkeys\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    required_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt','LogP','TPSA','RotatableBonds','NumAtoms'}\n",
    "\n",
    "    filters_local = {\n",
    "        'Tg': list(set([\n",
    "            'BalabanJ','BertzCT','Chi1','Chi3n','Chi4n','EState_VSA4','EState_VSA8',\n",
    "            'FpDensityMorgan3','HallKierAlpha','Kappa3','MaxAbsEStateIndex','MolLogP',\n",
    "            'NumAmideBonds','NumHeteroatoms','NumHeterocycles','NumRotatableBonds',\n",
    "            'PEOE_VSA14','Phi','RingCount','SMR_VSA1','SPS','SlogP_VSA1','SlogP_VSA5',\n",
    "            'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState4','VSA_EState6','VSA_EState7',\n",
    "            'VSA_EState8','fr_C_O_noCOO','fr_NH1','fr_benzene','fr_bicyclic','fr_ether',\n",
    "            'fr_unbrch_alkane'\n",
    "        ]).union(required_descriptors)),\n",
    "        'FFV': list(set([\n",
    "            'AvgIpc','BalabanJ','BertzCT','Chi0','Chi0n','Chi0v','Chi1','Chi1n','Chi1v',\n",
    "            'Chi2n','Chi2v','Chi3n','Chi3v','Chi4n','EState_VSA10','EState_VSA5',\n",
    "            'EState_VSA7','EState_VSA8','EState_VSA9','ExactMolWt','FpDensityMorgan1',\n",
    "            'FpDensityMorgan2','FpDensityMorgan3','FractionCSP3','HallKierAlpha',\n",
    "            'HeavyAtomMolWt','Kappa1','Kappa2','Kappa3','MaxAbsEStateIndex',\n",
    "            'MaxEStateIndex','MinEStateIndex','MolLogP','MolMR','MolWt','NHOHCount',\n",
    "            'NOCount','NumAromaticHeterocycles','NumHAcceptors','NumHDonors',\n",
    "            'NumHeterocycles','NumRotatableBonds','PEOE_VSA14','RingCount','SMR_VSA1',\n",
    "            'SMR_VSA10','SMR_VSA3','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA9','SPS',\n",
    "            'SlogP_VSA1','SlogP_VSA10','SlogP_VSA11','SlogP_VSA12','SlogP_VSA2',\n",
    "            'SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7',\n",
    "            'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState10','VSA_EState2',\n",
    "            'VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6','VSA_EState7',\n",
    "            'VSA_EState8','VSA_EState9','fr_Ar_N','fr_C_O','fr_NH0','fr_NH1',\n",
    "            'fr_aniline','fr_ether','fr_halogen','fr_thiophene'\n",
    "        ]).union(required_descriptors)),\n",
    "        'Tc': list(set([\n",
    "            'BalabanJ','BertzCT','Chi0','EState_VSA5','ExactMolWt','FpDensityMorgan1',\n",
    "            'FpDensityMorgan2','FpDensityMorgan3','HeavyAtomMolWt','MinEStateIndex',\n",
    "            'MolWt','NumAtomStereoCenters','NumRotatableBonds','NumValenceElectrons',\n",
    "            'SMR_VSA10','SMR_VSA7','SPS','SlogP_VSA6','SlogP_VSA8','VSA_EState1',\n",
    "            'VSA_EState7','fr_NH1','fr_ester','fr_halogen'\n",
    "        ]).union(required_descriptors)),\n",
    "        'Density': list(set([\n",
    "            'BalabanJ','Chi3n','Chi3v','Chi4n','EState_VSA1','ExactMolWt',\n",
    "            'FractionCSP3','HallKierAlpha','Kappa2','MinEStateIndex','MolMR','MolWt',\n",
    "            'NumAliphaticCarbocycles','NumHAcceptors','NumHeteroatoms',\n",
    "            'NumRotatableBonds','SMR_VSA10','SMR_VSA5','SlogP_VSA12','SlogP_VSA5',\n",
    "            'TPSA','VSA_EState10','VSA_EState7','VSA_EState8'\n",
    "        ]).union(required_descriptors)),\n",
    "        'Rg': list(set([\n",
    "            'AvgIpc','Chi0n','Chi1v','Chi2n','Chi3v','ExactMolWt','FpDensityMorgan1',\n",
    "            'FpDensityMorgan2','FpDensityMorgan3','HallKierAlpha','HeavyAtomMolWt',\n",
    "            'Kappa3','MaxAbsEStateIndex','MolWt','NOCount','NumRotatableBonds',\n",
    "            'NumUnspecifiedAtomStereoCenters','NumValenceElectrons','PEOE_VSA14',\n",
    "            'PEOE_VSA6','SMR_VSA1','SMR_VSA5','SPS','SlogP_VSA1','SlogP_VSA2',\n",
    "            'SlogP_VSA7','SlogP_VSA8','VSA_EState1','VSA_EState8','fr_alkyl_halide',\n",
    "            'fr_halogen'\n",
    "        ]).union(required_descriptors))\n",
    "    }\n",
    "    globals()[\"filters\"] = filters_local\n",
    "\n",
    "\n",
    "def run_code_h1():\n",
    "    print(\"Code H running\")\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    def augment_dataset(X, y, n_samples=1000, n_components=5, random_state=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        elif not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X must be a pandas DataFrame or a NumPy array\")\n",
    "        X.columns = X.columns.astype(str)\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = pd.Series(y)\n",
    "        elif not isinstance(y, pd.Series):\n",
    "            raise ValueError(\"y must be a pandas Series or a NumPy array\")\n",
    "        df = X.copy(); df['Target'] = y.values\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "        gmm.fit(df)\n",
    "        synthetic_data, _ = gmm.sample(n_samples)\n",
    "        synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "        augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n",
    "        return augmented_df.drop(columns='Target'), augmented_df['Target']\n",
    "    globals()[\"augment_dataset\"] = augment_dataset\n",
    "\n",
    "\n",
    "def run_code_i1():\n",
    "    print(\"code I running\")\n",
    "    import os, json, time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from xgboost import XGBRegressor as _XGBRegressor\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    if train_extended is None or test is None or separate_subtables is None \\\n",
    "       or augment_smiles_dataset is None or smiles_to_combined_fingerprints_with_descriptors is None \\\n",
    "       or filters is None or augment_dataset is None:\n",
    "        raise RuntimeError(\"Missing globals. Ensure run_code_d1/e1/f1/g1/h1 ran before run_code_i1.\")\n",
    "\n",
    "    # ---- GPU factory (forces GPU when available) ----\n",
    "    try:\n",
    "        import xgboost as _xgb\n",
    "        _xgb_version = tuple(int(p) for p in _xgb.__version__.split(\".\")[:2])\n",
    "    except Exception:\n",
    "        _xgb_version = (1, 7)\n",
    "\n",
    "    def _torch_cuda():\n",
    "        try:\n",
    "            import torch\n",
    "            return torch.cuda.is_available()\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def XGBRegressor(**kwargs):\n",
    "        if _torch_cuda():\n",
    "            if _xgb_version >= (2, 0):\n",
    "                kwargs.setdefault(\"device\", \"cuda\")\n",
    "                kwargs.setdefault(\"max_bin\", 256)\n",
    "                # (tree_method 'hist' works with device='cuda' in 2.x)\n",
    "            else:\n",
    "                kwargs.setdefault(\"tree_method\", \"gpu_hist\")\n",
    "                kwargs.setdefault(\"predictor\", \"gpu_predictor\")\n",
    "                kwargs.setdefault(\"gpu_id\", 0)\n",
    "                kwargs.setdefault(\"max_bin\", 256)\n",
    "        else:\n",
    "            kwargs.setdefault(\"n_jobs\", -1)\n",
    "        return _XGBRegressor(**kwargs)\n",
    "\n",
    "    # ---- NVML + nvidia-smi meters ----\n",
    "    def _print_gpu_status(tag=\"\"):\n",
    "        try:\n",
    "            import shutil, subprocess\n",
    "            if shutil.which(\"nvidia-smi\"):\n",
    "                out = subprocess.check_output(\n",
    "                    [\"nvidia-smi\",\"--query-gpu=name,memory.total,memory.used,utilization.gpu\",\n",
    "                     \"--format=csv,noheader,nounits\"]\n",
    "                ).decode().strip().splitlines()\n",
    "                for i, line in enumerate(out):\n",
    "                    name, mem_tot, mem_used, util = [s.strip() for s in line.split(\",\")]\n",
    "                    print(f\"[GPU{i} {tag}] {name} | {mem_used}/{mem_tot} MiB | util {util}%\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    _nvml = None\n",
    "    def _gpu_util_nvml():\n",
    "        nonlocal _nvml\n",
    "        try:\n",
    "            if _nvml is None:\n",
    "                import pynvml\n",
    "                pynvml.nvmlInit()\n",
    "                _nvml = pynvml\n",
    "            h = _nvml.nvmlDeviceGetHandleByIndex(0)\n",
    "            util = _nvml.nvmlDeviceGetUtilizationRates(h)\n",
    "            mem = _nvml.nvmlDeviceGetMemoryInfo(h)\n",
    "            return util.gpu, int(mem.used/1024/1024), int(mem.total/1024/1024)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _gpu_pulse_callback(label, every=50):\n",
    "        try:\n",
    "            from xgboost.callback import TrainingCallback\n",
    "        except Exception:\n",
    "            return None\n",
    "        class GPUPulse(TrainingCallback):\n",
    "            def after_iteration(self, model, epoch, evals_log):\n",
    "                if epoch % every == 0:\n",
    "                    u = _gpu_util_nvml()\n",
    "                    if u:\n",
    "                        uperc, used, tot = u\n",
    "                        print(f\"[NVML {label} iter {epoch}] util {uperc}% | {used}/{tot} MiB\")\n",
    "                    else:\n",
    "                        _print_gpu_status(f\"{label} iter {epoch}\")\n",
    "                return False\n",
    "        return GPUPulse()\n",
    "\n",
    "    print(f\"[CUDA available] {_torch_cuda()} | XGBoost {_xgb_version}\")\n",
    "    _print_gpu_status(\"start\")\n",
    "\n",
    "    # --- pipeline ---\n",
    "    train_df = train_extended\n",
    "    test_df = test\n",
    "    subtables_local = separate_subtables(train_df)\n",
    "    test_smiles = test_df['SMILES'].tolist()\n",
    "    test_ids = test_df['id'].values\n",
    "    labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "    output_df = pd.DataFrame({'id': test_ids})\n",
    "\n",
    "    drop_cols = ['BCUT2D_MWLOW','BCUT2D_MWHI','BCUT2D_CHGHI','BCUT2D_CHGLO','BCUT2D_LOGPHI',\n",
    "                 'BCUT2D_LOGPLOW','BCUT2D_MRLOW','BCUT2D_MRHI','MinAbsPartialCharge',\n",
    "                 'MaxPartialCharge','MinPartialCharge','MaxAbsPartialCharge','SMILES']\n",
    "\n",
    "    for label in labels:\n",
    "        originals = subtables_local[label]\n",
    "        original_smiles = originals['SMILES'].tolist()\n",
    "        original_labels = originals[label].values\n",
    "\n",
    "        original_smiles, original_labels = augment_smiles_dataset(original_smiles, original_labels, num_augments=1)\n",
    "\n",
    "        fingerprints, descriptors, valid_smiles, invalid_indices = \\\n",
    "            smiles_to_combined_fingerprints_with_descriptors(original_smiles, radius=2, n_bits=128)\n",
    "\n",
    "        descriptors = [d if isinstance(d, dict) else {} for d in descriptors]\n",
    "        X = pd.DataFrame(descriptors).drop(drop_cols, axis=1, errors='ignore')\n",
    "        fp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in builtins.range(fingerprints.shape[1])])\n",
    "        if invalid_indices:\n",
    "            X = X.drop(index=invalid_indices).reset_index(drop=True)\n",
    "            fp_df = fp_df.drop(index=invalid_indices).reset_index(drop=True)\n",
    "        y = np.delete(original_labels, invalid_indices)\n",
    "\n",
    "        X = X.filter(filters[label])\n",
    "        X = pd.concat([X.reset_index(drop=True), fp_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        selector = VarianceThreshold(threshold=0.01)\n",
    "        X = selector.fit_transform(X)\n",
    "\n",
    "        X, y = augment_dataset(X, y, n_samples=1000)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "        if label == \"Tg\":\n",
    "            Model = XGBRegressor(n_estimators=2173, learning_rate=0.0672418745539774, max_depth=6, reg_lambda=5.545520219149715)\n",
    "        if label == 'Rg':\n",
    "            Model = XGBRegressor(n_estimators=520, learning_rate=0.07324113948440986, max_depth=5, reg_lambda=0.9717380315982088)\n",
    "        if label == 'FFV':\n",
    "            Model = XGBRegressor(n_estimators=2202, learning_rate=0.07220580588586338, max_depth=4, reg_lambda=2.8872976032666493)\n",
    "        if label == 'Tc':\n",
    "            Model = XGBRegressor(n_estimators=1488, learning_rate=0.010456188013762864, max_depth=5, reg_lambda=9.970345982204618)\n",
    "        if label == 'Density':\n",
    "            Model = XGBRegressor(n_estimators=1958, learning_rate=0.10955287548172478, max_depth=5, reg_lambda=3.074470087965767)\n",
    "\n",
    "        # Live meter\n",
    "        pulse = _gpu_pulse_callback(label, every=50)\n",
    "        callbacks = [pulse] if pulse is not None else None\n",
    "\n",
    "        # Fit with eval_set so callbacks fire\n",
    "        Model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, callbacks=callbacks)\n",
    "\n",
    "        # Verify GPU by printing booster config (should contain \"device\":\"cuda\" for 2.x)\n",
    "        try:\n",
    "            cfg = Model.get_booster().save_config()\n",
    "            print(f\"[{label}] booster config snippet: \" + (\"cuda\" if \"cuda\" in cfg else \"cpu/fallback?\") )\n",
    "            # Uncomment to inspect: print(cfg)\n",
    "        except Exception as e:\n",
    "            print(f\"[{label}] could not read booster config: {e}\")\n",
    "\n",
    "        val_pred = Model.predict(X_test).flatten()\n",
    "        val_mae = mean_absolute_error(y_test, val_pred)\n",
    "        print(f\"[{label}] Holdout MAE: {val_mae:.6f}\")\n",
    "\n",
    "        # Refit on all data\n",
    "        Model.fit(X, y, verbose=False, callbacks=callbacks)\n",
    "\n",
    "        # Build test features\n",
    "        fingerprints_te, descriptors_te, valid_smiles_te, invalid_indices_te = \\\n",
    "            smiles_to_combined_fingerprints_with_descriptors(test_smiles, radius=2, n_bits=128)\n",
    "        descriptors_te = [d if isinstance(d, dict) else {} for d in descriptors_te]\n",
    "        test_df_desc = pd.DataFrame(descriptors_te).drop(drop_cols, axis=1, errors='ignore')\n",
    "        test_df_desc = test_df_desc.filter(filters[label])\n",
    "        fp_df_te = pd.DataFrame(fingerprints_te, columns=[f'FP_{i}' for i in builtins.range(fingerprints_te.shape[1])])\n",
    "        test_df_desc = pd.concat([test_df_desc.reset_index(drop=True), fp_df_te.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        testX = selector.transform(test_df_desc)\n",
    "        y_pred = Model.predict(testX).flatten()\n",
    "        output_df[label] = y_pred\n",
    "\n",
    "        u = _gpu_util_nvml()\n",
    "        if u:\n",
    "            up, used, tot = u\n",
    "            print(f\"[NVML end {label}] util {up}% | {used}/{tot} MiB\")\n",
    "        _print_gpu_status(f\"end {label}\")\n",
    "\n",
    "    \n",
    "    # === your requested save + append style ===\n",
    "    sub_path = \"submission_code_e.csv\"\n",
    "    output_df.to_csv(sub_path, index=False)\n",
    "    print(f\"\\n[Code e FAST] ✅ predictions saved to {sub_path}\")\n",
    "    \n",
    "    # return to global merger\n",
    "    try:\n",
    "        output_dfs.append(output_df)\n",
    "    except NameError:\n",
    "        globals()[\"output_dfs\"] = [output_df]\n",
    "    \n",
    "    print(\"\\n[Code e FAST] Submission preview:\")\n",
    "    print(output_df.head())\n",
    "    print(\"[Code e FAST] Finished.\\n\")\n",
    "\n",
    "\n",
    "# ---- driver ----\n",
    "def rangerr():\n",
    "    run_code_a1()\n",
    "    run_code_b1()\n",
    "    run_code_c1()\n",
    "    run_code_d1()\n",
    "    run_code_e1()\n",
    "    run_code_f1()\n",
    "    run_code_g1()\n",
    "    run_code_h1()\n",
    "    run_code_i1()\n",
    "    \n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    run_code_a()\n",
    "    run_code_b()\n",
    "    run_code_c()\n",
    "    run_code_d()\n",
    "    rangerr()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e22e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:53:40.187000Z",
     "iopub.status.busy": "2025-09-14T19:53:40.186748Z",
     "iopub.status.idle": "2025-09-14T19:53:40.192196Z",
     "shell.execute_reply": "2025-09-14T19:53:40.191643Z"
    },
    "papermill": {
     "duration": 0.032796,
     "end_time": "2025-09-14T19:53:40.193185",
     "exception": false,
     "start_time": "2025-09-14T19:53:40.160389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4e0416b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T19:53:40.246303Z",
     "iopub.status.busy": "2025-09-14T19:53:40.246116Z",
     "iopub.status.idle": "2025-09-14T19:53:40.264887Z",
     "shell.execute_reply": "2025-09-14T19:53:40.264297Z"
    },
    "papermill": {
     "duration": 0.046422,
     "end_time": "2025-09-14T19:53:40.265966",
     "exception": false,
     "start_time": "2025-09-14T19:53:40.219544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1109053969</td>\n",
       "      <td>161.604503</td>\n",
       "      <td>0.374693</td>\n",
       "      <td>0.176553</td>\n",
       "      <td>1.154877</td>\n",
       "      <td>21.624853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422188626</td>\n",
       "      <td>172.262268</td>\n",
       "      <td>0.377267</td>\n",
       "      <td>0.249543</td>\n",
       "      <td>1.089396</td>\n",
       "      <td>21.272616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032016830</td>\n",
       "      <td>118.896318</td>\n",
       "      <td>0.352311</td>\n",
       "      <td>0.258238</td>\n",
       "      <td>1.109892</td>\n",
       "      <td>20.875625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id          Tg       FFV        Tc   Density         Rg\n",
       "0  1109053969  161.604503  0.374693  0.176553  1.154877  21.624853\n",
       "1  1422188626  172.262268  0.377267  0.249543  1.089396  21.272616\n",
       "2  2032016830  118.896318  0.352311  0.258238  1.109892  20.875625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Average predictions from all output DataFrames\n",
    "final_df = pd.concat(output_dfs, axis=0).groupby('id').mean().reset_index()\n",
    "final_df.to_csv('submission.csv', index=False)\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "isSourceIdPinned": false,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7279248,
     "sourceId": 11605551,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7678100,
     "sourceId": 12189904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7690162,
     "sourceId": 12207625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7706066,
     "sourceId": 12237259,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7709869,
     "sourceId": 12330396,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8238327,
     "sourceId": 13048231,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8267203,
     "sourceId": 13055312,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8269638,
     "sourceId": 13058937,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 247698673,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 247701857,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 251268093,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 380893,
     "modelInstanceId": 359690,
     "sourceId": 442871,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1276.755052,
   "end_time": "2025-09-14T19:53:43.692050",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-14T19:32:26.936998",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
